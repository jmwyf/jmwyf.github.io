[{"content":"1.Introduction to Deep learning 震撼，第一节课直接放大招，用自己拍摄的视频和奥巴马合成来介绍这门课程。 不管老师在课程上讲什么，希望你们能真正的思考为什么这一步是重要而且必须的，正是这些思考才能做出真正令人惊讶的突破。 2.Deep Sequence Model Three way to solve gradient vanish\nGated Cells LSTM Forget Store Update Output Attention [[Transformer]] 3.Deep Computer Vision 介绍卷积操作，是一种提取特征的方法生成feature maps（还有其他的方法可以用吗？然后效果还不错）； 与全连接相比的优点； Fast RCNN用于目标检测，怎么实现推荐特定区域图像？ 医学图片分割 总结： 原理 CNN架构 应用 4.Deep Generative Models what 目标: 来自于一些分布中的训练样本，通过这些样本学习模型来表征这个分布； how 密度估计；神经网络适合来进行高维度表征； why Debiasing: Capable of uncovering underlying features in a dataset Outlier detection: how can we detect when we encounter something new or rare? Latent variable representation: 举例事物的投影，只能看见影子即表象，而被灯光照射的实物是看不见的即隐变量；要做的是通过观察到的投影来对实物进行建模 Autoencoder: reconstruction loss 完全是确定性性 VAEs：normal prior + regularization reconstruction loss + regularization term encoder: $q_\\phi(z|x)$ decoder: $p_\\theta(x|z)$ KL-divergence: $D(q_\\phi(z|x)||p(z))$ GANs make a generative model by having two neural networks compete with each other ⭐️CycleGAN: domain transformations 视频开头的视频就是用这个合成 5.Deep reinforcement learning Reward: $$R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \u0026hellip;$$ Q-function: expected total future reward $$Q(s_t, a_t) = E[R_t|s_t, a_t]$$ Policy: to infer the best action to take at its state, choose an action that maximizes future reward $$\\pi^*(s)=\\mathop{\\arg\\max}\\limits_{s}Q(s, a)$$ Value Learning find $Q(s, a)$ $a = \\mathop{\\arg\\max}\\limits_{a}Q(s, a)$ Police Learning find $\\pi(s)$ sample $a\\sim\\pi(s)$ Deep Q Network(DQN) Policy Gradient AlphaGo 6.DL Limitations and New Frontiers limitations Generalization data is important Uncertainty in Deep learning adversarial attack Algorithmic Bias Frontiers encoder many real world data cannot be captured by standard encodings GCN（Graph Convolutional Networks） Automated AI 7. LiDAR for Autonomous Driving @INNOVIZ\nCamera Vs LiDAR 互补，视线不好的情况 冗余能保证准确 Safety and Comfort 8. Automatic Speech Recognition @Rev\nConformer CTC 9. AI fore Science Principled AI Algorithms for challenging domains @Caltech\n10. Uncertainty in Deep Learning longer version：NeurIPS 2020 Tutorial @Google AI Brain Team\nReturn a distribution over predictions rather than a single prediction Out-of-Distribution Robustness covariate shift: distribution of features changes open-set recognition: new classes may appear at test time label shift: distribution of label changes sources of uncertainty Model uncertainty 认知上的不确定性 Data uncertainty human disagreement label noise measurement noise missing data how to compute BDN GP Deep Ensemble MCMC multi-input and multi output（MIMO） how to communicate with uncertainty? 7-10讲很一般，一个复杂的主题，需要将背景讲清楚，公司讲东西也没啥具体细节。\nRef 【双语字幕】MIT《深度学习导论(6.S191)》课程(2021)_哔哩哔哩_bilibili introtodeeplearning.com MIT 6.S191: Deep Generative Modeling - YouTube ","date":"2023-01-13T16:59:00Z","permalink":"https://jmwyf.github.io/p/mit-6.s91-introduction-deep-learning-notes/","title":"MIT 6.S91 Introduction Deep Learning Notes"},{"content":"《A Unified Approach to Interpreting Model Predictions》论文解读 Introduction 大数据让复杂模型的优势明显 提出一种新颖统一的方法用于模型解释 用模型的方法来解释复杂模型（用魔法打败魔法） 提出SHAP值作为各种方法近似统一特征重要度度量 提出新的SHAP值估计方法 Interpretation model properties 描述解释模型需要有的三个性质，而现在解释方法的缺陷有哪些1\n局部准确性：如果x‘是x的简化特征，对应解释模型g(x\u0026rsquo;) = f(x)，即解释模型在给定的特征情况下能解释为什么模型预测值是这么多。 缺失性：当x\u0026rsquo;=0的时候，贡献度$\\phi$为0 一致性：模型改变导致特征变的更重要时，贡献度也应该变大 Additive Feature Attribution methods 一大类方法中解释模型是一系列二元变量的线性函数 称为Additive Feature Attribution methods（AFA）相加特征归因方法 $$g(z\u0026rsquo;) = \\phi_0 + \\sum_{i=1}^{M}\\phi_i z\u0026rsquo;_{i}$$ $z\u0026rsquo; \\in {0, 1}^M$\nClassic Shapley Value Estimation $$\\phi_{i} = \\sum_{S \\subseteq F\\backslash{i}}\\frac{|S|!(|F|-|S|-1)!}{|F|!}[f_{S\\cup{i}}(x_{S\\cup{i}})-f_{S(x_S)}]$$ 基于上面公式精确计算很麻烦，特征的排列有$2^F$种，计算量巨大\nSHAP SHAP分为模型无关和模型相关两类方法用来近似求解，模型无关的代表是kernelSHAP，而模型相关的代表则有DeepSHAP和TreeSHAP，一个是针对深度学习，一个是针对树模型。\nKernel SHAP LIME LIME2（Local interpretable model-agnostic explanations)（why should I trust you: Explaining the predictions of any classifier）通过生成的包含需要解释点周围的扰动数据和基于黑箱模型预测结果的数据集，训练一个可以解释的模型，比如逻辑回归、决策树，这个可解释模型需要在解释点周围达到较好的效果。 $$\\xi = \\mathop{\\arg\\min}\\limits_{g\\in G}L(f,g,\\pi_x) + \\Omega(g)$$\nf为需解释模型 g为可能的解释模型 $\\pi_x$为定义实例周围多大范围 算法过程：\n选择需要解释感兴趣的实例 对其进行扰动，并得到黑箱模型对应结果产生新数据集 根据与实例的接近程度，对新数据集进行赋予权重 基于新数据集和上述损失函数求解可解释模型 解释预测值 Figure 3: Toy example to present intuition for LIME KernelSHAP（Linear LIME + Shapley value） LIME（Local interpretable model-agnostic explanations）方法的拓展，通过修改LIME需要求解loss等式参数，其主要思路是利用核变换，让$\\phi$符合shapley value3 算法过程4：\n根据$z_k^{\u0026rsquo;} \\in {0, 1}^M$选择k个样本 将$z_k^{\u0026rsquo;} \\in {0, 1}^M$转化为原始特征值并计算黑箱模型预测值 基于SHAP kernel计算$z_k^{\u0026rsquo;}$ 样本权重，$z_k$里面1的个数不一样权重就不一样 拟合线性模型 从线性模型中返回Shapley values SHAP核为(推导过程见2补充材料)： $$\\pi_{x\u0026rsquo;}(z\u0026rsquo;)= \\frac{(M-1)}{(M choose |z\u0026rsquo;|)|z\u0026rsquo;|(M-|z\u0026rsquo;|)}$$ $$L(f,g, \\pi_{x\u0026rsquo;})=\\sum_{z\u0026rsquo;\\in Z}[f(h_x^{-1}(z\u0026rsquo;))-g(z\u0026rsquo;)]^2\\pi_{x\u0026rsquo;}(z\u0026rsquo;)$$ x\u0026rsquo;为简化输入，$x=h_x(x\u0026rsquo;)$, $z\u0026rsquo; \\subseteq x\u0026rsquo;$ 其中第二步：The function h maps 1’s to the corresponding value from the instance x that we want to explain. For tabular data, it maps 0’s to the values of another instance that we sample from the data. This means that we equate “feature value is absent” with “feature value is replaced by random feature value from data”. Deep SHAP DeepLIFT（Learning Important FeaTures） DeepLIFT5方法使用神经元的激活与其“参考”进行比较，其中参考是神经元在网络获得“参考输入”时具有的激活状态（参考输入根据具体任务的内容定义）。该方法赋予每个特征重要度分数之和等于预测值与基于参考输入的预测值之间的差异6。 能解决基于梯度方法的不足，例如参考的差异不是0的情况下梯度仍然可能是0。 $$\\sum_{i=1}^n C_{\\Delta x_i \\Delta t} = \\Delta t \\tag1$$ $\\Delta t = t - t^0$, 神经元输出与参考输出的差异，$\\Delta x$输入相对参考输入的变化， C即特征的贡献\n$$m_{\\Delta x\\Delta t} = \\frac{C_{\\Delta x\\Delta t}}{\\Delta x} \\tag2$$ 定义乘子multiplier，满足链式法则 算法步骤7：\n定义参考值 choosing a good reference would rely on domain-specific knowledge, and in some cases it may be best to compute DeepLIFT scores against multiple different references 比如图像用全0或者模糊版本，基因数据用本底期望频率 区分正负贡献（2019） 贡献度规则 线性层直接是系数$w* \\Delta x_i$ 非线性变化是$m_{\\Delta x\\Delta y} = \\frac{C_{\\Delta x\\Delta y}}{\\Delta x} =\\frac{\\Delta y}{\\Delta x}$ DeepSHAP(DeepLIFT + Shapley value) 尽管kernelSHAP是适用于所有模型的包括深度学习模型的一种可解释方法，但是有没有能利用神经网络特性的可解释方法从而提高计算效率。 DeepLIFT计算的分数近似于Shapley value?并不是这个思路\nthe Shapely values measure the average marginal effect of including an input over all possible orderings in which inputs can be included. If we define “including” an input as setting it to its actual value instead of its reference value, DeepLIFT can be thought of as a fast approximation of the Shapely values8 Though a variety of methods exist for estimating SHAP values, we implemented a modified version of the DeepLIFT algorithm, which computes SHAP by estimating differences in model activations during backpropagation relative to a standard reference. figure from ref[3] 基于Shapley value的定义以及公式可以看出重要的一部分即边际效应，即模型包含该特征减去未包含该部分。在上述一个神经网络模块里面，特征顺序选择都不存在。在认为包含特征即相对于参考输入是真实输入的情况下，把包含特征后乘子直接链式法则做为SHAP值近似公式 在上述简单网络组件里面，输入到输出之间可以看作线性近似从而得到公式16 把用实际值代替参考值看作是包含某个特征，DeepLIFT方法与DeepSHAP似乎看不到区别? 6 – Interpretability – Machine Learning Blog | ML@CMU | Carnegie Mellon University\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRibeiro, M. T., Singh, S. \u0026amp; Guestrin, C. ‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1135–1144 (2016) doi:10.1145/2939672.2939778.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Unified Approach to Interpreting Model Predictions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n9.6 SHAP (SHapley Additive exPlanations) | Interpretable Machine Learning\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ndeeplift 0.6.13.0 on PyPI - Libraries.io\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExplainable Neural Networks: Recent Advancements, Part 3 | by G Roshan Lal | Towards Data Science\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShrikumar, A., Greenside, P., Shcherbina, A. \u0026amp; Kundaje, A. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences. Preprint at https://doi.org/10.48550/arXiv.1605.01713 (2017).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShrikumar, A., Greenside, P. \u0026amp; Kundaje, A. Learning Important Features Through Propagating Activation Differences. Preprint at http://arxiv.org/abs/1704.02685 (2019).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-01-06T16:59:00Z","permalink":"https://jmwyf.github.io/p/a-unified-approach-to-interpreting-model-predictions%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/","title":"《A Unified Approach to interpreting Model Predictions》论文解读"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-11-03T00:00:00Z","image":"https://jmwyf.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://jmwyf.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"PyTorch深度学习（2） Deep Learning = Learning Hierarchical Representations 深度学习即学习层次的表征。\n1. 卷积神经网络 1.1 神经网络可视化（Visualization of neural networks） 神经网络每一层的操作有点像将空间某些区域进行折叠\n1.2 卷积神经网络的起源（Convolutional Neural Network；CNN） 受到Fukushima在视觉皮层建模方面的启发，使用简单/复杂的细胞层次结构，结合有监督的训练和反向传播，由Yann LeCun教授于88-89年在多伦多大学开发了第一个CNN。\nFukushima的工作具体是什么呢？\n手写数字识别。首次提出应用多层简单或者复杂的细胞结构建模，特征：手工加无监督聚类学习。无反向传播。\n1.3 卷积神经网络分解 通用的CNN架构能被分解为以下几个基本结构。\n标准化（Normalisation）:对比度标准化等 滤波器组（Filter banks）:边缘检测等 非线性化（Non-linearities）:稀疏化、ReLU等 池化（pooling）:最大池化（max pooling）等 2. 自然信号数据（Natural Signals） 2.1 自然信号数据特性 周期性：在时域很多模式都会重复出现 局部性：相邻的点较相远的点来说更具关联性 合成性：复杂的事物可以由简单的事物组合而成。字母-\u0026gt;单词-\u0026gt;句子-\u0026gt;文章 2.2 对应神经网络中的处理方法 周期性$\\rightarrow$参数共享\n如果数据存在周期性，可以使用参数共享，即卷积核。 局部性$\\rightarrow$稀疏\n如果数据存在局部性，那么每个神经元只需要与前几个神经元连接 合成性$\\rightarrow$多层\n即神经网络中多层网络合成最终的结果 3. Pytorch实现Mnist手写字识别 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # load package and data import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms import matplotlib.pyplot as plt device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) # 神经网络模型偏爱标准化数据，原因是均值为0方差为1的数据在sigmoid、tanh经过激活函数后求导得到的导数很大， # 反之原始数据不仅分布不均（噪声大）而且数值通常都很大（本例中数值范围是0~255），激活函数后求导得到的导数 # 则接近与0，这也被称为梯度消失。 # 目录放自己下载好的mnist目录，没有下载将download=True,自己新建一个存放数据目录即可 train_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;../LSTM_mnist/mnist\u0026#39;, train=True, download=False, transform=transforms.Compose([ transforms.ToTensor(), # mnist数据集均值0.1307，标准差0.3081 transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True) test_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;../LSTM_mnist/mnist\u0026#39;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=1000, shuffle=True) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # define model class SimpleCNN(nn.Module): def __init__(self, input_size, n_feature, output_size): super(SimpleCNN, self).__init__() self.n_feature = n_feature # 关于nn.Conv2d()中参数的解释 # in_channels (int): Number of channels in the input image # out_channels (int): Number of channels produced by the convolution # kernel_size (int or tuple): Size of the convolving kernel # default stride=1, padding=0, dilation=1, groups=1 # [groupsc参数详解](https://www.jianshu.com/p/20ba3d8f283c) # [图解卷积神经网络中stride, padding等操作可视化](https://github.com/vdumoulin/conv_arithmetic) # input: (N, C_in, H_in, W_in) self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5) self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5) self.fc1 = nn.Linear(n_feature*4*4, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.conv1(x) # Mnist数据原始大小（28*28）28-5+1 = 24 (24*24*n_feature) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2) # (12*12*n_feature) x = self.conv2(x) # 12-5+1 = 8 (8*8*n_feature) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2) # (4*4*n_feature)这里解释了上面全连接时为啥是4*4 x = x.view(-1, self.n_feature*4*4) x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.log_softmax(x, dim=1) return x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # hyper parameters epochs = 1 input_size = 28*28 output_size = 10 n_features = 6 lr = 0.01 model = SimpleCNN(input_size, n_features, output_size) model.to(device) # optimizer optimizer = optim.SGD(model.parameters(), lr, momentum=0.5) print(\u0026#39;Number of parameters: {}\u0026#39;.format(get_n_params(model))) # model train for epoch in range(epochs): model.train() for i, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) output = model(data) loss = F.nll_loss(output, target) optimizer.zero_grad() loss.backward() optimizer.step() if i % 100 == 0: print(\u0026#39;Train Epoch [{}/{}], [{}/{} ({:.0f}%)], Loss: {:.4f}\u0026#39;.format( epoch+1, epochs, i*len(data), len(train_loader.dataset), 100*i/len(train_loader), loss.item())) # model eval model.eval() test_loss = 0 correct = 0 accuracy_list = [] for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction=\u0026#39;sum\u0026#39;).item() pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability correct += pred.eq(target.data.view_as(pred)).cpu().sum().item() test_loss /= len(test_loader.dataset) accuracy = 100. * correct / len(test_loader.dataset) accuracy_list.append(accuracy) print(\u0026#39;\\nTest set Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\u0026#39;.format( test_loss, correct, len(test_loader.dataset), accuracy)) 1 2 3 4 5 6 7 # 几个预测的实例可视化 for i in range(10): plt.subplot(2, 5, i+1) plt.imshow(data[i][0]) plt.title(\u0026#34;Prediction: {}\u0026#34;.format( output.data.max(1, keepdim=True)[1][i].item())) plt.show() 4. 补充 可以做一个有趣的实验即打乱图片中的像素后CNN识别正确率下降，而全连接网络则不会，即与最开始提到的三个特性以及对于神经网络采取的假设是吻合的。 参考2中是对卷积神经网络全面的介绍，包括CNN中常用那些层，以及常用的模型和参数多少计算。 ref NYC PyTorch Deep Learning课程网站 cs231n convolutional networks pytorch官方文档Conv2d 课程convnet.ipynb ","date":"2020-04-01T14:51:32Z","image":"https://jmwyf.github.io/images/pytorch/mnist_pre.png","permalink":"https://jmwyf.github.io/p/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02/","title":"PyTorch深度学习（2）"},{"content":"重读XGBoost 在使用xgboost方法调参时，对其中个别参数不是特别理解。故重新读了一遍原论文。\n1. 引言 阐述机器学习和数据驱动的方法应用时两个重要的因素：\n能捕捉数据间复杂依赖关系的模型 可扩展的学习系统，可以从大量数据中学习 在目前常用的方法中，梯度提升树（gradient tree boosting）在许多场景中效果都不错，作者列举了一些。提出xgboost方法在比赛以及各类问题中的应用。\n叙述XGBoost的优点：运行更快、拓展性更好。创新点包括：\n高度可拓展的端到端提升树（tree boosting）系统 用于高效计算的加权分位数图（weighted quantile sketch） 新颖的稀疏感知算法（sparsity-aware algorithm），用于并行树学习 有效的缓存优化以及块（cache-aware block）结构用于外存（out-of-core）树学习 关于以上几点在正文中详解。 论文结构：\n提升树（tree boosting）简介以及目标函数正则化 分裂点寻找的方法 系统设计，包括为每个优化提供量化支持的结果 相关工作 详细的端到端评估 总结 2. 提升树（Tree Boosting）简介 首先需要了解CART（Classification And Regression Tree）算法，对于cart分类树和回归树分别采用了：Gini系数、和方差度量方式来划分节点[1]。例如回归树，对于划分特征A, 划分点s使两边数据集D1和D2,求出使 D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小。 $$\\underline{min}_{\\text{A,s}}[\\underline{min}_{\\text{c1}}\\sum_{x_i\\in{D1(A,s)}}(y_i - c1)^2+\\underline{min}_{\\text{c2}}\\sum_{x_i\\in{D2(A,s)}}(y_i - c2)^2]$$ 其中，c1为D1的样本输出均值，c2为D2的样本输出均值, 回归树采用叶子节点的均值或者中位数来预测输出结果，$y_i$即样本的label，此时的输出值即下文中用到的$w_{q(x)}$。\n2.1 正则化目标函数 对于这种类型的集成树模型，用K棵树的结果来预测最后的结果（式1），那么问题来了我们怎么来求这些树的参数，每棵树都可以看做一个函数$f_i$包含树的结构以及最后叶节点权重，集成模型不像传统优化问题一样通过简单用梯度下降可以对所有的树进行学习求解，所以，在这里用到了加法策略，即固定已经学习到的，每次加一棵树来进行学习（式3）。 $$\\hat{y_i} = \\phi(x_i) = \\sum_{k = 1}^{K} f_k(x_i)\\tag1$$ 其中$f(x) = w_{q(x)}$，每个$f_k$对应一个独立的树结构q以及其叶节点权重w，为了学习模型中的参数，最小化下面正则化的目标函数。 $$L(\\phi) = \\sum_i l(\\hat{y_i}, y_i) + \\sum_k \\Omega(f_k)\\tag2$$\n$$\\Omega(f_k) = \\gamma T + \\frac{1}{2}\\lambda||w||^2$$ T是树的叶节点数\n2.2 梯度提升树（Gradient Tree Boosting） 第t次预测值等于t-1加上第t棵树的结果 $$\\hat{y_i} = \\hat{y_i}^{t-1} + f_t(x_i)\\tag3$$ 此时目标函数(式2)可以写成 $$L^{(t)} = \\sum_{i=1} ^n l(y_i, \\hat{y_i}^{(t-1)}+f_t(x_i)) + \\Omega(f_t)\\tag4$$ 该式子的二阶近似可以表达为(式5），可以参考补充中二阶泰勒展开的一般形式 $$L^{(t)} \\approx \\sum_{i=1} ^n [l(y_i, \\hat{y_i}^{(t-1)}) + g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)\\tag5$$ 其中 $g_i=\\partial_{\\hat{y}^{(t-1)}}{l(y_i, \\hat{y_i}^{(t-1)})}， h_i=\\partial_{\\hat{y}^{(t-1)}}^2{l(y_i, \\hat{y_i}^{(t-1)})}$\n式5中去掉常数项，即label与第t-1次结果的损失函数，可以得到：\n$$\\sum_{i=1}^n[g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)\\tag6$$\n式6即对新树的优化目标函数，进一步合并可以写为：\n$$obj^{t} \\approx \\sum_{i=1}^{n}[g_i w_{q(x_i)} + \\frac{1}{2}h_i w^2_{q(x_i)}] + \\gamma T+ \\frac{1}{2}\\lambda \\sum _{j=1}^T w_j^2$$\n定义$I_j = {i|q(x_i)=j}$即叶节点j上的实例。\n$$obj^{t} \\approx \\sum_{j=1}^T[(\\sum_{i\\in{I_j}}g_i)w_j+\\frac{1}{2}(\\sum_{i\\in{I_j}}h_i+\\lambda)w_j^2]+\\gamma T\\tag7$$\n上式7对$w_j$求导即可求出w最优值\n$$w_j^* = -\\frac{\\sum_{i\\in{I_j}}g_i}{\\sum_{i\\in{I_j}}h_i+\\lambda} $$\n令$G_j = \\sum_{i\\in{I_j}}g_i，H_j=\\sum_{i\\in{I_j}}h_i$\n此时，对应的最小值为: $$obj^* = -\\frac{1}{2}\\sum_{j=1}^T\\frac{G_j^2}{H_j+\\lambda}+\\gamma T$$ $\\color{red}\\frac{G_j^2}{H_j+\\lambda}$越大，loss越小，所以对叶节点进行分裂，分裂后增益定义为 $$Gain=\\frac{1}{2}[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}]-\\gamma\\tag8$$\n2.3 缩减和列抽样（Shrinkage and Column Subsampling） 除了在目标函数中引入正则项，在防止过拟合方面xgboost还运用了两项技术，给每一步tree boosting得到的结果一个权重$\\eta$，来降低每一步的影响从而给后面树的形成留下空间，比喻成优化问题中的学习率缩减；同时还用到随机森林中的列抽样，即随机特征筛选。\n3. 分裂点寻找算法 3.1 精确贪婪算法（Basic Exact Greedy Algorithm） 即按照2.2中式8来寻找分裂点 pythonscikit-learn，Rgbm，单机的xgboost都支持。\n3.2 近似算法（Approximate Algorithm） 精确贪婪算法由于列举了所有可能的分裂点，在数据量很大不能全部写入内存时会导致不是那么高效。所以提出近似算法。对于每个特征，只考察分位点，减少计算复杂度。 近似算法存在两个变种：\nglobal: 学习每棵树前，提出候选分裂点 local: 每次分裂前，重新提出候选分裂点 3.3 加权分位数图（Weighted Quantile Sketch） 近似算法中最重要一点即提出候选分裂点，xgboost不是简单的按照样本个体进行分位，而是以损失函数二阶导数值作为权重进行分位数分裂。如何寻找二阶导数分位点，首先是利用权重计算排序函数，然后相邻相减值作为判断依据。问题是为什么会想到利用损失函数二阶导数值作为权重来划分。 文中给出式6可以变形为 $$\\sum_{i=1}^n\\frac{1}{2}h_i(f_t(x_i)-g_i/h_i)^2 + \\Omega(f_t) + constant\\tag9$$ 指出该式恰好是权重平方差损失函数，权重$h_i$以及label $g_i/h_i$ 自己从式6变不到式9，觉得中间符号是+还差不多。 看有人理解说变成式10才对。是否作者真的是这样想的，不得而知。欢迎指正。 $$\\sum_{i=1}^n\\frac{1}{2}h_i(f_t(x_i)-(-g_i/h_i))^2 + \\Omega(f_t) + constant\\tag{10}$$ stackexchange上关于理解xgboost近似分裂点\n3.4 稀疏值感知分裂（Sparsity-aware split finding） 造成稀疏值的原因：1）缺失值 2）统计过程中频繁的0值输入 3）one-hot编码以及其他特征工程 所以让算法注意数据中稀疏规律很重要，遍历所有特征，在划分子节点时，统一将该特征的缺失值划分到右支或者左支，计算最大的gain。\n$\\color{red}这里也有个疑问就是为什么排序第一次是升序，第二次是降序$\n4. 系统设计 4.1 分块并行（Column Block for Parallel Learning） 基于树学习过程中最耗时的是将数据排序，为了减少排序的时间成本，提出基于内存的block结构。\n在Exact greedy算法中，将整个数据集存放在一个Block中 在近似算法中，使用多个Block，每个Block对应原来数据的子集。不同的Block可以在不同的机器上并行计算 4.2 缓存优化 这里指利用CPU缓存对算法进行优化。\n4.1中column block按特征大小顺序存储，相应的样本的梯度信息是分散的，造成内存的不连续访问，降低CPU cache命中率。 优化方法：\n对于精确贪婪算法，预取数据到buffer中（非连续-\u0026gt;连续），再统计梯度信息。 对于近似算法，调节block的大小，设置过大则容易导致命中率低，过小则容易导致并行化效率不高。 4.3 外存计算 除了处理器以及内存，利用磁盘空间来处理不能进入内存的数据也十分重要，数据划分为多个Block并存放在磁盘上。计算的时候，使用独立的线程预先将Block放入主内存，因此可以在计算的同时读取磁盘。在减少计算资源开销以及提高磁盘输入输出方面主要用到以下技术：\nBlock压缩，按列压缩，加载到主内存时由独立线程动态解压缩。具体压缩技术参看原文。 Block Sharding，将数据划分到不同硬盘上，提高磁盘吞吐率。 5. 端到端评估 利用4个数据集对xgboost评估：\n分类问题 排序问题 外存计算实验 分布计算实验 这几个方面进行评估，详细结果见论文。\nref CART分类树与回归树 Markdown数学公式 Mathjax应用在网页 XGBoost.ppt readthedocs xgboost tutorials推荐 gbdt.ppt xgboost原文 补充 文中很多术语翻译可能有不恰当的地方，欢迎指出。 二阶泰勒展开的一般形式： $$f(x^t) = f(x^{t-1}+\\Delta x)\\approx{f(x^{t-1})+ f^{\\prime}(x^{t-1})\\Delta{x}+f^{\\prime\\prime}(x^{t-1})\\frac{\\Delta x^2}{2}}$$ 式4中加入loss function是mean squared error(MSE)，可以求出相应的gi， hi作为一个特例来验证该做法。 基于树的算法理解时带着这几个问题去理解每一步是用来做什么的：选择哪个特征进行分裂？在特征什么点位进行分裂？分裂后叶节点取什么值？ 分别对应：遍历每个特征，加权分位数图，$w_j$\n对于系统设计中应用到的技术理解不是十分深刻，对应一个算法如何从计算机硬件的方方面面考虑去优化对非专业领域研究者还是比较难 ","date":"2020-03-23T16:18:13Z","image":"https://jmwyf.github.io/images/xgboost/algorithm1.jpg","permalink":"https://jmwyf.github.io/p/%E9%87%8D%E8%AF%BBxgboost/","title":"重读XGBoost"},{"content":"LSTM简介以及pytorch实例 在去年介绍的一篇paper中，应用了多任务RNN来解决问题，当时RNN指的即是LSTM。本文介绍LSTM实现以及应用。\n1. LSTM简介 循环神经网络要点在于可以将上一时刻的信息传递给下一时刻，但是在需要长程信息依赖的场景，训练一个好的RNN十分困难，存在梯度爆炸和梯度消失的情况。LSTM通过刻意的设计来解决该问题。\n简单的RNN网络中重复的模块只有一个简单的结构，例如一个relu层，而在LSTM中重复的模块拥有4个不同的结构相互交互来完成。\n1.1 首先决定从cell中丢弃什么信息 $$f_t = \\sigma(W_f*[h_{t-1}, X_t] + b_f) \\tag1$$ sigma函数在0到1选择代表丢弃与否\n1.2 什么样的新信息存放到cell中 $$i_t = \\sigma(W_i*[h_{t-1}, x_t] + b_i) \\tag2$$\n$$\\widetilde{C_t} = tanh(W_c*[h_{t-1}, x_t] + b_c) \\tag3$$\n$$C_t = f_t*C_{t-1} + {i_t} * \\widetilde{C_{t}} \\tag4$$\n4式中旧状态与$f_t$相乘，丢弃确定需要丢弃的信息，加上新的候选值。可以看到假如遗忘门一直为1，就可以保持以前的信息$C_{t-1}$\n1.3 输出结果 $$o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o)\\tag5$$ $$h_t = o_t*tanh(C_t)\\tag6$$\n2. LSTM实例以及Pytorch实现 循环神经网络可以应用到以下场景。\n点对点（单个图片（文字）被分类；图像分类） 点对序列（单个图像（文字）被分为多个类；图像输出文字） 序列分析（一系列图片（文字）被分类；情感分析） 不等长序列对序列（机器翻译） 等长序列对序列（视频帧分类） 举两个例子：图像分类以及时间序列预测\n2.1 LSTM图像分类 关于图片分类常用卷积神经网络，侧重空间上处理；而循环神经网络侧重序列处理。但是也能用来图片分类。第一个例子以常用的mnist手写字体识别为例。\n2.1.1 导入所需用到的包以及超参数设置等 1 2 3 4 5 6 7 8 9 10 11 # Setup import torch from torch import nn from torch.utils.data import DataLoader import torchvision.datasets as dsets import torchvision.transforms as transforms torch.manual_seed(1) # Device configuration device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) 2.1.2 导入数据集 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Mnist手写数字 train_data = dsets.MNIST(root=\u0026#39;./mnist/\u0026#39;, # 保存或者提取位置 train=True, # this is tra`ining data transform=transforms.ToTensor(), # 转换 PIL.Image or numpy.ndarray 成 # torch.FloatTensor (C x H x W), 训练的时候 normalize 成 [0.0, 1.0] 区间 download=True, # 没下载就下载, 下载了就不用再下了改成False ) test_data = dsets.MNIST(root=\u0026#39;./mnist/\u0026#39;, train=False, transform=transforms.ToTensor()) # Dataloader # PyTorch中数据读取的一个重要接口，该接口定义在dataloader.py中，只要是用PyTorch来训练模型基本都会用到该接口（除非用户重写…）， # 该接口的目的：将自定义的Dataset根据batch size大小、是否shuffle等封装成一个Batch Size大小的Tensor，用于后面的训练。 train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) # 在每个epoch开始的时候，对数据重新打乱进行训练。在这里其实没啥用，因为只训练了一次 test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False) 2.1.3 建立模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # LSTM # __init__ is basically a function which will \u0026#34;initialize\u0026#34;/\u0026#34;activate\u0026#34; the properties of the class for a specific object # self represents that object which will inherit those properties class simpleLSTM(nn.Module): def __init__(self, input_size, hidden_size, num_layers, num_classes): super(simpleLSTM, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) self.fc = nn.Linear(hidden_size, num_classes) def forward(self, x): # x shape (batch, time_step, input_size) # out shape (batch, time_step, output_size) # h_n shape (n_layers, batch, hidden_size) # h_c shape (n_layers, batch, hidden_size) # 初始化hidden和memory cell参数 h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # forward propagate lstm out, (h_n, h_c) = self.lstm(x, (h0, c0)) # 选取最后一个时刻的输出 out = self.fc(out[:, -1, :]) return out model = simpleLSTM(input_size, hidden_size, num_layers, num_classes) # loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr) 2.1.4 训练模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # train the model # 关于reshape(-1)的解释 https://www.zhihu.com/question/52684594 # view()和reshape()区别的解释 https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch # Hyper Parameters epochs = 1 # 训练整批数据多少次, 为了节约时间, 我们只训练一次 batch_size = 64 time_step = 28 # rnn 时间步数 / 图片高度 input_size = 28 # rnn 每步输入值 / 图片每行像素 hidden_size = 64 num_layers = 1 num_classes = 10 lr = 0.01 # learning rate total_step = len(train_loader) for epoch in range(epochs): for i, (images, labels) in enumerate(train_loader): images = images.reshape(-1, time_step, input_size).to(device) labels = labels.to(device) # forward pass outputs = model(images) loss = criterion(outputs, labels) # backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if i % 100 == 0: print(\u0026#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\u0026#39; .format(epoch+1, epochs, i+1, total_step, loss.item())) 2.1.5 测试模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Test the model # https://stackoverflow.com/questions/55627780/evaluating-pytorch-models-with-torch-no-grad-vs-model-eval # torch.max()用法。https://blog.csdn.net/weixin_43255962/article/details/84402586 model.eval() with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, time_step, input_size).to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(\u0026#39;Test Accuracy of the model on the 10000 test images: {} %\u0026#39;.format(100 * correct / total)) 2.2 时间序列预测 Todo\n2.3 图像输出文字 Todo\n补充 在原始发表文献用的图示是类似于下图的这种，看起来比较好容易理解当初形成LSTM的原因 pytorch lstm函数用法示例\n1 2 3 4 5 rnn = nn.LSTM(10, 20, 2) # input_size, hidden_size, num_layers input = torch.randn(5, 3, 10) # time_step, batch, input_size（这里input_size即features） h0 = torch.randn(2, 3, 20) # num_layers, batch, hidden_size c0 = torch.randn(2, 3, 20) # num_layers, batch, hidden_size output, (hn, cn) = rnn(input, (h0, c0)) # output包含从最后一层lstm中输出的ht。shape: time_step, batch, hidden_size hidden_size is the number of units of your LSTM cell. This means all the layers (input, forget, etc.) will have this size\nhidden_size即pytorch隐含层每个结构中含有的隐含cell数目\nlstm函数中加入bidirectional=True参数即双向神经网络 Reference 理解LSTM(http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 高效RNN(http://karpathy.github.io/2015/05/21/rnn-effectiveness/) Hochreiter \u0026amp; Schmidhuber (1997) LSTM Pytorch LSTM官方文档(https://pytorch.org/docs/stable/nn.html#lstm) ","date":"2020-03-09T15:07:13Z","image":"https://jmwyf.github.io/images/LSTM/LSTM.jpg","permalink":"https://jmwyf.github.io/p/lstm%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E4%BB%A5%E5%8F%8Apytorch%E5%AE%9E%E4%BE%8B/","title":"LSTM应用场景以及pytorch实例"},{"content":"PyTorch深度学习（1） 1. History, motivation and evolution of Deep Learning 科学技术发展如海浪一样也会潮起潮落，深度学习在经历了几次低谷后。2010年左右，在语音识别领域取得进展，2012年在计算机视觉领域也发展起来，随后各个领域都开始使用应用深度学习方法，而似乎渐渐抛弃了其他方法，那么深度学习是不是问题的最终解决之道呢？研究方向宽泛而多维才是合理的道路，不应过分追求热点领域。正如上世纪80年代日本学者在低谷时期仍然坚持自己的研究领域。\n学习表征：如何学习好的表征是深度学习要解决的问题之一，原始数据以一种有用的形式返回。自然状态下数据相互依赖有关系的。高效的表达方式应该是每类数据都是完全独立能完全单独表达某个方面。\nspace tiling random projections polynomial classifier radial basis functions kernel machines 2. Gradient Descent and Backpropagation 2.1 Gradient Descent $$J(w, b) = \\frac{1}{m}\\sum_1^mL(\\hat{y}^{(i)}, y^{i})$$ J(w, b)为问题的cost function即目标函数，即m个样本的损失函数平均值。使目标函数最小得到此时w,b参数是我们的优化问题。\n2.1.1 梯度下降(batch gradient descent) 梯度下降即上式对所有样本计算求出目标函数，通过对w,b求梯度来找到目标函数最小值，常用的一个比喻即找最快路径下山。数学理解是算法实现的重要一步，但与在计算机上实现还是有区别的，那么实际做法是什么样的呢？\n当你对复杂的问题想不清楚时，我们都可以从一个简单的例子出发来简化问题，对于这个问题考虑只有一个样本时，我们怎么编程实现呢？对w1、 b1，计算一个样本的loss然后对w1、b1求导优化思路很清晰，那么有m个样本的时候呢？只需将其他样本计算loss，然后对w1、b1求导相加。最后在通过学习率来更新w、b。可以看到每次更新都需要进行m次运算\n2.1.2 小样本梯度下降（mini-batch gradient descent） 在每次更新时用n个样本，不用全部的样本。在深度学习中常用这种方法。用mini-batch可以享受向量化带来的便利，也不用全梯度下降那么大计算量，同时这也是应对冗余数据的一种方法。\n2.1.3 随机梯度下降(stochastic gradient descent) 当n = 1的时候，每次更新的时候用1个样本。该方法在大多数情况下比全样本的梯度下降要快。\n三种优化方法最后收敛吗？最后能达到全局最小值吗？这是优化方法都需要考虑到的。可以阅读Optimization Methods for Large-Scale Machine Learning，我自己还没读过\u0026hellip;\n2.2 Backprop 反向传播是为了求梯度用到的微积分链式法则，从而使梯度下降算法运行。\n2.3 PyTorch训练神经网络步骤 output = model(input) 即神经网络前向传播 J = loss(output, label) 计算cost function model.zero_grad() 清除梯度计算 J.backward() 对requires_grad = True的变量计算梯度 optimiser.step() 进行梯度下降 3. 总结 看了前两节，觉得还是吴恩达大佬讲的好一些。建议网页上快速过内容即可，视频不用细看。\n","date":"2020-03-05T11:50:19Z","image":"https://jmwyf.github.io/images/pytorch/pytorch.jpeg","permalink":"https://jmwyf.github.io/p/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/","title":"PyTorch深度学习（1）"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $$ y = x_t $$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2019-03-08T00:00:00Z","permalink":"https://jmwyf.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"以下操作基于macOS，Windows仅供参考。\ngit初始化文件夹 进入目录\n1 git init 新建.gitignore 然后在其中加入需要忽略的文件或文件夹.gitignore 例如public\\\ngit删除远程分支文件 当我们需要删除暂存区或分支上的文件, 同时工作区也不需要这个文件了, 可以使用 git rm file_path\n当我们需要删除暂存区或分支上的文件, 但本地又需要使用, 只是不希望这个文件被版本控制, 可以使用 git rm –cached file_path\n所以我们经常使用以下命令来删除git中的文件\n1 2 3 git rm -r --cached filename git commit -m \u0026#39;delete some file\u0026#39; git push origin master git删除.DS_Store文件 从该仓库中删除已存在的DS_Store文件 1 find . -name .DS_Store -print0 | xargs -0 git rm -f --ignore-unmatch 新建.gitignore_global文件并将.DS_Store以及*/.DS_Store加入其中 1 2 3 4 vi .gitignore_global .DS_Store */.DS_Store git config --global core.excludesfile ~/.gitignore_global 推到仓库 1 2 git add .gitignore git commit -m \u0026#39;.DS_Store banished!\u0026#39; 检查仓库中是否还有 1 git status git冲突处理 git远程分支修改，本地也修改了准备提交出现冲突\n先拉在推 1 2 git pull --rebase #检查合并是否冲突 git push -u origin master 强制按本地更新 1 git push -f git子模块（submodule） 对于公共资源或者常用的代码，你可能会把最新版本逐个复制到N个项目中，如果使用了submodule模块，那么只需要在各个项目中\n1 git submodule update 进入子模块目录正常操作即可\ngit多账户切换 使用参考2中删除keychain access的方法，比较简单\nref git删除.DS_Store\nstackoverflow\ngit多账户切换\n","date":"2019-03-04T22:22:22Z","permalink":"https://jmwyf.github.io/p/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"git常用命令"}]