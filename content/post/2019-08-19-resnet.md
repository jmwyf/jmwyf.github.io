---
title: "ResNet导读要点解析"
author: "Yong"
date: '2019-08-19'
categories: Data Science
description: '风靡一时的残差网络简介'
tags: Deep Learning
featured_image: "/images/Resnet_kaiminghe2015/resnet_block_result.jpg"
---


# ResNet导读

本文主要详细介绍[ResNet](https://arxiv.org/abs/1512.03385)。

## 引言

Resnet提出主要是为了解决**在不断增加神经网络的深度时，会出现一个Degradation（退化）的问题，即准确率会先上升然后达到饱和，再持续增加深度则会导致准确率下降。这并不是过拟合的问题，因为不光在测试集上误差增大，训练集本身误差也会增大**。
（例如现在有个浅层网络,最后输出为y,已经是一个准确率较高的网络，现在在该浅层网络上继续增加层数得到一个深层网络，想让输出仍为y,即后面增加的网络层是恒等映射，而退化问题就说明对于神经网络来说恒等映射是不容易优化的）

## 深度残差学习（Deep Residual Learning）

* 残差学习（Residual Learning）

  $H(x) = F(x) + x$

  把恒等输入作为网络的一部分，其中H(x)为模型潜在的映射，x为输入

  $F(x) = H(x) - x$

  F(x)为残差函数

* 通过捷径（Shortcuts）进行恒等映射

  ![residual block](/images/Resnet_kaiminghe2015/resnet_block_result.jpg)

  $y = F(x,W_{i}) + x$
  
  每个元素对应相加（element-wise addition）

  其中$F(x, W_{i})$为残差，$F = W_2\sigma(W_1x)$（图中会让人产生一种F(x)是在两层中间的错觉，其实是在两层之后）

* 网络结构
  
1. 文献中图3左为VGG-19, 卷积层均使用same padding，输出大小变化发生在max pooling, stride = 2。
  $$(n+2p-f+1)/s$$

2. 构建Plain networks来复现退化现象
3. 残差网络中虚线表示通道数不同，使用 
   
   $H(x) = F(x) + Wx$ W用来调整维度
   
   对于更深层的残差网络使用了Bottleneck结构，降低参数量。

主要介绍一下图1，在深度学习论文中会经常看到类似图1的表。  
 ![residual architectures](/images/Resnet_kaiminghe2015/resnet_architectures_result.jpg)
第一列给每个卷积层取的名字，第2列为输出尺寸，例如对于第一层7*7的卷积核，步长为2，文中给出第一层卷积输出结果为112可以得到padding = 3

$$
(224-7+1+2*3)\/2 = 112
$$

以18层resnet为例，然后是2组残差网络基本模块，两组64个3*3卷积核，在将卷积核翻倍的同时stride = 2, padding = 1到聪conv3_x;而在单个残差模块里面stride = 1, padding = 1

## 结果
结果部分对比了模型参数，性能
- top-1 error
- top-5 error 

**top-N正确率是指图像识别算法给出前N个答案中有一个是正确的概率**

## 补充材料
resnet pytorch实现[代码](https://github.com/pytorch/vision/blob/0bd7080c6fa4d8a07f34aa66caf10683824e5b82/torchvision/models/resnet.py#L35)


