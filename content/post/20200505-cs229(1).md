---
title: "cs229笔记"
date: 2020-05-05 10:56:59
description: "cs229是吴恩达讲授的机器学习课程，学过机器学习的人应该比较了解，重新温习一遍公开的18年版本，此前还是08年的版本"
tags: machine learning
featured_image: ""
categories: Study
comment : true
draft: True
---

# cs229笔记
## Lecture1
## 开场
Andrew在讲课的时候手上拿着讲义，显得很自然。  
建议在组成学习小组共同学习2-3人。
## ML定义
赋予计算机没有明确的编程下学习的能力（用英文表述可能更好一点）
> Field of study that gives computers the ability to learn without being explicitly programmed.
## 课程涵盖的5个部分
### 有监督学习
  - 回归
  - 分类
### 机器学习技巧：
系统工程流程（systemic engineering process）
### Deep learning
cs230
### 无监督学习
cocktail party problem: 分离混杂在一起的声音  
ICA算法
### 强化学习
喂狗  
直升机控制

## Lecture2
### Linear regression  
提出了hypothesis的概念，即假设模型应该是怎么样
### Batch/stochastic gradient desent
### Normal equation  
仅限于线性回归
$$\nabla_\theta J(\theta) = 0$$

## Lecture3
Recap环节
### Locally weighted regression
* parametric algorithm: 对于任何大小的数据来说都是固定的参数量
* non-parametric algorithm: 数据（例如参数）的大小会随着数据集的大小而增长

这个解释不同于我看到的其他解释但是确实好理解，以前看到的都是参数模型即有没有对数据分布做相关的假设

$$w^{(i)}=e^{(-\frac{x^{(i)}-x}{\tau^2})}$$
$$\sum_{i=1}^m w^{(i)}(y^{(i)}-\theta^Tx^{(i)})$$
局部权重拟合：给近的样本大的权重，远的样本约等于0的权重，$\tau$决定了远近的超参数.

### Probabilistic interpretation
关于损失函数中为什么是平方的问题？

IID：independently and identity distribution 常说的独立同分布  
区分likelihood of parameters and probability of the data：
参数固定的情况即概率

MLE: maximum likelihood estimation  

从概率论的方法来解释lecture2中定义损失函数为什么是平方，而不是1次方或者4次方：
- 首先假设label和预测值之间存在一个误差，该误差是符合高斯分布的（为什么符合，后面解释是中心极限定理）
- 那么可以得到：
$$P(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})}$$
- 然后根据IID，得到所有样本的likelihood, 之后取log, 并用MLE法则即可得到结果

### Logistic regression
需要预测值在[0,1]  
$$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$
分别给出label在0，1下的概率，然后综合成一个式子
$$P(y|x;\theta)=h(x)^y(1-h(x))^{1-y}$$
求所有样本的likelihood,同上log后应用MLE,然后使用梯度上升的方法求参数最优值。比较有意思的是最后的形式和线性回归一样。

### Newton's method
想解决的问题是找到某个$\theta$, 使得$f(\theta)=0$  
可以通过简单的的图示，得到：
$$\theta^{t+1}=\theta^{t}-\frac{f(\theta^{0})}{f'(\theta^{0})}$$
而我们要求的是likelihood导数为0的地方，故上式中$f(\theta)=L'(\theta)$

当$\theta$为向量的时候：
$$\theta^{t+1}=\theta^{t}+H^{-1}\nabla_{\theta}l$$
- hessian matrix:$h_{ij} = \frac{\partial{l}}{\partial{\theta_i}\partial{\theta_j}}$

当参数的维度特别高时，计算量巨大，比如10000*10000的矩阵的转置

## Lecture4
这节换了人讲，不是Andrew大佬失望，不想听。
### Perceptron
和logistic Regression类似，将g(x)换成分段函数即可。
可以根据最后的优化函数，可视化整个决策过程，画出决策边界的变化。

从概率上无法解释。
- [ ] logistics回归的概率解释（伯努利分布）
### Exponential Family
PDF:
$$p(y,\eta)=b(y)exp[\eta^TT(y)-a(\eta)]$$
其中:
$\eta$ - natural parameter  
T(y) - sufficient statistics  
b(y) - base measure  
$a(\eta)$ - log partition

在这个部分可以看到伯努利分布，高斯分布都是指数族的特例

### Generalized Linear Models（GLM）
1. $y|x;\theta \sim ExpFamily(\eta)$
2. 给定x，目标是求E(T(y)|x)，即h(x)=E(T(y)|x)
3. $\eta=\theta^Tx$

### Softmax Regression
Multinomial 多项分布（二项分布推广到多种状态）  
适用于多分类问题  

### 小结 
- [ ] 知道结果后用起来很简单，但还是比较不容易理解的，需要上手推导一下整个过程。

## Lecture5
先大概介绍了discriminative model和generative learning model  
- discriminative model试图学习从x$\rightarrow$y的映射关系：p(y|x)
- generative model则试图学习每个y所对应的特征应该是什么样：p(x|y)
 
### Gaussian discriminant algorithm (GDA)
$$argmax_yP(y|x)=argmax_yp(x|y)p(y)$$
图示对比LR和GDA

### Generative vs Discriminate algorithm
以LR和GDA为例，可以看到从GDA的假设中可以导出LR的假设，将LR的假设称为弱假设，而GDA假设为强假设，这里前提是x|y=1是符合高斯分布的。

弱假设得到模型更具鲁棒性。

模型获得知识的来源有两部分，一我们告诉模型的，二模型自己从数据中获得的，数据够大的情况下建议用弱假设从数据中获得知识。

- [ ] 讨论样本量的问题，在数据量少的时候该怎么做？

机器学习的技巧以及你告诉模型的知识就比较重要了

### Naive Bayes
- Feature vector  
- Naive Bayes (NB) assumption
- MLE

### 小结
- [ ] GDA和NB的参数推导需要推一下

## Lecture6
Recap: 复习了naive bayes

### Naive Bayes
- Laplace smoothing  
为了解决样本中没有出现某种情况的方法
- Event models  
解决有些词出现多次的例子

- [ ] word embedding(cs224, cs230)

### SVM intros
- 非线性决策边界  
- just need to turn the key
- kernels: 2维的特征扩充到多维
- inseparable case
- [ ] factor analysis

## Lecture7

### optimization problem

- [ ] represented theorem

### kernal tricks


### 小结
- [ ] svm推导

## Lecture8

### Bias and Variance
hard to master

### Regularization

### Train/dev/test
根据需要得到的精度来选择dev&test的样本，如果需要的精度越高，可以加大样本

### Model selection and cross validation
* k-fold CV: 当数据集较小的时候  
* leave-one-out CV (m <100)

### Feature selection
* avoid overfitting
依次加特征，看dev set中性能程度，即forward & backward

## Lecture9
换人讲了

### Bias and variance
进一步讨论bias and variance，从data view和parameter view。

如何解决variance?
- increase M 
- regularization: add bias and low variance

如何解决high bias
- make H bigger: hypothesis学习样本、学习参数都可以使其变化

### EMR: empirical risk minimize
- [ ] ?

### VC dimension


## Lecture10
换成一个小哥，还行

### Decision Tree
画图很直观的给出决策树的决策边界。  

* cross-entropy
* Gini 

Regression tree  

regularization of DTs
* min leaf size
* max depth
* max number of nodes
* min decrease in loss
* pruning: 

### Ensemble Methods
1. different algorithms
2. different training sets

### Bagging
bootstrap aggregation

### Random Forests
decrease variance 

decorrelate models

### Boosting
decrease bias

### 小结
- [ ] adaboost理解

## Lecture11
依旧不是Ng

### Logistic regression
neuron = linear + activation
model = architecture + parameters

在猫狗问题上假如要预测猫的年龄该如何做？  
将activation换为relu。然后要考虑的即loss function该如何选择等。

### Neural Networks
* propagation equation  
* broadcasting
* cost function
* backward propagation

## Lecture12
依旧不是Ng

### backpropagation
$$J(\hat{y},y)= \frac{1}{m}\sum_{i=1}^{m}L^{(i)}(\hat{y},y)$$
with `cross entropy`:
$$L^{(i)}=-[y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)})]$$

### improve NNs
* different activation function 
* initialization
  - normalizing your input(loss function can converge quick; test set need to use same u and $\sigma$, because net is trained by that transformation)
* optimization: batch
* momentum algorithim

### 小结
- [ ] 反向传播推导严格，关于matrix shape

## Lecture13
Ng回归了

* art -> science
* diagnostics 

### Debugging the learning algorithm
- bias vs variance diagnose

- [ ] bayes error

### error analysis

## Lecture14

unsupervised learning

### k-means cluster
* initilize cluster centroids
* repeat until convergence
(a) 找距离j中心最近的点
(b) 算平均值

how to choose k? 

### density estimation
model p(x), then p(x)< a -> anomely

### mixed guassian model
lantent variables






## ref
1. [bilibili课程视频地址](https://www.bilibili.com/video/BV1Up4y1X7t1)
2. [cs229课程表](http://cs229.stanford.edu/syllabus.html)