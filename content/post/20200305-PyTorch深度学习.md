---
title: "PyTorch深度学习（1）"
date: 2020-03-05T11:50:19
description: "本文是NYC深度学习课程学习笔记第一二节"
tags: [pytorch]
image: "/images/pytorch/pytorch.jpeg"
categories: Deep Learning
---

# PyTorch深度学习（1）
## 1. History, motivation and evolution of Deep Learning
科学技术发展如海浪一样也会潮起潮落，深度学习在经历了几次低谷后。2010年左右，在语音识别领域取得进展，2012年在计算机视觉领域也发展起来，随后各个领域都开始使用应用深度学习方法，而似乎渐渐抛弃了其他方法，那么深度学习是不是问题的最终解决之道呢？研究方向宽泛而多维才是合理的道路，不应过分追求热点领域。正如上世纪80年代日本学者在低谷时期仍然坚持自己的研究领域。 

学习表征：如何学习好的表征是深度学习要解决的问题之一，原始数据以一种有用的形式返回。自然状态下数据相互依赖有关系的。高效的表达方式应该是每类数据都是完全独立能完全单独表达某个方面。
* space tiling
* random projections
* polynomial classifier
* radial basis functions
* kernel machines

## 2. Gradient Descent and Backpropagation
### 2.1 Gradient Descent
$$J(w, b) = \frac{1}{m}\sum\_1^mL(\hat{y}^{(i)}, y^{i})$$
J(w, b)为问题的cost function即目标函数，即m个样本的损失函数平均值。使目标函数最小得到此时w,b参数是我们的优化问题。

#### 2.1.1 梯度下降(batch gradient descent)
梯度下降即上式对所有样本计算求出目标函数，通过对w,b求梯度来找到目标函数最小值，常用的一个比喻即找最快路径下山。数学理解是算法实现的重要一步，但与在计算机上实现还是有区别的，那么实际做法是什么样的呢？

当你对复杂的问题想不清楚时，我们都可以从一个简单的例子出发来简化问题，对于这个问题考虑只有一个样本时，我们怎么编程实现呢？对w1、 b1，计算一个样本的loss然后对w1、b1求导优化思路很清晰，那么有m个样本的时候呢？只需将其他样本计算loss，然后对w1、b1求导相加。最后在通过学习率来更新w、b。可以看到每次更新都需要进行m次运算

#### 2.1.2 小样本梯度下降（mini-batch gradient descent）
在每次更新时用n个样本，不用全部的样本。在深度学习中常用这种方法。用mini-batch可以享受向量化带来的便利，也不用全梯度下降那么大计算量，同时这也是应对冗余数据的一种方法。

#### 2.1.3 随机梯度下降(stochastic gradient descent)
当n = 1的时候，每次更新的时候用1个样本。该方法在大多数情况下比全样本的梯度下降要快。

三种优化方法最后收敛吗？最后能达到全局最小值吗？这是优化方法都需要考虑到的。可以阅读Optimization Methods for Large-Scale Machine Learning，我自己还没读过...

### 2.2 Backprop
反向传播是为了求梯度用到的微积分链式法则，从而使梯度下降算法运行。

### 2.3 PyTorch训练神经网络步骤
1. output = model(input) 即神经网络前向传播
2. J = loss(output, label) 计算cost function
3. model.zero_grad() 清除梯度计算
4. J.backward() 对requires_grad = True的变量计算梯度
5. optimiser.step() 进行梯度下降

## 3. 总结
看了前两节，觉得还是吴恩达大佬讲的好一些。建议网页上快速过内容即可，视频不用细看。

