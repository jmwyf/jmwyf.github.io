---
title: "Pytorch LSTM时间序列分析"
date: 2020-03-17 12:44:45
description: "LSTM时间序列分析"
tags: [LSTM, deep learning]
featured_image: "/images/LSTM_timeseries/output_20_0.png"
categories: Data Science
comment : true
---

# Pytorch LSTM时间序列分析（单变量）

本文内容建议在jupyter notebook中使用
## 1. 数据集


```python
import torch
from torch import nn

import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
%matplotlib inline
```


```python
sns.get_dataset_names()
```




    ['anscombe',
     'attention',
     'brain_networks',
     'car_crashes',
     'diamonds',
     'dots',
     'exercise',
     'flights',
     'fmri',
     'gammas',
     'iris',
     'mpg',
     'planets',
     'tips',
     'titanic']



以上是seaborn包中有的著名的数据集，可以随便打开一个看一下。


```python
data = sns.load_dataset("anscombe")
data.head(10)
```
|    | dataset   |   x |     y |
|---:|:----------|----:|------:|
|  0 | I         |  10 |  8.04 |
|  1 | I         |   8 |  6.95 |
|  2 | I         |  13 |  7.58 |
|  3 | I         |   9 |  8.81 |
|  4 | I         |  11 |  8.33 |
|  5 | I         |  14 |  9.96 |
|  6 | I         |   6 |  7.24 |
|  7 | I         |   4 |  4.26 |
|  8 | I         |  12 | 10.84 |
|  9 | I         |   7 |  4.82 |

这样可能看不出什么东西，但是画一幅图，大家就知道这组数据了。


```python
# or sns.FacetGrid
sns.relplot(x = "x", y = "y", data = data, col = "dataset", hue = "dataset")
```




    <seaborn.axisgrid.FacetGrid at 0x1a295c8898>




![png](/images/LSTM_timeseries/output_7_1.png)


该数据集市统计学家F.J. Anscombe构造出了四组奇特的数据。它告诉人们，在分析数据之前，描绘数据所对应的图像有多么的重要。同样可以来看看其他数据集分别是什么。本分析用到的数据集是flights。


```python
flights = sns.load_dataset("flights")
flights.head()
```
|    |   year | month     |   passengers |
|---:|-------:|:----------|-------------:|
|  0 |   1949 | January   |          112 |
|  1 |   1949 | February  |          118 |
|  2 |   1949 | March     |          132 |
|  3 |   1949 | April     |          129 |
|  4 |   1949 | May       |          121 |
|  5 |   1949 | June      |          135 |
|  6 |   1949 | July      |          148 |
|  7 |   1949 | August    |          148 |
|  8 |   1949 | September |          136 |
|  9 |   1949 | October   |          119 |


```python
plt.title('Month vs Passenger')
plt.ylabel('Total Passengers')
plt.xlabel('Months')
plt.grid(True)
plt.autoscale(axis='x',tight=True)
plt.plot(flights['passengers'])
```




    [<matplotlib.lines.Line2D at 0x1a2a3b44a8>]




![png](/images/LSTM_timeseries/output_10_1.png)



```python
flights.shape
```




    (144, 3)




```python
all_data = flights['passengers'].values.astype(float)
```


```python
def sliding_windows(data, seq_length):
    x = []
    y = []

    for i in range(len(data)-seq_length-1):
        _x = data[i:(i+seq_length)]
        _y = data[i+seq_length]
        x.append(_x)
        y.append(_y)

    return np.array(x),np.array(y)

sc = MinMaxScaler()
training_data = sc.fit_transform(all_data.reshape(-1, 1))

seq_length = 4 # 分割序列窗口，实验表明越小可以学到更多细节
x, y = sliding_windows(training_data, seq_length)

train_size = int(len(y) * 0.67)
test_size = len(y) - train_size

# 注意torch的版本，最好能了解最新版更新了哪些特性，比如0.4版本还有variable函数，更新后就合并到一个里面了
dataX = torch.Tensor(np.array(x)).float()
dataY = torch.Tensor(np.array(y)).float()

trainX = torch.tensor(np.array(x[0:train_size])).float()
trainY = torch.tensor(np.array(y[0:train_size])).float()

testX = torch.Tensor(np.array(x[train_size:len(x)])).float()
testY = torch.Tensor(np.array(y[train_size:len(y)])).float()
```

## 2. 建立模型


```python
class simpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(simpleLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        out, (h_n, h_c) = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out        
```

## 3. 训练模型


```python
epochs = 2000
input_size = 1
hidden_size = 2 
num_layers = 1
num_classes = 1
lr = 0.01

# torch.manual_seed(1) 

model = simpleLSTM(input_size, hidden_size, num_layers, num_classes)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr)

for epoch in range(epochs):
    outputs = model(trainX)
    loss = criterion(outputs, trainY)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 100 == 0:
        print("Epoch: [{}/{}], loss: {:.4f}"
              .format(epoch, epochs, loss.item()))
```

    Epoch: [0/2000], loss: 0.4666
    Epoch: [100/2000], loss: 0.0141
    Epoch: [200/2000], loss: 0.0037
    Epoch: [300/2000], loss: 0.0029
    Epoch: [400/2000], loss: 0.0021
    Epoch: [500/2000], loss: 0.0018
    Epoch: [600/2000], loss: 0.0017
    Epoch: [700/2000], loss: 0.0017
    Epoch: [800/2000], loss: 0.0017
    Epoch: [900/2000], loss: 0.0017
    Epoch: [1000/2000], loss: 0.0017
    Epoch: [1100/2000], loss: 0.0017
    Epoch: [1200/2000], loss: 0.0017
    Epoch: [1300/2000], loss: 0.0017
    Epoch: [1400/2000], loss: 0.0017
    Epoch: [1500/2000], loss: 0.0017
    Epoch: [1600/2000], loss: 0.0016
    Epoch: [1700/2000], loss: 0.0016
    Epoch: [1800/2000], loss: 0.0016
    Epoch: [1900/2000], loss: 0.0016


理解epoch https://www.quora.com/What-is-an-epoch-in-deep-learning

%whos 变量

del 删除变量

## 4. 模型验证


```python
model.eval()
with torch.no_grad():
    test_predict = model(dataX)
    data_predict = test_predict.data.numpy()
    dataY_plot = dataY.data.numpy()

    data_predict = sc.inverse_transform(data_predict)
    dataY_plot = sc.inverse_transform(dataY_plot)

    
plt.axvline(x=train_size, c='r', linestyle='--')
plt.plot(dataY_plot)
plt.plot(data_predict)
plt.suptitle('Time-Series Prediction')
plt.show()
```


![png](/images/LSTM_timeseries/output_20_0.png)

