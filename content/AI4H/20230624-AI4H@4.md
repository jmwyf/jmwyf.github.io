---
title: "医学人工智能周刊4｜如何解决医学人工智能的可解释性"
date: 2023-06-23 19:57:00
tags: [artificial intelligence, healthcare]
categories: AI4H
---

## 构建临床医生需求与开发目标之间的桥梁来解决医学人工智能可解释性
可解释人工智能（XAI）已成为AI/ML在医疗领域实践应用一种有前途的解决方法。然而，开发者和临床人员如何解释XAI，以及他们之间冲突的目标和需求不太清楚。这篇文章介绍了一项纵向多种方法（调查问卷、实验、采访）的研究结果，该研究涉及112个开发人员和临床人员共同设计用于临床决策支持系统（CDSS）的XAI解决方案。本研究确定了XAI在开发者和临床医生心智模型中3个重要的差别，包括对立的目标（模型可解释性与临床合理性），不同事实来源（数据vs患者），以及探索新知识和利用旧知识。基于上述发现，文中提出能帮助解决医疗领域XAI难题的解决方案，包括利用因果推断模型，个性化解释以及探索和利用间的平衡。本研究强调在XAI系统设计中同时考虑开发者和临床医生角度观点的重要性并且提供提高医疗领域XAI有效性和可用性的实用建议。

![](https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/XAI.png)

[Solving the explainable AI conundrum by bridging clinicians’ needs and developers’ goals | npj Digital Medicine](https://www.nature.com/articles/s41746-023-00837-4)

## 医学影像人工智能解释的现状与未来
医学影像的解释是放射科医生工作的核心任务，近年来人工智能应用逐渐参与其中。这篇文章回顾了影像人工智能模型发展及其在临床实践中应用的进展、挑战和机遇。文章讨论了AI算法帮助放射科医生完成包括检查、工作流分类以及量化的功能，也讨论了非影像科医生使用医学影像AI的趋势。文章指出了在放射学使用AI算法时泛化的核心挑战，以及对包含临床医生-AI合作、透明度以及部署后监测的验证保障措施的需求。最后，文章讨论了人工智能领域多模态LLM的快速发展，这一进展为开发通用医疗人工智能模型提供了重大机遇，这些模型可以处理各种医学影像解释任务等。

![](https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/AIinterpretation.png)

[The Current and Future State of AI Interpretation of Medical Images | NEJM](https://doi.org/10.1056/NEJMra2301725)

## 基于深度学习方法从疾病轨迹中预测胰腺癌风险
胰腺癌是一种侵袭性疾病，通常表现较晚结局较差，这意味着需要早期诊断。本研究中，将人工智能方法应用在丹麦（丹麦国家患者登记处，DNPR）六百万患者（24000胰腺癌患者病例）和美国（美国退伍军人事务部，US-VA）三百万患者（3900胰腺癌病例）的临床数据中，在临床病史的疾病代码序列中训练机器学习模型，并在逐渐增加的时间窗口中测试预测癌症发生率。对于在36个月内的发生癌症，DNPR数据最佳模型性能为AUROC=0.88，当在癌症诊断前3个月内的疾病事件从训练机中排除时，AUROC（3m）减少至0.83，1000个年龄高于50岁的高风险人群估计相对风险为59。将丹麦模型交叉验证到US-VA中有较低的性能表现（AUROC=0.71），可以通过再训练来提高模型性能。这些结果说明为存在风险升高的患者设计现实监督计划的能力，通过早期发现这种侵袭性的癌症可能有益于寿命以及生活质量。

[A deep learning algorithm to predict risk of pancreatic cancer from disease trajectories | Nature Medicine](https://www.nature.com/articles/s41591-023-02332-5)

