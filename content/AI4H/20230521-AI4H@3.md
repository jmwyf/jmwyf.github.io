---
title: "医学人工智能周刊 #3"
date: 2023-05-20 09:00:00
tags: [artificial intelligence, healthcare]
categories: AI4H
---

# 医学影像分类的自监督学习：系统综述以及实施指南
深度学习和计算机视觉的发展给医学影像分析提供了有前景的解决方案，有潜力提高医疗水平以及患者治疗效果。然而，训练深度学习模型的主流范式需要大量标注的训练数据，这对于医学影像数据管理既耗时又花费巨大。自监督学习有可能从丰富没有标签的医学数据集中学习有用的见解，为发展鲁棒性高的医学影像模型做出巨大贡献。本综述对不同自监督策略进行了描述，并对2012年到2022年间在PubMed、Scopus、ArXiv上发表的使用自监督学习进行医学影像分类对研究进行系统综述。我们综合了前期工作的知识，并且为未来**利用自监督学习建立医学影像分类模型对研究人员提供了实践指南**。

## 自监督常见技术
### 内在关系
在一些手工制定的任务上预训练模型，可以利用数据的内部结构，而无需获取额外的标签。例如图像相对关系、预测图像旋转角度。

### 生成模型
生成模型随着传统的自编码器（AE）、变分编码器（VAE）和生成式对抗网络（GANs）出现而变得流行起来，能够学习训练数据的分布，从而重建原始输入或者创建新的合成数据实例。通过使用现成的数据作为目标，生成模型能在不需要显式标签的情况下被训练用于自动学习有用的隐含表征。

### 对比学习
基于转化图像引起的变化不能改变图像语义的假设。针对相同图像不同的数据增强方法组成了所谓的正样本对，相对于该图像其他图片以及增强样本组成了负样本对。优化模型让正样本对在潜空间距离变小并与负样本距离变远。
- SimCLR
- MoCo

### 自预测
自预测SSL是对部分输入进行掩码或增强，然后用没有变化的部分来重建原始输入。自预测SSL想法来源于自然语言处理领域掩码模型。


## 微调技术
主要有两种策略用于微调已被SSL预训练的模型。如果将任意的影像模型都看成编码器和分类器两部分。两种策略能被分为
- 端到端的微调，所有权重都训练
- 固定编码器提取特征，对分类器进行微调


## 自监督医学影像实施指南
需要多种**自监督学习策略相互比较**，现有研究很少进行比较而是有无自监督学习策略比较。

在大型自然图像数据集中自监督预训练的模型也可以被利用到医学影像，但由于**医学影像的独特性**，究竟能迁移多少有待研究。
- 由于医学图像采集的标准化协议和人体解剖学的同质性，医学图像具有很高的类间视觉相似性，即不同类也很相似；
- 在医学成像领域，感兴趣的语义很少是诸如解剖器官之类的对象，而是该器官或组织内是否存在病理异常。许多异常的特征是非常微妙和局部的视觉线索，这些线索可能会由于增强变得模糊或被掩盖；
- 自预测型自监督学习方法所使用的随机掩码（通过移除有疾病或者异常的图像）可能改变医学影像的语义。

在**对比学习形成正样本对**时应该探索更多策略，而不是使用相同图片的不同增强版本，比如通过临床信息的相似性来定义正样本对。

在**自监督学习中引入多模态信息**提高下游任务模型性能。


[Self-supervised learning for medical image classification: a systematic review and implementation guidelines | npj Digital Medicine](https://www.nature.com/articles/s41746-023-00811-0)