---
title: DenseNet导读
author: Yong
date: '2019-08-29'
slug: densenet
categories: Data Science
tags: Deep Learning
lastmod: '2019-08-29T10:55:25+08:00'
description: 'DenseNet网络结构与Resnet类似，但是流行程度好像并不是很高'
featured_image: "/images/DenseNet/dense_blocks_result.jpg"
---

# DenseNet导读
在前一篇介绍了resnet之后，本篇介绍与之结构类似升级版[DenseNet](https://arxiv.org/abs/1608.06993)

## 引言（Introduction）
当CNN层数变得越来越深，输入的信息或者在网络中传递的梯度信息可能会消失或者随着层数增加而消失。已经有许多论文提出了各种各样的解决方法，例如ResNet前一层与下一层网络间的恒等连接，Stochastic depth随机丢弃某些网络层，分形网络（FractalNet）中的分形结构。以上这些方法都存在一个共同特点：在层与层之间创造捷径。

本文将这些想法提炼总结成：为了确保网络中最大的信息流通，让每层都与之前的所有层相连，即每层的的输入都是之前所有层的输出（通过连接concatenate而成）。

在引文中还阐述了该网络架构的三个特点：
1. 需要更少的参数（Densenet每层用到了12个卷积核）
2. 由于信息流以及梯度传递更加方便，便于训练 、
3. 自带正则化

## 相关工作（realated work）
介绍了一些网络，和resnet那篇差不多

## DenseNets
$H_l(.)$表示一种组合的操作，例如可以包括批量归一化（BN）、ReLU、Pooling、卷积（Conv）

### Dense connectivity
$x_l = H_l([x_0, x_1, ..., x_{l-1}])$

### composite function
BN > ReLU > 3x3 convolution 

### dense blocks & transition layers
假如一直以dense connect这种架构，最后一层的feature maps尺寸会很大，以前降采样的方式是在某种模式的卷积后进行，此处则是在**dense block**这种结构进行convolution和pooling,这两层称为**transition layer**
![denseblock](/images/DenseNet/dense_blocks_result.jpg)


### Growth rate
每个网络层输出的特征图数量k称为Growth rate，第l层有
$$ k_0 + k * (l-1) $$

### Bottleneck Layers
使用1*1 Conv作为作为特征降维的方式。
BN > ReLU > ReLU > Conv(1\*1) > BN > ReLU > Conv(3\*3) 
想象你有个6 x 6 x 32 的特征图，用1 x 1 x 32 Conv来进行降维
*DenseNet-B*

### Compression
$\theta$为transition layer的压缩系数，在本文中使用了$\theta = 0.5$
*DenseNet-C*

## 数据集
* CIFAR
* SVHN
* ImageNet

## 实验结果
略（没啥好说的）

## 算法分析
* 模型参数方面，与之前的网络如resnet对比
* 深层监督（deep supervision）
* 以前人们用resnet with stochastic depth, DenseNet也能做到正则化？（然而没读懂啥意思）
* 特征重用,热图表示前面层对后面层的作用

## 附录
1. [作者黄亮](http://www.gaohuang.net)

2. [densenet](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)