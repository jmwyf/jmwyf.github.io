---
title: "Embedding是什么？"
date: 2023-02-21 23:13:00
summary: "万物皆可Embedding"
feature: https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/embedding.png
tags: [nlp]
categories: Deep Learning
showSummary: true
---

{{<katex>}}

![](https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/embedding.png)

## 背景
在nlp领域，如何把词进行编码成数字，从而能输入到数学模型是需要考虑的：

索引编码[^2]：
- 整数编码，特征之间的关系无法捕捉

one-hot编码的缺点：
- 对于具有非常多类型的类别变量，变换后的向量维数过于巨大，且过于稀疏。
- 映射之间完全独立，并不能表示出不同类别之间的关系。

## Embedding是什么
嵌入是将正整数（索引值）转换为固定尺寸的稠密向量[^1]。这句话来着keras文档中对embedding层的解释，非常概括，不太容易理解，但确实概括了要干的事情。

比如一句话，“我爱中国”对应的索引为[0,1,2,3]，要将这个索引转化为固定大小且稠密的向量来表示，而不是稀疏的one-hot编码。可以表示为$[[0.2, 0.5], [0.6,-0.1], [0.8, 0.4], [0.5, 0.5]]$，。

词嵌入通常是8-1024维度，根据数据量的大小来调整，高维度的嵌入能更好的捕捉词之间的关系，但是需要更多的数据来训练。

## Embedding是如何实现的
通过Embedding层实现，embedding层可以看作是一张从索引映射到稠密向量的查找表，当使用embedding层的时候，embedding层和神经网络其他层一样，权重是随机初始化的。根据你的训练任务，embedding层通过反向传播逐渐调整。

embedding层的具体结构即[索引长度，emb维度]的权重矩阵也可以看作查询表，输入为整数索引，对应权重矩阵即词嵌入。skip-gram模型的前半部分即词嵌入。

例如在tensorflow中，用于句子分类时的嵌入层，输入是整数索引，经过嵌入层、池化层、全连接输入训练可以得到嵌入层权重，即词嵌入。
```python
embedding_dim=16
model = Sequential([  
	vectorize_layer,  
	Embedding(vocab_size, embedding_dim, name="embedding"),  
	GlobalAveragePooling1D(),  Dense(16, activation='relu'),  
	Dense(1)
	])
```

## 应用
最常用的就是词嵌入表达，但是万物可嵌入。Embedding在输入数据没有较好的数据表示时，能将输入数据根据下游任务转化为可学习的高维度向量表示，比如输入的为单词、图片或者输入的为空间位置等。

mnist数据集中的图片，可以通过嵌入层来表示，如下图所示，每个点代表一个图片(10000*784)，通过嵌入层，将图片的像素点转化为稠密的向量，然后通过t-SNE/pca降维，可以看到图片的空间分布。(generated by copilot)
![](https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/mnistembedding.png)

在进行特征工程时，很难捕捉空间(时间)维度。通过使用深度学习嵌入层，我们可以通过提供一系列用户行为(作为索引)作为模型的输入来有效地捕捉这个空间维度。



[^1]: [嵌入层 Embedding - Keras 中文文档](https://keras.io/zh/layers/embeddings/)
[^2]: [Word embeddings  |  Text  |  TensorFlow](https://www.tensorflow.org/text/guide/word_embeddings)

我的博客即将同步至腾讯云开发者社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=2cy4t3peazy8s