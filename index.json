[{"content":" 日常搬砖📝，业余科研🧬\n","date":"20 May 2023","permalink":"/","section":"","summary":"","title":""},{"content":"","date":"20 May 2023","permalink":"/categories/ai4h/","section":"Categories","summary":"","title":"AI4H"},{"content":"","date":"20 May 2023","permalink":"/tags/artificial-intelligence/","section":"Tags","summary":"","title":"artificial intelligence"},{"content":"","date":"20 May 2023","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"20 May 2023","permalink":"/tags/healthcare/","section":"Tags","summary":"","title":"healthcare"},{"content":"","date":"20 May 2023","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":" ","date":"20 May 2023","permalink":"/tags/","section":"Tags","summary":" ","title":"Tags"},{"content":"医学影像分类的自监督学习：系统综述以及实施指南 # 深度学习和计算机视觉的发展给医学影像分析提供了有前景的解决方案，有潜力提高医疗水平以及患者治疗效果。然而，训练深度学习模型的主流范式需要大量标注的训练数据，这对于医学影像数据管理即耗时又花费巨大。自监督学习有可能通过从丰富没有标签的医学数据集中学习有用的见解，为发展鲁棒性高的医学影像模型做出巨大贡献。本综述对不同自监督策略进行了一致的描述，并对2012年到2022年间在PubMed、Scopus、ArXiv上发表的使用自监督学习进行医学影像分类进行系统综述。我们综合了前期工作的知识，并且为未来对利用自监督学习建立医学影像分类模型对研究人员提供了实践指南。\n自监督常见技术 # 内在关系 # 在一些手工任务上预训练模型，在没有多余标签的情况下能够利用数据的内在关系，例如图像相对关系、预测图像旋转角度。\n生成模型 # 生成模型随着传统的自编码器（AE）、变分编码器（VAE）和生成式对抗网络（GANs）出现而变得流行起来，能够学习训练数据的分布，从而重建原始输入或者创建新的合成数据实例。\n对比学习 # 基于转化图像引起的变化不能改变图像语义的假设。所以，针对相同图像不同的数据增强方法组成了所谓的正样本对，相对于该图像其他图片以及增强样本组成了负样本对。优化模型让正样本对在潜空间距离变小并与负样本距离变远。\nSimCLR MoCo 自预测 # 自预测SSL是对部分输入进行掩码或增强，然后用没有变化的部分来重建原始输入。自预测SSL想法来源于自然语言处理领域掩码模型。\n微调技术 # 主要有两种策略用于微调已被SSL预训练的模型。如果将任意的影像模型都看成编码器和分类器两部分。两种策略能被分为\n端到端的微调，所有权重都训练 固定编码器提取特征，训练分类器 自监督医学影像实施指南 # 需要多种自监督学习策略相互比较，现有研究很少进行比较而是有无自监督学习策略对比较。\n在大型自然图像数据集中自监督预训练的模型也可以被利用到医学影像，由于医学影像的独特性，究竟能迁移多少有待研究。\n由于医学图像采集的标准化协议和人体解剖学的同质性，医学图像具有很高的类间视觉相似性，即不同类也很相似； 在医学成像领域，感兴趣的语义很少是诸如解剖器官之类的对象，而是该器官或组织内是否存在病理异常。许多异常的特征是非常微妙和局部的视觉线索，这些线索可能会由于增强变得模糊或掩盖； 自预测型自监督学习方法所使用的随机掩码通过移除有疾病或者异常的图像可能改变医学影像的语义。 在对比学习形成正样本对时应该探索更多策略，而不是使用相同图片的不同增强版本，比如通过临床信息的相似性来定义正样本对。\n在自监督学习中引入多模态信息提高下游任务模型性能。\nSelf-supervised learning for medical image classification: a systematic review and implementation guidelines | npj Digital Medicine\n","date":"20 May 2023","permalink":"/posts/20230521-ai4h@3/","section":"Posts","summary":"","title":"医学人工智能周刊 #3"},{"content":"大型语言模型在医学和医学研究中的伦理问题 # 大型语言模型（LLM）是一种在大量文本数据中训练的深度学习模型，其目标是生成类似人类响应的新文本。2022年11月30日发布的基于大型语言模型的对话机器人ChatGPT（OpenAI, San Francisco, CA, USA），将大型语言模型推动到公众视野并且让数百万能够使用它进行试验。自从那时起，医学从业者和研究者就开始探索LLM的潜在应用，因为很多医学实践和研究都围绕着大量基于文本的任务，例如展示、发表、记录和报告。使用LLM来帮助和简化这些任务可以节约大量时间，让临床人员和研究者能进行其他工作。目前有许多在不同开发阶段的其他LLMs，包括BioGPT（MIT），LaMDA（Google），Sparrow（Deepmind AI），Pangu Alpha（Huawei），OPT-IML（Meta）以及Megataron Turing MLG（Nvidia）。一些新的变种，例如基于PubMed文献训练后专注于生物医学领域文本生产以及挖掘的BioGPT，可能对未来医学和医学研究有重大的影响。与任何新兴、颠覆式的技术一样，重要的是考虑使用中的伦理并优先考虑符合社会最佳利益的负责任和有益的应用。本文从偏见、信任、责任人、公平性以及隐私探讨了LLM在医学实践和研究中关键的伦理问题。\nEthics of large language models in medicine and medical research - The Lancet Digital Health\n对可穿戴设备数据去隐私化是不是给我们一个虚假的安全感？系统综述 # 可穿戴设备让采集和分享个人数据变得更加容易，这篇系统综述调查来源于可穿戴设备的去隐私化数据是不是足以保护数据集中个体的隐私。通过文献综述，本文总结正确识别率约86%-100%，表明重新标识风险很高。（重新识别通常是将去标识化或者匿名的数据集与有标识化数据的数据集建立连接）而且，只需要1-300秒记录数据就可以从通常被认为不能产生标识化信息的传感器数据例如ECG进行重新标识化。该发现提醒重新思考数据分享的方法，在促进研究创新过程中，同时保护个人隐私。\nDoes deidentification of data from wearable devices give us a false sense of security? A systematic review - The Lancet Digital Health\n特定时间和事件深度学习方法用于心肌灌注成像后个性化风险评估 # 心肌灌注成像（MPI）的标准临床解释已被证实对主要不良心血管事件（MACE）有预后价值。然而，对特定事件类型和时间间隔进行个性化预测具有挑战性。本文建立一个可解释的深度学习模型，该模型能够直接从MPI和15中临床特征中分别预测全因死亡、急性冠状动脉综合症和血管重建的特定时间风险。这种方法能将个体事件概率表示为时间的函数并且关注特定患者和特定事件的风险解释，这可能有助于引起人们对可改变风险因素的关注。该模型使用了DeepHit中架构以及修改后的损失函数，输出是一个3*131的2维矩阵分别代表了从0到每30天直到最长随访时间各事件发生的概率。\nTime and event-specific deep learning for personalized risk assessment after cardiac perfusion imaging | npj Digital Medicine\n临床预测算法验证展望 # 临床预测算法的泛化性与临床实践应用十分相关。本文基于现有文献概述了三种类型的泛化性：时序，地理，和领域泛化，以及其目标、方法、利益相关方。\nPerspectives on validation of clinical predictive algorithms | npj Digital Medicine\n","date":"8 May 2023","permalink":"/posts/20230509-ai4h@2/","section":"Posts","summary":"","title":"医学人工智能周刊 #2"},{"content":"通用医学人工智能的基础模型 # 高灵活性、可复用人工智能模型的异常快速发展可能会在医疗领域迎来新的能力。我们提出一种医学人工智能新范式，称之为通用医学人工智能（GMAI）。GMAI有能力只用少量数据或者非特定任务标签数据来解决各种各样的任务。通过在大样本，多样性数据中进行自监督学习，GMAI能灵活地解释不同组合的医学模态，包括影像、电子病历、实验室检查结果、基因组学、图表或者是医学文本。模型会产生富有表现力的输出，包括能显示先进医学推理能力的自由文本解释，口头建议或者图像标注。在本文中我们给出了对于GMAI一系列潜在高影响力的应用，并列出了实现这些应用对应的技术和训练所必须的数据集。我们预计GMAI驱动的应用将挑战当前监管和验证医疗AI器械现有的策略，而且将会改变收集大型医学数据的做法。\nFoundation models for generalist medical artificial intelligence | Nature\n商业医学数据集对医学研究和医疗健康算法的影响 # 随着医疗保健行业进入由云数据存储、分布式计算和机器学习驱动的数字健康新时代，医疗保健数据已成为对私人和公共实体具有价值的优质商品。目前无论是来自工业界、学术界还是政府机构的卫生数据收集和分发框架都并不完善，无法让研究人员充分利用下游分析的能力。在这篇医学政策论文中，我们回顾了现有商业医学数据供应商的现状，特别强调了他们数据的来源、数据可重复性、泛化性的挑战以及数据售卖伦理方面的考虑。我们主张使用可持续的方法来组织管理公开健康数据，从而让全球人口能够被纳入生物医学研究界。然而，为了全面实施这些方法，关键利益相关者应齐心协力来让医学数据集更容易获得、更全面和更具有代表性，同时平衡好被收集数据个人隐私以及权利。\nThe impact of commercial health datasets on medical research and health-care algorithms - The Lancet Digital Health\n用于检测早产儿视网膜病变的定制工程和无代码深度学习模型的开发和国际验证：回顾性研究 # 早产儿视网膜病变（ROP）是儿童失明的主要原因，由儿科眼科医生通过间隔筛查进行诊断。然而，早产儿生存率的提高加上现有专家的稀缺，引起了人们对这种方法可持续性的担忧。我们的目标是在英国伦敦的种族多元化人群中开发定制、无代码的基于深度学习的分类器，用于附加病变检查（ROP 的标志），并在四个国家和三大洲的种族、地理和社会经济多样化人群中对其进行外部验证。无代码深度学习不依赖于受过专业培训的数据科学家，因此对资源匮乏的医疗保健环境具有特别的潜在好处。\nDevelopment and international validation of custom-engineered and code-free deep-learning models for detection of plus disease in retinopathy of prematurity: a retrospective study - The Lancet Digital Health\n目前对基于人工智能的医疗设备临床研究是否足够全面，以支持全面的健康技术评估？系统综述 # 基于人工智能的医疗设备（AI-based MDs）在医疗保健领域正在经历指数级增长。这项研究旨在调查目前评估人工智能的研究是否包含健康技术评价（HTA）机构进行HTA所需的信息。在讨论部分，从各个角色的角度分析，建议该从哪些方面进行改进AI-MD评价\nAre current clinical studies on artificial intelligence-based medical devices comprehensive enough to support a full health technology assessment? A systematic review - ScienceDirect\n#medical-data\n","date":"1 May 2023","permalink":"/posts/20230501-ai4h@1/","section":"Posts","summary":"","title":"医学人工智能周刊 开刊"},{"content":"","date":"30 March 2023","permalink":"/tags/statistics/","section":"Tags","summary":"","title":"statistics"},{"content":"","date":"30 March 2023","permalink":"/categories/statistics/","section":"Categories","summary":"","title":"Statistics"},{"content":"","date":"30 March 2023","permalink":"/tags/survival-analysis/","section":"Tags","summary":"","title":"survival analysis"},{"content":" 简介 # 生存分析是将观察的结局和出现结局所经历的时间结合起来进行分析的一系列统计方法，常用于研究影响因素与生存时间和结局的关系，预测不同因素水平个体生存预测。\n因为跟时间相关，所以要定义要事件起点，以及事件终点。生存时间T也可以根据事件起终点计算出来。\n由于有些事件无法被观测或者没有观察到，导致生存时间无法被记录的情况称为删失。其中最为常见的情形称为右删失（right censoring，下图）\n右删失：对这样的病人我们只知道其生存时间要大于从试验开始到删失发生的时间。有多种原因可以导致右删失情况的出现，其中包括：(1)病人在某时间点上退出试验或失去随访信息；(2)病人在整个试验结束时事件还未发生；(3)病人由于毒性等原因停用被分派的药物或换用其它药物；(4)竞争风险事件的发生1。 左删失2：事件发生了，且发生时间在（0，t） ，但确切时间并不清楚。 区间删失: 事件发生在一个已知的具体时间段内，但是并不知道具体时间。 生存函数 # 生存函数$$S(t) = P(T\u0026gt;t)$$描述的是事件发生时间大于时间点t的概率，理论上T是连续变量，那么生存函数是一个连续的递减函数，然而实际实验中T都是有限时间点的，所以连续曲线变成离散的。“若想用光滑曲线来连接就需要对随机变量T的分布做出假设的参数拟合法，而曲线一般不宜正好经过所有的红点 (那样会导致过度拟合而使得统计模型没有多大效用) ；若用非参数的阶梯函数来连接，那么曲线简单而唯一确定！”这也是为什么k-m曲线这么重要的原因。 风险函数 # 风险函数的定义即在t时刻发生事件的概率。$$h(t)=P(T=t|T\\ge t)$$生存函数是可以看到是从1到0递减的，而风险函数没有固定的单调性，可以是常数风险、可以随时间变化，比如上升、下降、先下降后上升。\n假设生存时间T这个变量概率分布满足$f(t)$，累积分布$F(t)=P(T\\le t)= \\int_0^t f(t)dt$，可以看到生存函数$S(t)=1-F(t)$，两边求导数可得$$f(t)=-S\u0026rsquo;(t)$$ 那么在t时刻发生事件的风险用极限的观点来看即在$t -\u0026gt; t+\\Delta t$ 这个区间内发生事件的数量除以在t时刻剩下的总人数$$h(t)=\\lim_{\\Delta t \\to 0}\\frac{F(t+\\Delta t)-F(t)}{\\Delta t*S(t)}=\\frac{f(t)}{S(t)}=\\frac{-S\u0026rsquo;(t)}{S(t)}$$ $$h(t)=\\frac{-S\u0026rsquo;(t)}{S(t)}=-\\frac{\\partial}{\\partial y}log[S(t)]\\tag1$$\n$$S(t)=exp[-\\int_0^th(t)dt]\\tag2$$ 式1后面一个等式右边求偏微分等于等式左边，两边积分即式2。以上为风险函数与生存函数之间的关系3\nKaplan-Meier生存曲线 # K-M曲线是非参数估计生存函数的一种方法。\n如何绘制K-M生存曲线？ 临床原始资料一般如下：\n患者 生存时间 发生事件与否 删失与否 a 10 0 r b 28 1 c 30 1 d 2 1 e 7 0 r 假设是来分析患者生存分析，定义结局是死亡，最长实验观察时间是90天， 首先是判断删失数据，假设a和e患者分别出院，观察不到死亡结局，那么以上两个患者是删失数据。\n将资料整理成生存概率随时间变化表格\n患者 生存时间 存活患者数n 死亡人数d 当前存活概率(n-d)/n 生存概率 0 1 d 2 5 1 4/5 4/5 e 7+ a 10+ b 28 2 1 1/2 ${4/5}*{1/2}=2/5$ c 30 1 1 0 0 将上表生存时间以及生存概率可视化即可得到k-m曲线。\n以上介绍了生存分析的基本原理以及非参数生存分析K-M曲线是如何来的，有空介绍半参以及参数化的生存分析方法。\n揭秘生存曲线背后的生物统计学\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n怎么理解生存分析的风险函数? - 数据的小米虫的回答 - 知乎 https://www.zhihu.com/question/343779367/answer/2439383246\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n怎么理解生存分析的风险函数? - 郭老师医学统计的回答 - 知乎 https://www.zhihu.com/question/343779367/answer/1493205766\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"30 March 2023","permalink":"/posts/20230401-survival-analysis/","section":"Posts","summary":"医学统计学","title":"生存分析（一）"},{"content":"","date":"22 March 2023","permalink":"/tags/hypothesis-testing/","section":"Tags","summary":"","title":"hypothesis-testing"},{"content":" 简介 # 药厂宣传新药疗效很好，研究宣称研发的算法比之前的要好或者某项运动是有助于长寿的，我们怎么样来判断这些结果是否靠谱？这些问题就可以用统计学中的假设检验来判断。\n统计推断是根据抽样分布规律和概率理论，由样本结果去推论总体特征。它主要包括假设检验和参数估计两个内容。\n假设检验的理论依据是“小概率事件原理”。“小概率事件原理”就是概率很小的事件在一次试验中认为是不可能发生的。如果预先的假设使得小概率事件发生了，类似于数学中传统推理的反证法出现逻辑矛盾那样，就认为出现了不合理现象，从而拒绝假设。一般把概率不超过0.10、0.05、0.01的事件当作“小概率事件”，称为检验水准或显著水平，通常取0.05、0.01，实际问题中也可取0.10、0.001等。1\n假设检验步骤 # 提出假设原假设和备择假设 # 根据要比较的统计量类型，选择不同的假设检验类型，比如样本均值与指定值，汽车百公里油耗为xx；样本比例，支持率低于30%；样本方差，矿泉水容量的离散程度 原假设通常是不存在差异或者没有关联，比如A组和B组均值没有差异；备选假设可以选择左右或双侧（大、小或不等于） 原假设是唯一的，而备择假设有很多，这也是为什么对无差异进行检验的原因，即假设无差异为真来检验到底是不是无差异 抽样（optional） # 一般问题就说了自己样本是什么。对于需要实验验证的问题，采样时由于不可能涵盖所有的样本，需要选择合适具有代表性的样本，进行两组比较或者与指定总体样本比较 选择检验统计量 # 对假设进行检验的统计量，一般为抽样的样本在原假设情况下符合什么分布，计算对所关注差异或者效果的度量。 统计显著性水平、拒绝域/临界点 # 显著性水平：原假设为真时拒绝原假设的概率，一般是0.05。后续计算以原假设为真统计量出现的概率，假如小于显著性水平，即认为出现的概率很低（小概率事件），拒绝原假设。假如原假设确实为真，这里就犯了第一类错误，但是这个概率是我们可以直接设置的。 第一类错误2：没差异判断有差异$\\alpha$ 第二类错误：有差异判断没差异$\\beta$ 统计检验能力：有差异能判断出差异，$1-\\beta$ 通常需要大雨0.8（Jacob Cohen） 第一类错误和第二类错误的关系 如下图应该选择备择假设，而选了null假设的概率为$\\beta$, 可以看到$\\alpha$越小，$\\beta$越大,无法设置让第一类错误和第二类错误同时变小 拒绝域：根据分布以及显著性水平可以确定拒绝域值 求出检查统计量的p值 # 双边或单边某中分布下检验量统计量出现的概率 查看样本结果是否位于拒绝域 # 一般通过p与显著性水平比较 决定是否接受原假设 # 如果p小于显著性水平，拒绝原假设 案例一 女士品茶 # 以女士品茶为例，一位女士宣称自己可以分辨先奶后茶还是先茶后奶，大家都觉得不可思议，准备了10杯奶茶让其分辨，正确分辨了每一杯，现在问题是是不是真的能分辨？\n提出原假设和备择假设 原假设：不能分辨 选择检验统计量 在不能分辨的情况10次都对，对该事件的度量 显著性水平 0.05 检验统计量概率：不能分辨就是瞎猜每次判断的概率为1/2 ，该次事件的概率为$(\\frac{1}{2})^{10}$ 判断是否位于拒绝域 远小于显著性水平，落在拒绝域，小概率事件不可能发生，但是在这次实验中发生，说明原假设不对 拒绝原假设 即该女士有分辨奶茶先后的能力 案例二 均值检验 # 测定土地PH值是否为7，进行了17次采样，采样结果mean = 6.676， sd= 0.4553\n提出原假设与备择假设 ph为7 选择检验统计量 大样本数据一般认为符合正态分布，正态分布的均值$$z=\\frac{\\bar{x}-\\mu_{0}}{\\sigma/\\sqrt{n}}$$而总体标准差一般很难获取，这时一般用样本标准差来代替，样本统计量服从t分布，上述公式变为$$z=\\frac{\\bar{x}-\\mu_{0}}{s/\\sqrt{n}}$$样本的均值应满足上式 显著性水平 0.05 检验统计量概率 代入公式得t统计量为-2.9，查分布表找到概率为0.009 拒绝 实验设计与统计分析\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n统计学图鉴\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n统计学习理论与方法\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"22 March 2023","permalink":"/posts/20230323-hypothesis-testing/","section":"Posts","summary":"统计学回顾","title":"统计学中的假设检验"},{"content":"","date":"6 March 2023","permalink":"/tags/question/","section":"Tags","summary":"","title":"question"},{"content":"","date":"6 March 2023","permalink":"/categories/science/","section":"Categories","summary":"","title":"Science"},{"content":"科学问题：地球人还未找到答案，且无法确定是否能找到答案的问题 ；12\n技术问题：实现某个现实已有功能的途径；\n工程问题：结合多个技术解决一个系统化现实问题的方案；\n（科学问题：我不知道；\n技术问题：我没想到；\n工程问题：我做不了；）\n参考新版世界前沿125个科学问题3\nSJTU \u0026amp; Science 125个科学问题（2021年版本） # Medicine \u0026amp; Health (11) # 医学与健康\nCan we predict the next pandemic? 1.我们可以预测下一次流行病吗？\nWill we ever find a cure for the common cold? 2.我们会找到治疗感冒的方法吗？\nCan we design and manufacture medicines customized for individual people? 3.我们可以设计和制造出为个人定制的药物吗？\nCan a human tissue or organ be fully regenerated? 4.人体组织或器官可以完全再生吗？\nHow is immune homeostasis maintained and regulated? 5.如何维持和调节免疫稳态？\nIs there a scientific basis to the Meridian System in traditional Chinese medicine? 6.中医的经络系统有科学依据吗？\nHow will the next generation of vaccines be made? 7.下一代疫苗将如何生产？\nCan we ever overcome antibiotic resistance? 8.我们能否克服抗生素耐药性？\nWhat is the etiology of autism? 9.自闭症的病因是什么？\nWhat role does our microbiome play in health and disease? 10.我们的微生物组在健康和疾病中扮演什么角色？\nCan xenotransplantation solve the shortage of donor organs? 11.异种移植能否解决供体器官的短缺问题？\nBiology(22) # 生命科学\nWhat could help conservation of the oceans? 1.什么可以帮助保护海洋？\nCan we stop ourselves from aging? 2.我们可以阻止自己衰老吗？\nWhy can only some cells become other cells? 3.为什么只有一些细胞会变成其他细胞？\nWhy are some genomes so big and others very small? 4.为什么有些基因组非常大而另一些却很小？\nWill it be possible to cure all cancers? 5.有可能治愈所有癌症吗？\nWhat genes make us uniquely human? 6.哪些基因使我们人类与众不同？\nHow do migratory animals know where they\u0026rsquo;re going? 7.迁徙动物如何知道它们要去哪里？\nHow many species are there on Earth? 8.地球上有多少物种？\nHow do organisms evolve? 9.有机体是如何进化的？\nWhy did dinosaurs grow to be so big? 10.为什么恐龙长得如此之大？\nDid ancient humans interbreed with other human-like ancestors? 11.远古人类是否曾与其他类人祖先杂交？\nWhy do humans get so attached to dogs and cats? 12.人类为什么会对猫狗如此着迷？\nWill the world\u0026rsquo;s population keep growing indefinitely? 13.世界人口会无限增长吗？\nWhy do we stop growing? 14.我们为什么会停止生长？\nIs de-extinction possible? 15.能否复活灭绝生物？\nCan humans hibernate? 16.人类可以冬眠吗？\nWhere do human emotions originate? 17.人类的情感源于何处？\nWill humans look physically different in the future? 18.未来人类的外貌会有所不同吗？\nWhy were there species explosions and mass extinction? 19.为什么会发生物种大爆发和大灭绝？\nHow might genome editing be used to cure disease? 20.基因组编辑将如何用于治疗疾病？\nCan a cell be artificially synthesized? 21.可以人工合成细胞吗？\nHow are biomolecules organized in cells to function orderly and effectively? 22.细胞内的生物分子是如何组织从而有序有效发挥作用的？\nInformation Science(4) # 信息科学\nIs there an upper limit to computer processing speed? 1.计算机处理速度是否有上限？\nCan AI replace a doctor? 2.AI可以代替医生吗？\nCan topological quantum computing be realized? 3.拓扑量子计算可以实现吗？\nCan DNA act as an information storage medium? 4.DNA可以用作信息存储介质吗？\nNeuroscience(12) # 神经科学\nWhat are the coding principles embedded in neuronal spike trains? 1.神经元放电序列的编码准则是什么？\nWhere does consciousness lie? 2.意识存在于何处？\nCan human memory be stored, manipulated, and transplanted digitally? 3.能否数字化地存储、操控和移植人类记忆？\nWhy do we need sleep? 4.为什么我们需要睡眠？\nWhat is addiction and how does it work? 5.什么是成瘾？\nWhy do we fall in love? 6.为什么我们会坠入爱河？\nHow did speech evolve and what parts of the brain control it? 7.言语如何演变形成，大脑的哪些部分对其进行控制？\nHow smart are nonhuman animals? 8.除人类以外的其他动物有多聪明？\nWhy are most people right-handed? 9.为什么大多数人都是右撇子？\nCan we cure neurodegenerative diseases? 10.我们可以治愈神经退行性疾病吗？\nIs it possible to predict the future? 11.有可能预知未来吗？\nCan we more effectively diagnose and treat complex mental disorders? 12.精神障碍能否有效诊断和治疗？\nArtificial Intelligence(8) # 人工智能\nWill injectable, disease-fighting nanobots ever be a reality? 1.可注射的抗病纳米机器人会成为现实吗？\nWill it be possible to create sentient robots? 2.是否有可能创建有感知力的机器人？\nIs there a limit to human intelligence? 3.人类智力是否有极限？\nWill artificial intelligence replace humans? 4.人工智能会取代人类吗？\nHow does group intelligence emerge? 5.群体智能是如何出现的？\nCan robots or AIs have human creativity? 6.机器人或 AI 可以具有人类创造力吗？\nCan quantum artificial intelligence imitate the human brain? 7.量子人工智能可以模仿人脑吗？\nCould we integrate with computers to form a human-machine hybrid species? 8.我们可以和计算机结合以形成人机混合物种吗？\nMathematical Sciences(3) # 数学\nWhat makes prime numbers so special? 1.什么使素数如此特别？\nWill the Navier–Stokes problem ever be solved? 2.纳维尔-斯托克斯问题会得到解决吗？\nIs the Riemann hypothesis true? 3.黎曼猜想是真的吗？\nChemistry(9) # 化学\nAre there more color pigments to discover? 1.还有更多色彩元素可发现吗？\nWill the periodic table ever be complete? 2.元素周期表会完整吗？\nHow can we measure interface phenomena on the microscopic level? 3.如何在微观层面测量界面现象？\nWhat is the future for energy storage？ 4.能量存储的未来是怎样的？\nWhy does life require chirality? 5.为什么生命需要手性？\nHow can we better manage the world\u0026rsquo;s plastic waste? 6.我们如何更好地管理世界上的塑料废物？\nWill AI redefine the future of chemistry? 7.AI会重新定义化学的未来吗？\nHow can matter be programmed into living materials? 8.物质如何被编码而成为生命材料？\nWhat drives reproduction in living systems? 9.是什么驱动生命系统的复制？\nAstronomy(23) # 天文学\nHow many dimensions are there in space? 1.空间中有多少个维度？\nWhat is the shape of the universe? 2.宇宙的形状是怎样的？\nWhere did the big bang start? 3.大爆炸从何处开始？\nWhy don\u0026rsquo;t the orbits of planets decay and cause them to crash into each other? 4.为什么行星的轨道不衰减并导致它们相互碰撞？\nWhen will the universe die? Will it continue to expand? 5.宇宙何时消亡？它会继续膨胀吗？\nIs it possible to live permanently on another planet? 6.我们有可能在另一个星球上长期居住吗？\nWhy do black holes exist? 7.为什么存在黑洞？\nWhat is the universe made of? 8.宇宙是由什么构成的？\nAre we alone in the universe? 9.我们是宇宙中唯一的生命体吗？\nWhat is the origin of cosmic rays? 10.宇宙射线的起源是什么？\nWhat is the origin of mass? 11.物质的起源是什么？\nWhat is the smallest scale of space-time? 12.时空的最小尺度是是多少？\nIs water necessary for all life in the universe, or just on Earth? 13.水是宇宙中所有生命所必需的么，还是仅对地球生命？\nWhat is preventing humans from carrying out deep-space exploration? 14.是什么阻止了人类进行深空探测？\nIs Einstein\u0026rsquo;s general theory of relativity correct? 15.爱因斯坦的广义相对论是正确的吗？\nHow are pulsars formed? 16.脉冲星是如何形成的？\nIs our Milky Way Galaxy special? 17.我们的银河系特别吗？\nWhat is the volume, composition, and significance of the deep biosphere? 18.深层生物圈的规模、组成和意义是什么？\nWill humans one day have to leave the planet (or die trying)? 19.人类有一天会不得不离开地球吗（还是会在尝试中死去）？\nWhere do the heavy elements in the universe come from? 20.宇宙中的重元素来自何处？\nIs it possible to understand the structure of compact stars and matter? 21.有可能了解致密恒星和物质的结构吗？\nWhat is the origin of the high-energy cosmic neutrinos? 22.高能宇宙中微子的起源是什么？\nWhat is gravity? 23.什么是重力？\nPhysics(18) # 物理学\nIs there a diffraction limit? 1.有衍射极限吗？\nWhat is the microscopic mechanism for high-temperature superconductivity? 2.高温超导的微观机理是什么？\nWhat are the limits of heat transfer in matter? 3.物质传热的极限是什么？\nWhat are the fundamental principles of collective motion? 4.集体运动的基本原理是什么？\nWhat are the smallest building blocks of matter? 5.什么是物质的最小组成部分？\nWill we ever travel at the speed of light? 6.我们会以光速行驶吗？\nWhat is quantum uncertainty and why is it important? 7.什么是量子不确定性，为什么它很重要？\nWill there ever be a \u0026ldquo;theory of everything\u0026rdquo;? 8.会有“万有理论”吗？\nWhy does time seem to flow in only one direction? 9.为什么时间似乎只朝一个方向流动？\nWhat is dark matter? 10.什么是暗物质？\nCan we make a real, human-size invisibility cloak? 11.我们可以制作出真人大小的隐形斗篷吗？\nAre there any particles that behave oppositely to the properties or states of photons? 12.是否存在与光子性质或状态相反的粒子？\nWill the Bose-Einstein condensate be widely used in the future? 13.玻色-爱因斯坦冷凝体未来会被广泛使用吗？\nCan humans make intense lasers with incoherence comparable to sunlight? 14.人类能制造出与太阳光相似的非相干强激光吗？\nWhat is the maximum speed to which we can accelerate a particle? 15.我们最多可以将粒子加速到多快？\nIs quantum many-body entanglement more fundamental than quantum fields? 16.量子多体纠缠比量子场更基本吗？\nWhat is the optimum hardware for quantum computers? 17.量子计算机的最佳硬件是什么？\nCan we accurately simulate the macro- and microworld? 18.我们可以精确模拟宏观和微观世界吗？\nEngineering \u0026amp; Material Science(4) # 工程与材料科学\nWhat is the ultimate statistical invariances of turbulence? 1.湍流的最终统计不变性是什么？\nHow can we break the current limit of energy conversion efficiencies? 2.我们如何突破当前的能量转换效率极限？\nHow can we develop manufacturing systems on Mars? 3.我们如何在火星上开发制造系统？\nIs a future of only self-driving cars realistic? 4.纯无人驾驶汽车的未来是否现实？\nEcology(8) # 生态学\nCan we stop global climate change? 1.我们可以阻止全球气候变化吗？\nWhere do we put all the excess carbon dioxide? 2.我们能把过量的二氧化碳存到何处？\nWhat creates the Earth\u0026rsquo;s magnetic field (and why does it move)? 3.是什么创造了地球的磁场（为什么它会移动）？\nWill we be able to predict catastrophic weather events (tsunami, hurricanes, earthquakes) more accurately? 4.我们是否能够更准确地预测灾害性事件（海啸、飓风、地震）？\nWhat happens if all the ice on the planet melts? 5.如果地球上所有的冰融化会怎样？\nCan we create an environmentally friendly replacement for plastics? 6.我们可以创造一种环保的塑料替代品吗？\nCan we achieve a situation where essentially every material can be recycled and reused? 7.几乎所有材料都可以回收再利用是否可以实现？\nWill we soon see the end of monocultures like wheat, maize, rice, and soy? 8.我们会很快看到小麦、玉米、大米和大豆等单一作物的终结吗？\nEnergy Science(3) # 能源科学\nCould we live in a fossil-fuel-free world? 1.我们可以生活在一个去化石燃料的世界中吗？\nWhat is the future of hydrogen energy? 2.氢能的未来是怎样的？\nWill cold fusion ever be possible? 3.冷聚变有可能实现吗？\n思考 # 科学问题有大有小，上面是全人类目前这个阶段关注的大科学问题，那么小而精的科学问题该如何凝练？\n【复旦赵斌】避免尴尬，教你一句话说清楚什么是科学问题、技术问题和工程问题_哔哩哔哩_bilibili\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n【复旦赵斌】老师喊你搞事情 | 125个科学问题，你最想了解哪个？_哔哩哔哩_bilibilic\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n全世界最前沿的125个科学问题！\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"6 March 2023","permalink":"/posts/20230306-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A7%91%E5%AD%A6%E9%97%AE%E9%A2%98/","section":"Posts","summary":"科学问题是还不知道答案的问题","title":"什么是科学问题？"},{"content":"","date":"4 March 2023","permalink":"/categories/deep-learning/","section":"Categories","summary":"","title":"Deep Learning"},{"content":"","date":"4 March 2023","permalink":"/tags/mimic-iii/","section":"Tags","summary":"","title":"mimic-iii"},{"content":"MULTIBENCH，一个系统而统一的大规模多模态学习基准，涵盖15个数据集、10种模式、20个预测任务和6个研究领域1。\n引言 # 背景：\n语言和视觉领域多模态学习发展不错，但是其他领域欠缺 现在的基准评价关注性能，没有量化缺点包括时间空间复杂度，由于不完美模态导致的鲁棒性降低，需要在性能、鲁棒性、复杂度取得平衡 提出multibench就是解决以上问题：\n扩充收集各领域数据集、数据模态 量化复杂度 提出标准流程评价对噪声和缺失模态情况下的鲁棒性 MultiBench是一个端到端的过程，包括数据预处理、数据集拆分、多模态算法、评估指标和交叉验证。\n开发工具包MultiZoo 可以用于workshop、教学等 多尺度多模态基准 # 第一版集中在多模态融合，对于多模态翻译等问题未来版本可能涉及\n数据集 # 介绍了6大领域15个数据集，表1\n情感计算（affective computing） 医疗：时变和静态变量的整合使用 机器人 金融 人机交互 多媒体 评价标准 # 性能：\nregression: MSE, MAE, classification: F1-score, AUPRC 复杂度：\ndata size in bits number of model parameters time and memory resources on CPU and GPU 鲁棒性：\n单模态独有噪音：对图像、音频等单独处理 考虑多模态整体的不完善：比如缺失模态等 MultiZoo：多模态算法集合 # 涵盖实现multibench整个过程中的算法\n数据预处理 # WordAlign算法 将各模态信息调整到统一粒度 融合范式 # 早期和晚期融合 EF，LF 多模态张量: 多模态互补 Tensor Fusion Low-rank Tensor Fusion 多模态乘法交互: 多模态交互 MI-MATRIX MI-VECTOR MI-SCALAR 多模态门控 NL GATE: 自注意力机制 时序注意力模型 MULT: 多模态Transformer 网络架构搜索 MFAS 优化目标 # 除了标准的监督损失函数，纳入一些新提出的目标函数\nCCA REFNET MFM MCTN 训练过程 # Gradient Blending来计算融合的权重 Regularization by Maximizing Functional Entropies 实验 # 泛化性能 目前的方法表现出高方差，没有放之四海而皆准的模型，特别是对于未被研究的模式和任务。 后期融合表现比较均衡 有些融合方法是专门为2模态设计，有些在2/3模态表现不好 单模态与多模态的权衡 性能与复杂度的权衡 性能与鲁棒性的权衡 结论 # 一个大规模的基准，统一了以前在多模态研究中互不相干的工作，重点是易用性、可及性和可重复性。\n未来拓展 # 其他的多模态问题 新的评价指标 多模态迁移学习或者协同学习 多模态多任务学习 思考 # MultiBench把以前多模态研究中使用的公开数据集，算法，评价指标等都统一在了一个框架下，期望标准化多模态学习过程，并且能将不同的算法模型在其他模态、任务中进行比较。大而全的框架确实能为各类多模态任务提供一个baseline，但是各专业领域内的多模态模型应该是存在一些差异的，就像我们很难期待一个医生能掌握律师干的事情，然而，人工智能的发展确实很快，比人还强大的通用人工智能应该也会实现。\nLiang, P. P. et al. MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (2021) doi:10.48550/arXiv.2107.07502.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"4 March 2023","permalink":"/posts/20230304-multibench/","section":"Posts","summary":"Multiscale Benchmarks for Multimodal Representation Learning论文解读","title":"MultiBench多模态表征学习的多尺度基准"},{"content":"","date":"4 March 2023","permalink":"/tags/multimodal/","section":"Tags","summary":"","title":"multimodal"},{"content":"","date":"1 March 2023","permalink":"/tags/bert/","section":"Tags","summary":"","title":"bert"},{"content":" 使用临床文本预训练BERT然后在再入院任务中微调\n引言 # 非结构化、高维稀疏信息例如临床文本难以在临床机器学习模型中使用。临床文本中包含什么样的临床价值？更加丰富、详细。然而重症监护室医生在有限时间内需要做出最优决策，读大量的临床文本，增加工作量。\n再入院会降低患者生活质量、增加花费。这篇文章旨在建立一个出院决策模型，根据医护人员笔记动态的赋予患者30天再入院的风险。\n背景 # 临床文本会有缩写、黑话、不标准的语法结构，从临床文本中学习有用的表征具有挑战。以往的方法无法捕捉获取临床意义的文本长程依赖，介绍BERT，以及基于BERT已经开展的工作，已经有人把BERT用在临床文本了，本文在再入院任务上评估改进ClinicalBERT并且在更长的序列上进行预训练。\n介绍前人在ICU再入院预测上的工作，缺点：大多数工作都只用了出院的信息，ClinicalBERT使用患者住院整个时间段信息。\n该工作的重要性 # 用出院信息来预测意味着减少了再入院风险的机会少了，都要出院了，此刻告诉有再入院的风险，难以采取措施； 由于医院已经有很多误报警，医疗模型需要高的PPV1，该模型同其他模型相比有最高的recall2； 模型中attention能用于可视化解释。 方法 # 什么是BERT # BERT是基于transformer编码器架构的深度神经网络，它用于学习文本的嵌入表达。\n自注意力机制 BERT模型通过2个无监督任务进行预训练：掩码模型和下一个句子预测。 临床文本嵌入 # 先分词成token3，这里是子词粒度的tokenization4 ClinicalBert的token包括子词、分段嵌入、位置嵌入相加后的结果 分段嵌入是当多个序列输入时，表示当前的token属于哪一段 位置嵌入即在输入序列中token的位置 自注意力机制 # 用于输入token之间的关系捕捉\n预训练 # BERT是在BooksCorpus和Wikipedia中预训练的，临床文本黑话缩写，与一般文本可能语法也不一样，需要在临床文本中进行预训练。损失函数是预测掩码单词任务和预测两个句子是否连续任务损失函数之和。\n微调 # 在再入院任务中微调 $$P(readmit = 1 | h_{[cls]}) = \\sigma(Wh_{[cls]})$$ 式中W为参数，h为BERT模型输出。\n实验 # 数据 # MIMIC-III中2083180份去隐私化后的文本，五折每一轮其中四折预训练，最后一折微调\n实证研究I # 在临床语言建模中ClinicalBERT与BERT进行比较：预测掩码token以及2个句子是否连续任务中均优于BERT 定性分析：专家给出相似医学概念，ClinicalBERT学习嵌入表达后，进行降维可视化，发现相近 定量分析：采用相似度度量公式计算表征之前相似度，然后与专家打分的相似度进行关联分析计算pearson相关系数 实证研究II # 再入院队列：34560患者，2963再入院，42358负样本，这里为啥有这么多负样本？ 调整后的再入院预测： $$P(readmit = 1|h_{patient}) = \\frac{P^n_{max}+P^n_{mean}n/c}{1+n/c}$$ 有些文本是比较重要，有些文本对再入院预测不重要，所以要包括最大的概率 噪声会降低性能，消除噪音的方法还是取大多数值的平均，如果序列越长，噪声出现的可能性越大，所以需要平均值的权重越大，引入了n/c作为比例因子 分母则是用于概率归一化到0,1区间 评估指标 AUROC AUPRC RP80：准确度为80%时候到召回率 模型比较：Bag of words，BI-LSTM，BERT 用出院记录来进行再入院预测 用24-48小时数据预测，以及48-72小时数据预测 可解释性 给出一句话的self-attention权重示意图 讨论 # 建议在私有数据集上重新训练后在下游任务中使用\n代码 # 源码：GitHub - kexinhuang12345/clinicalBERT: ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission (CHIL 2020 Workshop) 模型：emilyalsentzer/Bio_ClinicalBERT · Hugging Face 思考 # 自chatgpt后，大型语言模型受到广泛关注，医学语言模型的发展似乎有多种路径，一种是直接在通用文本上预训练，一种是在医学文本中预训练，或是通用模型在领域微调，个人感觉应该是第三种效果会较好。\nHuang, K., Altosaar, J. \u0026amp; Ranganath, R. ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission. in CHIL (arXiv, 2020). doi:10.48550/arXiv.1904.05342.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPPV: 阳性预测里面真正的阳性比例\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nrecall: 正样本中实际预测为正，即真阳性率\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ntoken: 将原始文本切分成子单元的过程就叫做Tokenization，子单元即token\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"1 March 2023","permalink":"/posts/20230302-clinicalbert/","section":"Posts","summary":"ClinicalBERT Modeling Clinical Notes and Predicting Hospital Readmission论文解读","title":"ClinicalBERT: 对医学文本建模用于再入院预测"},{"content":"","date":"1 March 2023","permalink":"/tags/nlp/","section":"Tags","summary":"","title":"nlp"},{"content":" 背景 # 在nlp领域，如何把词进行编码成数字，从而能输入到数学模型是需要考虑的：\n索引编码1：\n整数编码，特征之间的关系无法捕捉 one-hot编码的缺点：\n对于具有非常多类型的类别变量，变换后的向量维数过于巨大，且过于稀疏。 映射之间完全独立，并不能表示出不同类别之间的关系。 Embedding是什么 # 嵌入是将正整数（索引值）转换为固定尺寸的稠密向量2。这句话来着keras文档中对embedding层的解释，非常概括，不太容易理解，但确实概括了要干的事情。\n比如一句话，“我爱中国”对应的索引为[0,1,2,3]，要将这个索引转化为固定大小且稠密的向量来表示，而不是稀疏的one-hot编码。可以表示为$[[0.2, 0.5], [0.6,-0.1], [0.8, 0.4], [0.5, 0.5]]$，。\n词嵌入通常是8-1024维度，根据数据量的大小来调整，高维度的嵌入能更好的捕捉词之间的关系，但是需要更多的数据来训练。\nEmbedding是如何实现的 # 通过Embedding层实现，embedding层可以看作是一张从索引映射到稠密向量的查找表，当使用embedding层的时候，embedding层和神经网络其他层一样，权重是随机初始化的。根据你的训练任务，embedding层通过反向传播逐渐调整。\nembedding层的具体结构即[索引长度，emb维度]的权重矩阵也可以看作查询表，输入为整数索引，对应权重矩阵即词嵌入。skip-gram模型的前半部分即词嵌入。\n例如在tensorflow中，用于句子分类时的嵌入层，输入是整数索引，经过嵌入层、池化层、全连接输入训练可以得到嵌入层权重，即词嵌入。\nembedding_dim=16 model = Sequential([ vectorize_layer, Embedding(vocab_size, embedding_dim, name=\u0026#34;embedding\u0026#34;), GlobalAveragePooling1D(), Dense(16, activation=\u0026#39;relu\u0026#39;), Dense(1) ]) 应用 # 最常用的就是词嵌入表达，但是万物可嵌入。Embedding在输入数据没有较好的数据表示时，能将输入数据根据下游任务转化为可学习的高维度向量表示，比如输入的为单词、图片或者输入的为空间位置等。\nmnist数据集中的图片，可以通过嵌入层来表示，如下图所示，每个点代表一个图片(10000*784)，通过嵌入层，将图片的像素点转化为稠密的向量，然后通过t-SNE/pca降维，可以看到图片的空间分布。(generated by copilot) 在进行特征工程时，很难捕捉空间(时间)维度。通过使用深度学习嵌入层，我们可以通过提供一系列用户行为(作为索引)作为模型的输入来有效地捕捉这个空间维度。\n我的博客即将同步至腾讯云开发者社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=2cy4t3peazy8s\nWord embeddings | Text | TensorFlow\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n嵌入层 Embedding - Keras 中文文档\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"21 February 2023","permalink":"/posts/20230222-embedding/","section":"Posts","summary":"万物皆可Embedding","title":"Embedding是什么？"},{"content":" 简介 # Skip-gram1属于Word2Vec的一种，给定input，预测上下文，而CBOW（见补充）是通过上下文来预测input。\nWord2Vec模型分为两个步骤2：\n建立模型，这类方法与自编码模型有点像，建模不是最终目的； 通过模型获取嵌入词向量。 模型细节 # 整体框架图\n输入层 # 词不能直接输入神经网络模型，比如训练样本中有10000个不同的词，将某个词“我”或者“我们”进行one-hot编码，形成10000维的向量，其中“我”的地方为1，其他均为0。对于Skip-gram输入单个词向量，输出就是这个词附近的词组成的向量。\n模型输入输出均为10000维度向量$\\{0,1\\}^{10000}$\n隐含层 # 假如想用300个特征来表征词，那么隐含层单个神经元权重为[10000, 1]，300个神经元为[10000, 300] 上图可以理解为输入经过隐含层作用刚好得到单词的表征。输入和隐含层两部分可以进行词嵌入即embedding。\n输出层 # 模型输出为10000维度向量，要达到这样的输出层为10000个神经元，并且这些神经元输出和为1，使用softmax函数来达到这种效果。以下两个等式展示一个词向量在神经网络中的前向传播。\n$$ \\begin{bmatrix} 1 \u0026amp; 10000 \\end{bmatrix} \\times \\begin{bmatrix} 10000 \u0026amp; 300 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 300 \\end{bmatrix} \\ $$\n$$ \\begin{bmatrix} 1 \u0026amp; 300 \\end{bmatrix} \\times \\begin{bmatrix} 300 \u0026amp; 10000 \\end{bmatrix} = \\begin{bmatrix} 1 \u0026amp; 10000 \\end{bmatrix} \\ $$\n补充 # skip-gram名字由来3：\n首先n-gram是一系列连续的词（tokens），而skip-gram，或者skip-n-gram，skip的是token之间的gap，jumps over the是一个3-gram，那么(jumps, the)刚好skip了一个gram (over)。\n什么是Softmax？4\nSoftmax从字面上来说，可以分成soft和max两个部分。max故名思议就是最大值的意思。Softmax的核心在于soft，而soft有软的含义，与之相对的是hard硬。很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax。hardmax最大的特点就是只选出其中一个最大的值，即非黑即白。Softmax的含义就在于不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。经过使用指数形式的Softmax函数能够将差距大的数值距离拉的更大。\nCBOW是什么？\nContinuous Bag of Word Model连续词带模型，通过上下文来预测中间的词\nWord2Vec Tutorial - The Skip-Gram Model · Chris McCormick\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n理解 Word2Vec 之 Skip-Gram 模型 - 知乎\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nskip-gramm模型skip了什么？为什么叫skip-gramm模型？ - 知乎\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n一文详解Softmax函数 - 知乎\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"5 February 2023","permalink":"/posts/20230205-skip-gram-part1/","section":"Posts","summary":"","title":"Skip-gram模型（1）"},{"content":"","date":"4 February 2023","permalink":"/tags/medical-data/","section":"Tags","summary":"","title":"medical data"},{"content":"","date":"4 February 2023","permalink":"/tags/mimic-iv/","section":"Tags","summary":"","title":"mimic-iv"},{"content":"","date":"4 February 2023","permalink":"/categories/paper/","section":"Categories","summary":"","title":"Paper"},{"content":" 《The MIMIC Code Repository: Enabling reproducibility in critical care research》论文 引言 # 科学结果的可重复性越来越受到关注1； 医疗领域进入数字化革命（本文是2017年接收），引出形成MIMIC-III数据库； EHR二次分析需要临床专家和数据科学家的合作，在EHR数据库上推导或者定义一些概念是需要资源的，对于没有特别强的临床背景或者数据科学技能的人来说巨大障碍； 该文介绍MIMIC代码仓库，介绍与重症相关概念的导出以及相关假设条件等； 公开数据已经逐渐有了，公开相应的数据代码同样重要。加速并提升未来研究的一致性以及有效性。 代码仓库详情 # Concepts 从电子病历中提取重要概念的代码。比如提取AKI的模块 Executable documents 可执行的Notebooks文件，可重复的示例研究或者教程 Community 建立公开讨论便于社区成员贡献 概念concepts # 代码库中常用的概念\n疾病严重程度评分Severity of illness scores # 在回顾性数据库中难以计算\n大多都是在前瞻性实验中获取的； 常规收集的数据缺相应元素。有些特征未纳入结构化电子病历系统，另外则是对某种情况的患者没有统一的协议来定义状态 目前MIMIC代码库中有：\nacute physiology score(APS)-III simplified acute physiology score(SAPS) SAPS-II Oxford acute severity of illness score(OASIS) 器官衰竭Organ dysfunction scores # SOFA计算方式不同，由于GCS评分定义不同\nSequential Organ Failure Assessment(SOFA), Logistic Organ Dysfunction system(LODS)\n治疗时间Time of treatment # 由于数据获取的限制，许多药物和确切的治疗时间无法得出，需要根据临床经验识别其他可替代的数据\n机械通气时长：识别机械通气时长需要复杂的逻辑规则（文中图3） 血管加压药物使用 CRRT 脓毒症sepsis # sepsis定义有多种版本，这里给出了Angus 2001，Martin 2003，Iwashyna 2014三个版本\n共病Comorbidities # 给出了4个版本\nElixhauser A 1998 American Health and Research Quality group（AHRQ） Quan 2005 Van Walraven 2009 concept指南 # 可执行文档 # 当数据和代码都公开可获取，提供一种研究可以被重现的框架，基于Rmd或notebook给出实例。\nHsu 2015研究复现 indwelling arterial catheters and their association with in-hospital mortality for hemodynamically stable patients with respiratory failure aline.ipynb提取数据 aline.Rmd数据分析 教程 definition of CRRT introduction to SQL a step-by-step guide to selecting a study cohort an outline of the data-capture process 社区 # 让研究人员和数据维护人员、临床人员共同提升代码\n结论 # 公开数据库的案例已经不少，为了让研究更加透明，也需要公开相应数据分析和数据处理的代码\n补充 # 代码库地址：https://github.com/MIT-LCP/mimic-code 之前以MIMIC-III为主，现在mimic-iii和mimic-iv合并在一起了 mimic数据库为了让研究者访问更加方便，很大一个改变是部署在云上比如google的云平台，云平台上需要big query语法来访问，所以现在代码库关于数据提取的代码更新以big query为主，需要通过脚本转化为适合postgres语法 Open a terminal in the concepts folder. Run convert_bigquery_to_postgres.sh. e.g. bash convert_bigquery_to_postgres.sh This file outputs the scripts to the postgres subfolder after applying a few changes. This also creates the postgres_make_concepts.sql script in the postgres subfolder. 从代码仓库导出的概念concepts都放到mimic_derived数据集里 Johnson, A. E. W., Stone, D. J., Celi, L. A. \u0026amp; Pollard, T. J. The MIMIC Code Repository: enabling reproducibility in critical care research. J Am Med Inform Assn 25, 32–39 (2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"4 February 2023","permalink":"/posts/20230204-mimic-code/","section":"Posts","summary":"","title":"公开重症监护数据库MIMIC代码仓库介绍"},{"content":"背景 # 本文1首先介绍mimic项目的由来，医院数据归档系统都不是为研究设计的，难以访问以及查询； MIMIC从0-4发展简介； 其他数据库简介 eICU-CRD，美国多中心200859例ICU记录 AmsterdamUMCdb，荷兰单医学中心20109例患者23106次入院记录 HiRIDData details，瑞士34000入院患者高分辨率数据（每两分钟一个数据点，712中常规手机生理数据） HiRID has a higher time resolution than other published datasets, most importantly for bedside monitoring with most parameters recorded every 2 minutes 去隐私化方法可以借鉴 PIC，中国12881例患者13941次儿科ICU记录 问题，当前数据库都只是单模态的数据或各有特点，且临床实践发展变化快，数据库需要不断更新完善； MIMIC-IV是当代的综合多模态数据库 方法 # mimic-iv数据库是怎么建的？\n获取、转化、去隐私化 获取 # BIDMC医院常规的临床数据存储在microsoft SQL中，通过VPN转移到MIT服务器的PostgreSQL，补充数据如ICD、患者死亡信息等外部导入到PostgreSQL\n队列：2008-2019，18岁以上 床旁信息来自于MetaVision，医院EHR会通过HL7推信息到MetaVision 外部数据主要说明DRGs和ICD是如何处理以及死亡信息如何处理 DRGs和ICD由于数据库跨度时间长以及不同版本，在数据库中把这些都导入了 死亡信息通过与马萨诸塞州生命记录和统计登记处进行匹配，而非社保档案 转化 # 转化有两条原则\n与MIMIC-III保持兼容 尽量减少处理过程让公开数据与临床实践数据保持一致 数据被分为三个组：hosp、icu和note\nhosp：admission/discharge/transfer（ADT），实验室检查结果，微生物培养，处方，管理数据 icu：患者出入量、输液、操作、记录到的观测值等 note：出院总结和放射学报告，也创建了相对应的自由文本结构化表“实体-属性-值” 去隐私化 # 遵从The Health Insurance Portability and Accountability Act（HIPAA）条款规定了18项标识符，包括姓名、地址、年龄等需要去掉 日期移动，但时间点间距保留 结合了两个公开的算法23从自由文本中移除个人健康信息（PHI） 两个算法都没捕捉到的，从数据库中移除？这个没写具体怎么做 数据记录结果 # 这一部分类似传统论文的结果，详细介绍了hosp、icu、note模块里面的数据情况，对各表进行介绍\nHosp # 首先介绍模块里主要键值以及表之间连接关系，与MIMIC-III一致\n患者基本信息：patients、admissions和transfers表\n患者时间信息，anchor_year、anchor_age、anchor_year_group这几个重要项 anchor_year锚定年份，由于时间都是去隐私化的平移过的，这里可以看作平移之后的参考年份 anchor_year_group是真实年份的区间 患者死亡信息最多到患者出院1年为止 管理信息：services、poe、poe_detail表\nservices 患者住院期间所受到的医疗服务 poe 医嘱录入系统：治疗和操作 计费信息：diagnoses_icd、procedures_icd、drgcodes、hcpcsevents\n检查结果：microbiologyevents、labevents\n药物信息：prescriptions、pharmacy、emar、emar_detail\n处方、药房信息； 2016年部署electronic Medicine Administration Record，eMAR； 看起来关系比较复杂使用得结合实例 ICU # chartevents, d_items, datetimeevents, icustays, inputevents, outputevents, and procedureevents表\nNote # 出院总结：\n主诉、现病史、既往病史、简要病程、体格检查和出院诊断 放射学报告：\nx射线、CT、MRI、超声 技术验证结果 # 完整性检查 数据库自身是完备的，不会出现某个患者的信息只出现一张表其他表都找不到的情况 一致性检查 数据结果与一些常识一致 去隐私化检查 使用笔记 # 如何获取 参加课程 签署data use agreement, DUA 代码库以及模版 思考 # MIMIC数据库一步一步发展已经来到了第四版，很好的体现了科学研究的可持续发展。 与MIMIC-III论文写法不同，mimic-iv对数据库构建过程写的更加详细，可操作性更高，而mimic-iii由于没有很好的把这个过程结构化抽象分成几步显得构建细节不足，而mimic-iii论文表格比较丰富，对数据库进行了一些粗粒度的介绍，mimic-iv论文没有。个人认为iv这篇论文写的更好一点。 伴随着数据类型越来越多、数据库越来越多，数据分析人员也需要掌握更多类型数据预处理方法。面对文本、图像、波形的多模态数据分析或者在不完备数据情况下模型的不确定性成为两个相对应的研究方向。 Johnson, A. E. W. et al. MIMIC-IV, a freely accessible electronic health record dataset. Sci Data 10, 1 (2023).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNeamatullah, I. et al. Automated de-identification of free-text medical records. BMC medical informatics and decision making 8,1–17 (2008).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJohnson, A. E. W., Bulgarelli, L. \u0026amp; Pollard, T. J. Deidentification of free-text medical records using pre-trained bidirectional transformers. In Proceedings of the ACM Conference on Health, Inference, and Learning, 214–221 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"17 January 2023","permalink":"/posts/20230117-mimiciv/","section":"Posts","summary":"《MIMIC-IV, a freely accessible electronic health record dataset》论文笔记","title":"公开重症监护数据库MIMIC-IV介绍"},{"content":"","date":"13 January 2023","permalink":"/categories/class/","section":"Categories","summary":"","title":"Class"},{"content":"","date":"13 January 2023","permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"deep learning"},{"content":" 1.Introduction to Deep learning # 震撼，第一节课直接放大招，用自己拍摄的视频和奥巴马合成来介绍这门课程。 不管老师在课程上讲什么，希望你们能真正的思考为什么这一步是重要而且必须的，正是这些思考才能做出真正令人惊讶的突破。 2.Deep Sequence Model # Three way to solve gradient vanish\nGated Cells LSTM Forget Store Update Output Attention [[Transformer]] 3.Deep Computer Vision # 介绍卷积操作，是一种提取特征的方法生成feature maps（还有其他的方法可以用吗？然后效果还不错）； 与全连接相比的优点； Fast RCNN用于目标检测，怎么实现推荐特定区域图像？ 医学图片分割 总结： 原理 CNN架构 应用 4.Deep Generative Models # what 目标: 来自于一些分布中的训练样本，通过这些样本学习模型来表征这个分布； how 密度估计；神经网络适合来进行高维度表征； why Debiasing: Capable of uncovering underlying features in a dataset Outlier detection: how can we detect when we encounter something new or rare? Latent variable representation: 举例事物的投影，只能看见影子即表象，而被灯光照射的实物是看不见的即隐变量；要做的是通过观察到的投影来对实物进行建模 Autoencoder: reconstruction loss 完全是确定性性 VAEs：normal prior + regularization reconstruction loss + regularization term encoder: $q_\\phi(z|x)$ decoder: $p_\\theta(x|z)$ KL-divergence: $D(q_\\phi(z|x)||p(z))$ GANs make a generative model by having two neural networks compete with each other ⭐️CycleGAN: domain transformations 视频开头的视频就是用这个合成 5.Deep reinforcement learning # Reward: $$R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \u0026hellip;$$ Q-function: expected total future reward $$Q(s_t, a_t) = E[R_t|s_t, a_t]$$ Policy: to infer the best action to take at its state, choose an action that maximizes future reward $$\\pi^*(s)=\\mathop{\\arg\\max}\\limits_{s}Q(s, a)$$ Value Learning find $Q(s, a)$ $a = \\mathop{\\arg\\max}\\limits_{a}Q(s, a)$ Police Learning find $\\pi(s)$ sample $a\\sim\\pi(s)$ Deep Q Network(DQN) Policy Gradient AlphaGo 6.DL Limitations and New Frontiers # limitations Generalization data is important Uncertainty in Deep learning adversarial attack Algorithmic Bias Frontiers encoder many real world data cannot be captured by standard encodings GCN（Graph Convolutional Networks） Automated AI 7. LiDAR for Autonomous Driving # @INNOVIZ\nCamera Vs LiDAR 互补，视线不好的情况 冗余能保证准确 Safety and Comfort 8. Automatic Speech Recognition # @Rev\nConformer CTC 9. AI fore Science # Principled AI Algorithms for challenging domains @Caltech\n10. Uncertainty in Deep Learning # longer version：NeurIPS 2020 Tutorial @Google AI Brain Team\nReturn a distribution over predictions rather than a single prediction Out-of-Distribution Robustness covariate shift: distribution of features changes open-set recognition: new classes may appear at test time label shift: distribution of label changes sources of uncertainty Model uncertainty 认知上的不确定性 Data uncertainty human disagreement label noise measurement noise missing data how to compute BDN GP Deep Ensemble MCMC multi-input and multi output（MIMO） how to communicate with uncertainty? 7-10讲很一般，一个复杂的主题，需要将背景讲清楚，公司讲东西也没啥具体细节。\nRef # 【双语字幕】MIT《深度学习导论(6.S191)》课程(2021)_哔哩哔哩_bilibili introtodeeplearning.com MIT 6.S191: Deep Generative Modeling - YouTube ","date":"13 January 2023","permalink":"/posts/20230113-introduction-deep-learning/","section":"Posts","summary":"","title":"MIT 6.S91 Introduction Deep Learning Notes"},{"content":" Introduction # 大数据让复杂模型的优势明显 提出一种新颖统一的方法用于模型解释 用模型的方法来解释复杂模型（用魔法打败魔法） 提出SHAP值作为各种方法近似统一特征重要度度量 提出新的SHAP值估计方法 Interpretation model properties # 描述解释模型需要有的三个性质，而现在解释方法的缺陷有哪些1\n局部准确性：如果x‘是x的简化特征，对应解释模型g(x\u0026rsquo;) = f(x)，即解释模型在给定的特征情况下能解释为什么模型预测值是这么多。 缺失性：当x\u0026rsquo;=0的时候，贡献度$\\phi$为0 一致性：模型改变导致特征变的更重要时，贡献度也应该变大 Additive Feature Attribution methods # 一大类方法中解释模型是一系列二元变量的线性函数 称为Additive Feature Attribution methods（AFA）相加特征归因方法 $$g(z\u0026rsquo;) = \\phi_0 + \\sum_{i=1}^{M}\\phi_i z\u0026rsquo;_{i}$$ $z\u0026rsquo; \\in {0, 1}^M$\nClassic Shapley Value Estimation # $$\\phi_{i} = \\sum_{S \\subseteq F\\backslash{i}}\\frac{|S|!(|F|-|S|-1)!}{|F|!}[f_{S\\cup{i}}(x_{S\\cup{i}})-f_{S(x_S)}]$$ 基于上面公式精确计算很麻烦，特征的排列有$2^F$种，计算量巨大\nSHAP # SHAP分为模型无关和模型相关两类方法用来近似求解，模型无关的代表是kernelSHAP，而模型相关的代表则有DeepSHAP和TreeSHAP，一个是针对深度学习，一个是针对树模型。\nKernel SHAP # LIME # LIME2（Local interpretable model-agnostic explanations)（why should I trust you: Explaining the predictions of any classifier）通过生成的包含需要解释点周围的扰动数据和基于黑箱模型预测结果的数据集，训练一个可以解释的模型，比如逻辑回归、决策树，这个可解释模型需要在解释点周围达到较好的效果。 $$\\xi = \\mathop{\\arg\\min}\\limits_{g\\in G}L(f,g,\\pi_x) + \\Omega(g)$$\nf为需解释模型 g为可能的解释模型 $\\pi_x$为定义实例周围多大范围 算法过程：\n选择需要解释感兴趣的实例 对其进行扰动，并得到黑箱模型对应结果产生新数据集 根据与实例的接近程度，对新数据集进行赋予权重 基于新数据集和上述损失函数求解可解释模型 解释预测值 Figure 3: Toy example to present intuition for LIME KernelSHAP（Linear LIME + Shapley value） # LIME（Local interpretable model-agnostic explanations）方法的拓展，通过修改LIME需要求解loss等式参数，其主要思路是利用核变换，让$\\phi$符合shapley value3 算法过程4：\n根据$z_k^{\u0026rsquo;} \\in {0, 1}^M$选择k个样本 将$z_k^{\u0026rsquo;} \\in {0, 1}^M$转化为原始特征值并计算黑箱模型预测值 基于SHAP kernel计算$z_k^{\u0026rsquo;}$ 样本权重，$z_k$里面1的个数不一样权重就不一样 拟合线性模型 从线性模型中返回Shapley values SHAP核为(推导过程见2补充材料)： $$\\pi_{x\u0026rsquo;}(z\u0026rsquo;)= \\frac{(M-1)}{(M choose |z\u0026rsquo;|)|z\u0026rsquo;|(M-|z\u0026rsquo;|)}$$ $$L(f,g, \\pi_{x\u0026rsquo;})=\\sum_{z\u0026rsquo;\\in Z}[f(h_x^{-1}(z\u0026rsquo;))-g(z\u0026rsquo;)]^2\\pi_{x\u0026rsquo;}(z\u0026rsquo;)$$ x\u0026rsquo;为简化输入，$x=h_x(x\u0026rsquo;)$, $z\u0026rsquo; \\subseteq x\u0026rsquo;$ 其中第二步：The function h maps 1’s to the corresponding value from the instance x that we want to explain. For tabular data, it maps 0’s to the values of another instance that we sample from the data. This means that we equate “feature value is absent” with “feature value is replaced by random feature value from data”. Deep SHAP # DeepLIFT（Learning Important FeaTures） # DeepLIFT5方法使用神经元的激活与其“参考”进行比较，其中参考是神经元在网络获得“参考输入”时具有的激活状态（参考输入根据具体任务的内容定义）。该方法赋予每个特征重要度分数之和等于预测值与基于参考输入的预测值之间的差异6。 能解决基于梯度方法的不足，例如参考的差异不是0的情况下梯度仍然可能是0。 $$\\sum_{i=1}^n C_{\\Delta x_i \\Delta t} = \\Delta t \\tag1$$ $\\Delta t = t - t^0$, 神经元输出与参考输出的差异，$\\Delta x$输入相对参考输入的变化， C即特征的贡献\n$$m_{\\Delta x\\Delta t} = \\frac{C_{\\Delta x\\Delta t}}{\\Delta x} \\tag2$$ 定义乘子multiplier，满足链式法则 算法步骤7：\n定义参考值 choosing a good reference would rely on domain-specific knowledge, and in some cases it may be best to compute DeepLIFT scores against multiple different references 比如图像用全0或者模糊版本，基因数据用本底期望频率 区分正负贡献（2019） 贡献度规则 线性层直接是系数$w* \\Delta x_i$ 非线性变化是$m_{\\Delta x\\Delta y} = \\frac{C_{\\Delta x\\Delta y}}{\\Delta x} =\\frac{\\Delta y}{\\Delta x}$ DeepSHAP(DeepLIFT + Shapley value) # 尽管kernelSHAP是适用于所有模型的包括深度学习模型的一种可解释方法，但是有没有能利用神经网络特性的可解释方法从而提高计算效率。 DeepLIFT计算的分数近似于Shapley value?并不是这个思路\nthe Shapely values measure the average marginal effect of including an input over all possible orderings in which inputs can be included. If we define “including” an input as setting it to its actual value instead of its reference value, DeepLIFT can be thought of as a fast approximation of the Shapely values8 Though a variety of methods exist for estimating SHAP values, we implemented a modified version of the DeepLIFT algorithm, which computes SHAP by estimating differences in model activations during backpropagation relative to a standard reference. figure from ref[3] 基于Shapley value的定义以及公式可以看出重要的一部分即边际效应，即模型包含该特征减去未包含该部分。在上述一个神经网络模块里面，特征顺序选择都不存在。在认为包含特征即相对于参考输入是真实输入的情况下，把包含特征后乘子直接链式法则做为SHAP值近似公式 在上述简单网络组件里面，输入到输出之间可以看作线性近似从而得到公式16 把用实际值代替参考值看作是包含某个特征，DeepLIFT方法与DeepSHAP似乎看不到区别? 6 – Interpretability – Machine Learning Blog | ML@CMU | Carnegie Mellon University\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRibeiro, M. T., Singh, S. \u0026amp; Guestrin, C. ‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1135–1144 (2016) doi:10.1145/2939672.2939778.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Unified Approach to Interpreting Model Predictions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n9.6 SHAP (SHapley Additive exPlanations) | Interpretable Machine Learning\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ndeeplift 0.6.13.0 on PyPI - Libraries.io\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExplainable Neural Networks: Recent Advancements, Part 3 | by G Roshan Lal | Towards Data Science\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShrikumar, A., Greenside, P., Shcherbina, A. \u0026amp; Kundaje, A. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences. Preprint at https://doi.org/10.48550/arXiv.1605.01713 (2017).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShrikumar, A., Greenside, P. \u0026amp; Kundaje, A. Learning Important Features Through Propagating Activation Differences. Preprint at http://arxiv.org/abs/1704.02685 (2019).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"6 January 2023","permalink":"/posts/20230106-shap/","section":"Posts","summary":"","title":"《A Unified Approach to interpreting Model Predictions》论文解读"},{"content":"","date":"6 January 2023","permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"machine learning"},{"content":"","date":"6 January 2023","permalink":"/tags/shap/","section":"Tags","summary":"","title":"SHAP"},{"content":"I am currently an engineer of Center for Artificial Intelligence In Medicine in a Hospital. My research focuses on three areas:\nSecondary analysis EHRs Wearable device data analysis Multimodal fusion learning in healthcare ","date":"3 November 2022","permalink":"/about/","section":"","summary":"I am currently an engineer of Center for Artificial Intelligence In Medicine in a Hospital.","title":"About"},{"content":"","date":"1 April 2020","permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"pytorch"},{"content":" Deep Learning = Learning Hierarchical Representations 深度学习即学习层次的表征。 1. 卷积神经网络 # 1.1 神经网络可视化（Visualization of neural networks） # 神经网络每一层的操作有点像将空间某些区域进行折叠\n1.2 卷积神经网络的起源（Convolutional Neural Network；CNN） # 受到Fukushima在视觉皮层建模方面的启发，使用简单/复杂的细胞层次结构，结合有监督的训练和反向传播，由Yann LeCun教授于88-89年在多伦多大学开发了第一个CNN。\nFukushima的工作具体是什么呢？\n手写数字识别。首次提出应用多层简单或者复杂的细胞结构建模，特征：手工加无监督聚类学习。无反向传播。\n1.3 卷积神经网络分解 # 通用的CNN架构能被分解为以下几个基本结构。\n标准化（Normalisation）:对比度标准化等 滤波器组（Filter banks）:边缘检测等 非线性化（Non-linearities）:稀疏化、ReLU等 池化（pooling）:最大池化（max pooling）等 2. 自然信号数据（Natural Signals） # 2.1 自然信号数据特性 # 周期性：在时域很多模式都会重复出现 局部性：相邻的点较相远的点来说更具关联性 合成性：复杂的事物可以由简单的事物组合而成。字母-\u0026gt;单词-\u0026gt;句子-\u0026gt;文章 2.2 对应神经网络中的处理方法 # 周期性$\\rightarrow$参数共享\n如果数据存在周期性，可以使用参数共享，即卷积核。 局部性$\\rightarrow$稀疏\n如果数据存在局部性，那么每个神经元只需要与前几个神经元连接 合成性$\\rightarrow$多层\n即神经网络中多层网络合成最终的结果 3. Pytorch实现Mnist手写字识别 # # load package and data import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms import matplotlib.pyplot as plt device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) # 神经网络模型偏爱标准化数据，原因是均值为0方差为1的数据在sigmoid、tanh经过激活函数后求导得到的导数很大， # 反之原始数据不仅分布不均（噪声大）而且数值通常都很大（本例中数值范围是0~255），激活函数后求导得到的导数 # 则接近与0，这也被称为梯度消失。 # 目录放自己下载好的mnist目录，没有下载将download=True,自己新建一个存放数据目录即可 train_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;../LSTM_mnist/mnist\u0026#39;, train=True, download=False, transform=transforms.Compose([ transforms.ToTensor(), # mnist数据集均值0.1307，标准差0.3081 transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=64, shuffle=True) test_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;../LSTM_mnist/mnist\u0026#39;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=1000, shuffle=True) # define model class SimpleCNN(nn.Module): def __init__(self, input_size, n_feature, output_size): super(SimpleCNN, self).__init__() self.n_feature = n_feature # 关于nn.Conv2d()中参数的解释 # in_channels (int): Number of channels in the input image # out_channels (int): Number of channels produced by the convolution # kernel_size (int or tuple): Size of the convolving kernel # default stride=1, padding=0, dilation=1, groups=1 # [groupsc参数详解](https://www.jianshu.com/p/20ba3d8f283c) # [图解卷积神经网络中stride, padding等操作可视化](https://github.com/vdumoulin/conv_arithmetic) # input: (N, C_in, H_in, W_in) self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5) self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5) self.fc1 = nn.Linear(n_feature*4*4, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.conv1(x) # Mnist数据原始大小（28*28）28-5+1 = 24 (24*24*n_feature) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2) # (12*12*n_feature) x = self.conv2(x) # 12-5+1 = 8 (8*8*n_feature) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2) # (4*4*n_feature)这里解释了上面全连接时为啥是4*4 x = x.view(-1, self.n_feature*4*4) x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.log_softmax(x, dim=1) return x # hyper parameters epochs = 1 input_size = 28*28 output_size = 10 n_features = 6 lr = 0.01 model = SimpleCNN(input_size, n_features, output_size) model.to(device) # optimizer optimizer = optim.SGD(model.parameters(), lr, momentum=0.5) print(\u0026#39;Number of parameters: {}\u0026#39;.format(get_n_params(model))) # model train for epoch in range(epochs): model.train() for i, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) output = model(data) loss = F.nll_loss(output, target) optimizer.zero_grad() loss.backward() optimizer.step() if i % 100 == 0: print(\u0026#39;Train Epoch [{}/{}], [{}/{} ({:.0f}%)], Loss: {:.4f}\u0026#39;.format( epoch+1, epochs, i*len(data), len(train_loader.dataset), 100*i/len(train_loader), loss.item())) # model eval model.eval() test_loss = 0 correct = 0 accuracy_list = [] for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction=\u0026#39;sum\u0026#39;).item() pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability correct += pred.eq(target.data.view_as(pred)).cpu().sum().item() test_loss /= len(test_loader.dataset) accuracy = 100. * correct / len(test_loader.dataset) accuracy_list.append(accuracy) print(\u0026#39;\\nTest set Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\u0026#39;.format( test_loss, correct, len(test_loader.dataset), accuracy)) # 几个预测的实例可视化 for i in range(10): plt.subplot(2, 5, i+1) plt.imshow(data[i][0]) plt.title(\u0026#34;Prediction: {}\u0026#34;.format( output.data.max(1, keepdim=True)[1][i].item())) plt.show() 4. 补充 # 可以做一个有趣的实验即打乱图片中的像素后CNN识别正确率下降，而全连接网络则不会，即与最开始提到的三个特性以及对于神经网络采取的假设是吻合的。 参考2中是对卷积神经网络全面的介绍，包括CNN中常用那些层，以及常用的模型和参数多少计算。 ref # NYC PyTorch Deep Learning课程网站 cs231n convolutional networks pytorch官方文档Conv2d 课程convnet.ipynb ","date":"1 April 2020","permalink":"/posts/20200401-pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02/","section":"Posts","summary":"","title":"PyTorch深度学习（2）"},{"content":"","date":"23 March 2020","permalink":"/tags/xgboost/","section":"Tags","summary":"","title":"xgboost"},{"content":" 在使用xgboost方法调参时，对其中个别参数不是特别理解。故重新读了一遍原论文。\n1. 引言 # 阐述机器学习和数据驱动的方法应用时两个重要的因素：\n能捕捉数据间复杂依赖关系的模型 可扩展的学习系统，可以从大量数据中学习 在目前常用的方法中，梯度提升树（gradient tree boosting）在许多场景中效果都不错，作者列举了一些。提出xgboost方法在比赛以及各类问题中的应用。\n叙述XGBoost的优点：运行更快、拓展性更好。创新点包括：\n高度可拓展的端到端提升树（tree boosting）系统 用于高效计算的加权分位数图（weighted quantile sketch） 新颖的稀疏感知算法（sparsity-aware algorithm），用于并行树学习 有效的缓存优化以及块（cache-aware block）结构用于外存（out-of-core）树学习 关于以上几点在正文中详解。 论文结构：\n提升树（tree boosting）简介以及目标函数正则化 分裂点寻找的方法 系统设计，包括为每个优化提供量化支持的结果 相关工作 详细的端到端评估 总结 2. 提升树（Tree Boosting）简介 # 首先需要了解CART（Classification And Regression Tree）算法，对于cart分类树和回归树分别采用了：Gini系数、和方差度量方式来划分节点[1]。例如回归树，对于划分特征A, 划分点s使两边数据集D1和D2,求出使 D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小。 $$\\underline{min}_{\\text{A,s}}[\\underline{min}_{\\text{c1}}\\sum_{x_i\\in{D1(A,s)}}(y_i - c1)^2+\\underline{min}_{\\text{c2}}\\sum_{x_i\\in{D2(A,s)}}(y_i - c2)^2]$$ 其中，c1为D1的样本输出均值，c2为D2的样本输出均值, 回归树采用叶子节点的均值或者中位数来预测输出结果，$y_i$即样本的label，此时的输出值即下文中用到的$w_{q(x)}$。\n2.1 正则化目标函数 # 对于这种类型的集成树模型，用K棵树的结果来预测最后的结果（式1），那么问题来了我们怎么来求这些树的参数，每棵树都可以看做一个函数$f_i$包含树的结构以及最后叶节点权重，集成模型不像传统优化问题一样通过简单用梯度下降可以对所有的树进行学习求解，所以，在这里用到了加法策略，即固定已经学习到的，每次加一棵树来进行学习（式3）。 $$\\hat{y_i} = \\phi(x_i) = \\sum_{k = 1}^{K} f_k(x_i)\\tag1$$ 其中$f(x) = w_{q(x)}$，每个$f_k$对应一个独立的树结构q以及其叶节点权重w，为了学习模型中的参数，最小化下面正则化的目标函数。 $$L(\\phi) = \\sum_i l(\\hat{y_i}, y_i) + \\sum_k \\Omega(f_k)\\tag2$$\n$$\\Omega(f_k) = \\gamma T + \\frac{1}{2}\\lambda||w||^2$$ T是树的叶节点数\n2.2 梯度提升树（Gradient Tree Boosting） # 第t次预测值等于t-1加上第t棵树的结果 $$\\hat{y_i} = \\hat{y_i}^{t-1} + f_t(x_i)\\tag3$$ 此时目标函数(式2)可以写成 $$L^{(t)} = \\sum_{i=1} ^n l(y_i, \\hat{y_i}^{(t-1)}+f_t(x_i)) + \\Omega(f_t)\\tag4$$ 该式子的二阶近似可以表达为(式5），可以参考补充中二阶泰勒展开的一般形式 $$L^{(t)} \\approx \\sum_{i=1} ^n [l(y_i, \\hat{y_i}^{(t-1)}) + g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)\\tag5$$ 其中 $g_i=\\partial_{\\hat{y}^{(t-1)}}{l(y_i, \\hat{y_i}^{(t-1)})}， h_i=\\partial_{\\hat{y}^{(t-1)}}^2{l(y_i, \\hat{y_i}^{(t-1)})}$\n式5中去掉常数项，即label与第t-1次结果的损失函数，可以得到：\n$$\\sum_{i=1}^n[g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)\\tag6$$\n式6即对新树的优化目标函数，进一步合并可以写为：\n$$obj^{t} \\approx \\sum_{i=1}^{n}[g_i w_{q(x_i)} + \\frac{1}{2}h_i w^2_{q(x_i)}] + \\gamma T+ \\frac{1}{2}\\lambda \\sum _{j=1}^T w_j^2$$\n定义$I_j = {i|q(x_i)=j}$即叶节点j上的实例。\n$$obj^{t} \\approx \\sum_{j=1}^T[(\\sum_{i\\in{I_j}}g_i)w_j+\\frac{1}{2}(\\sum_{i\\in{I_j}}h_i+\\lambda)w_j^2]+\\gamma T\\tag7$$\n上式7对$w_j$求导即可求出w最优值\n$$w_j^* = -\\frac{\\sum_{i\\in{I_j}}g_i}{\\sum_{i\\in{I_j}}h_i+\\lambda} $$\n令$G_j = \\sum_{i\\in{I_j}}g_i，H_j=\\sum_{i\\in{I_j}}h_i$\n此时，对应的最小值为: $$obj^* = -\\frac{1}{2}\\sum_{j=1}^T\\frac{G_j^2}{H_j+\\lambda}+\\gamma T$$ $\\color{red}\\frac{G_j^2}{H_j+\\lambda}$越大，loss越小，所以对叶节点进行分裂，分裂后增益定义为 $$Gain=\\frac{1}{2}[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}]-\\gamma\\tag8$$\n2.3 缩减和列抽样（Shrinkage and Column Subsampling） # 除了在目标函数中引入正则项，在防止过拟合方面xgboost还运用了两项技术，给每一步tree boosting得到的结果一个权重$\\eta$，来降低每一步的影响从而给后面树的形成留下空间，比喻成优化问题中的学习率缩减；同时还用到随机森林中的列抽样，即随机特征筛选。\n3. 分裂点寻找算法 # 3.1 精确贪婪算法（Basic Exact Greedy Algorithm） # 即按照2.2中式8来寻找分裂点 pythonscikit-learn，Rgbm，单机的xgboost都支持。\n3.2 近似算法（Approximate Algorithm） # 精确贪婪算法由于列举了所有可能的分裂点，在数据量很大不能全部写入内存时会导致不是那么高效。所以提出近似算法。对于每个特征，只考察分位点，减少计算复杂度。 近似算法存在两个变种：\nglobal: 学习每棵树前，提出候选分裂点 local: 每次分裂前，重新提出候选分裂点 3.3 加权分位数图（Weighted Quantile Sketch） # 近似算法中最重要一点即提出候选分裂点，xgboost不是简单的按照样本个体进行分位，而是以损失函数二阶导数值作为权重进行分位数分裂。如何寻找二阶导数分位点，首先是利用权重计算排序函数，然后相邻相减值作为判断依据。问题是为什么会想到利用损失函数二阶导数值作为权重来划分。 文中给出式6可以变形为 $$\\sum_{i=1}^n\\frac{1}{2}h_i(f_t(x_i)-g_i/h_i)^2 + \\Omega(f_t) + constant\\tag9$$ 指出该式恰好是权重平方差损失函数，权重$h_i$以及label $g_i/h_i$ 自己从式6变不到式9，觉得中间符号是+还差不多。 看有人理解说变成式10才对。是否作者真的是这样想的，不得而知。欢迎指正。 $$\\sum_{i=1}^n\\frac{1}{2}h_i(f_t(x_i)-(-g_i/h_i))^2 + \\Omega(f_t) + constant\\tag{10}$$ stackexchange上关于理解xgboost近似分裂点\n3.4 稀疏值感知分裂（Sparsity-aware split finding） # 造成稀疏值的原因：1）缺失值 2）统计过程中频繁的0值输入 3）one-hot编码以及其他特征工程 所以让算法注意数据中稀疏规律很重要，遍历所有特征，在划分子节点时，统一将该特征的缺失值划分到右支或者左支，计算最大的gain。\n$\\color{red}这里也有个疑问就是为什么排序第一次是升序，第二次是降序$\n4. 系统设计 # 4.1 分块并行（Column Block for Parallel Learning） # 基于树学习过程中最耗时的是将数据排序，为了减少排序的时间成本，提出基于内存的block结构。\n在Exact greedy算法中，将整个数据集存放在一个Block中 在近似算法中，使用多个Block，每个Block对应原来数据的子集。不同的Block可以在不同的机器上并行计算 4.2 缓存优化 # 这里指利用CPU缓存对算法进行优化。\n4.1中column block按特征大小顺序存储，相应的样本的梯度信息是分散的，造成内存的不连续访问，降低CPU cache命中率。 优化方法：\n对于精确贪婪算法，预取数据到buffer中（非连续-\u0026gt;连续），再统计梯度信息。 对于近似算法，调节block的大小，设置过大则容易导致命中率低，过小则容易导致并行化效率不高。 4.3 外存计算 # 除了处理器以及内存，利用磁盘空间来处理不能进入内存的数据也十分重要，数据划分为多个Block并存放在磁盘上。计算的时候，使用独立的线程预先将Block放入主内存，因此可以在计算的同时读取磁盘。在减少计算资源开销以及提高磁盘输入输出方面主要用到以下技术：\nBlock压缩，按列压缩，加载到主内存时由独立线程动态解压缩。具体压缩技术参看原文。 Block Sharding，将数据划分到不同硬盘上，提高磁盘吞吐率。 5. 端到端评估 # 利用4个数据集对xgboost评估：\n分类问题 排序问题 外存计算实验 分布计算实验 这几个方面进行评估，详细结果见论文。\nref # CART分类树与回归树 Markdown数学公式 Mathjax应用在网页 XGBoost.ppt readthedocs xgboost tutorials推荐 gbdt.ppt xgboost原文 补充 # 文中很多术语翻译可能有不恰当的地方，欢迎指出。 二阶泰勒展开的一般形式： $$f(x^t) = f(x^{t-1}+\\Delta x)\\approx{f(x^{t-1})+ f^{\\prime}(x^{t-1})\\Delta{x}+f^{\\prime\\prime}(x^{t-1})\\frac{\\Delta x^2}{2}}$$ 式4中加入loss function是mean squared error(MSE)，可以求出相应的gi， hi作为一个特例来验证该做法。 基于树的算法理解时带着这几个问题去理解每一步是用来做什么的：选择哪个特征进行分裂？在特征什么点位进行分裂？分裂后叶节点取什么值？ 分别对应：遍历每个特征，加权分位数图，$w_j$\n对于系统设计中应用到的技术理解不是十分深刻，对应一个算法如何从计算机硬件的方方面面考虑去优化对非专业领域研究者还是比较难 ","date":"23 March 2020","permalink":"/posts/20200323-%E9%87%8D%E8%AF%BBxgboost/","section":"Posts","summary":"","title":"重读XGBoost"},{"content":"","date":"9 March 2020","permalink":"/tags/lstm/","section":"Tags","summary":"","title":"lstm"},{"content":" 在去年介绍的一篇paper中，应用了多任务RNN来解决问题，当时RNN指的即是LSTM。本文介绍LSTM实现以及应用。\n1. LSTM简介 # 循环神经网络要点在于可以将上一时刻的信息传递给下一时刻，但是在需要长程信息依赖的场景，训练一个好的RNN十分困难，存在梯度爆炸和梯度消失的情况。LSTM通过刻意的设计来解决该问题。\n简单的RNN网络中重复的模块只有一个简单的结构，例如一个relu层，而在LSTM中重复的模块拥有4个不同的结构相互交互来完成。\n1.1 首先决定从cell中丢弃什么信息 # $$f_t = \\sigma(W_f*[h_{t-1}, X_t] + b_f) \\tag1$$ sigma函数在0到1选择代表丢弃与否\n1.2 什么样的新信息存放到cell中 # $$i_t = \\sigma(W_i*[h_{t-1}, x_t] + b_i) \\tag2$$\n$$\\widetilde{C_t} = tanh(W_c*[h_{t-1}, x_t] + b_c) \\tag3$$\n$$C_t = f_t*C_{t-1} + {i_t} * \\widetilde{C_{t}} \\tag4$$\n4式中旧状态与$f_t$相乘，丢弃确定需要丢弃的信息，加上新的候选值。可以看到假如遗忘门一直为1，就可以保持以前的信息$C_{t-1}$\n1.3 输出结果 # $$o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o)\\tag5$$ $$h_t = o_t*tanh(C_t)\\tag6$$\n2. LSTM实例以及Pytorch实现 # 循环神经网络可以应用到以下场景。\n点对点（单个图片（文字）被分类；图像分类） 点对序列（单个图像（文字）被分为多个类；图像输出文字） 序列分析（一系列图片（文字）被分类；情感分析） 不等长序列对序列（机器翻译） 等长序列对序列（视频帧分类） 举两个例子：图像分类以及时间序列预测\n2.1 LSTM图像分类 # 关于图片分类常用卷积神经网络，侧重空间上处理；而循环神经网络侧重序列处理。但是也能用来图片分类。第一个例子以常用的mnist手写字体识别为例。\n2.1.1 导入所需用到的包以及超参数设置等 # # Setup import torch from torch import nn from torch.utils.data import DataLoader import torchvision.datasets as dsets import torchvision.transforms as transforms torch.manual_seed(1) # Device configuration device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) 2.1.2 导入数据集 # # Mnist手写数字 train_data = dsets.MNIST(root=\u0026#39;./mnist/\u0026#39;, # 保存或者提取位置 train=True, # this is tra`ining data transform=transforms.ToTensor(), # 转换 PIL.Image or numpy.ndarray 成 # torch.FloatTensor (C x H x W), 训练的时候 normalize 成 [0.0, 1.0] 区间 download=True, # 没下载就下载, 下载了就不用再下了改成False ) test_data = dsets.MNIST(root=\u0026#39;./mnist/\u0026#39;, train=False, transform=transforms.ToTensor()) # Dataloader # PyTorch中数据读取的一个重要接口，该接口定义在dataloader.py中，只要是用PyTorch来训练模型基本都会用到该接口（除非用户重写…）， # 该接口的目的：将自定义的Dataset根据batch size大小、是否shuffle等封装成一个Batch Size大小的Tensor，用于后面的训练。 train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) # 在每个epoch开始的时候，对数据重新打乱进行训练。在这里其实没啥用，因为只训练了一次 test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False) 2.1.3 建立模型 # # LSTM # __init__ is basically a function which will \u0026#34;initialize\u0026#34;/\u0026#34;activate\u0026#34; the properties of the class for a specific object # self represents that object which will inherit those properties class simpleLSTM(nn.Module): def __init__(self, input_size, hidden_size, num_layers, num_classes): super(simpleLSTM, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) self.fc = nn.Linear(hidden_size, num_classes) def forward(self, x): # x shape (batch, time_step, input_size) # out shape (batch, time_step, output_size) # h_n shape (n_layers, batch, hidden_size) # h_c shape (n_layers, batch, hidden_size) # 初始化hidden和memory cell参数 h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # forward propagate lstm out, (h_n, h_c) = self.lstm(x, (h0, c0)) # 选取最后一个时刻的输出 out = self.fc(out[:, -1, :]) return out model = simpleLSTM(input_size, hidden_size, num_layers, num_classes) # loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr) 2.1.4 训练模型 # # train the model # 关于reshape(-1)的解释 https://www.zhihu.com/question/52684594 # view()和reshape()区别的解释 https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch # Hyper Parameters epochs = 1 # 训练整批数据多少次, 为了节约时间, 我们只训练一次 batch_size = 64 time_step = 28 # rnn 时间步数 / 图片高度 input_size = 28 # rnn 每步输入值 / 图片每行像素 hidden_size = 64 num_layers = 1 num_classes = 10 lr = 0.01 # learning rate total_step = len(train_loader) for epoch in range(epochs): for i, (images, labels) in enumerate(train_loader): images = images.reshape(-1, time_step, input_size).to(device) labels = labels.to(device) # forward pass outputs = model(images) loss = criterion(outputs, labels) # backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if i % 100 == 0: print(\u0026#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\u0026#39; .format(epoch+1, epochs, i+1, total_step, loss.item())) 2.1.5 测试模型 # # Test the model # https://stackoverflow.com/questions/55627780/evaluating-pytorch-models-with-torch-no-grad-vs-model-eval # torch.max()用法。https://blog.csdn.net/weixin_43255962/article/details/84402586 model.eval() with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, time_step, input_size).to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(\u0026#39;Test Accuracy of the model on the 10000 test images: {} %\u0026#39;.format(100 * correct / total)) 2.2 时间序列预测 # Todo\n2.3 图像输出文字 # Todo\n补充 # 在原始发表文献用的图示是类似于下图的这种，看起来比较好容易理解当初形成LSTM的原因 pytorch lstm函数用法示例\nrnn = nn.LSTM(10, 20, 2) # input_size, hidden_size, num_layers input = torch.randn(5, 3, 10) # time_step, batch, input_size（这里input_size即features） h0 = torch.randn(2, 3, 20) # num_layers, batch, hidden_size c0 = torch.randn(2, 3, 20) # num_layers, batch, hidden_size output, (hn, cn) = rnn(input, (h0, c0)) # output包含从最后一层lstm中输出的ht。shape: time_step, batch, hidden_size hidden_size is the number of units of your LSTM cell. This means all the layers (input, forget, etc.) will have this size\nhidden_size即pytorch隐含层每个结构中含有的隐含cell数目\nlstm函数中加入bidirectional=True参数即双向神经网络 Reference # 理解LSTM(http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 高效RNN(http://karpathy.github.io/2015/05/21/rnn-effectiveness/) Hochreiter \u0026amp; Schmidhuber (1997) LSTM Pytorch LSTM官方文档(https://pytorch.org/docs/stable/nn.html#lstm) ","date":"9 March 2020","permalink":"/posts/20200307-lstm%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","section":"Posts","summary":"","title":"LSTM应用场景以及pytorch实例"},{"content":" 1. History, motivation and evolution of Deep Learning # 科学技术发展如海浪一样也会潮起潮落，深度学习在经历了几次低谷后。2010年左右，在语音识别领域取得进展，2012年在计算机视觉领域也发展起来，随后各个领域都开始使用应用深度学习方法，而似乎渐渐抛弃了其他方法，那么深度学习是不是问题的最终解决之道呢？研究方向宽泛而多维才是合理的道路，不应过分追求热点领域。正如上世纪80年代日本学者在低谷时期仍然坚持自己的研究领域。\n学习表征：如何学习好的表征是深度学习要解决的问题之一，原始数据以一种有用的形式返回。自然状态下数据相互依赖有关系的。高效的表达方式应该是每类数据都是完全独立能完全单独表达某个方面。\nspace tiling random projections polynomial classifier radial basis functions kernel machines 2. Gradient Descent and Backpropagation # 2.1 Gradient Descent # $$J(w, b) = \\frac{1}{m}\\sum_1^mL(\\hat{y}^{(i)}, y^{i})$$ J(w, b)为问题的cost function即目标函数，即m个样本的损失函数平均值。使目标函数最小得到此时w,b参数是我们的优化问题。\n2.1.1 梯度下降(batch gradient descent) # 梯度下降即上式对所有样本计算求出目标函数，通过对w,b求梯度来找到目标函数最小值，常用的一个比喻即找最快路径下山。数学理解是算法实现的重要一步，但与在计算机上实现还是有区别的，那么实际做法是什么样的呢？\n当你对复杂的问题想不清楚时，我们都可以从一个简单的例子出发来简化问题，对于这个问题考虑只有一个样本时，我们怎么编程实现呢？对w1、 b1，计算一个样本的loss然后对w1、b1求导优化思路很清晰，那么有m个样本的时候呢？只需将其他样本计算loss，然后对w1、b1求导相加。最后在通过学习率来更新w、b。可以看到每次更新都需要进行m次运算\n2.1.2 小样本梯度下降（mini-batch gradient descent） # 在每次更新时用n个样本，不用全部的样本。在深度学习中常用这种方法。用mini-batch可以享受向量化带来的便利，也不用全梯度下降那么大计算量，同时这也是应对冗余数据的一种方法。\n2.1.3 随机梯度下降(stochastic gradient descent) # 当n = 1的时候，每次更新的时候用1个样本。该方法在大多数情况下比全样本的梯度下降要快。\n三种优化方法最后收敛吗？最后能达到全局最小值吗？这是优化方法都需要考虑到的。可以阅读Optimization Methods for Large-Scale Machine Learning，我自己还没读过\u0026hellip;\n2.2 Backprop # 反向传播是为了求梯度用到的微积分链式法则，从而使梯度下降算法运行。\n2.3 PyTorch训练神经网络步骤 # output = model(input) 即神经网络前向传播 J = loss(output, label) 计算cost function model.zero_grad() 清除梯度计算 J.backward() 对requires_grad = True的变量计算梯度 optimiser.step() 进行梯度下降 3. 总结 # 看了前两节，觉得还是吴恩达大佬讲的好一些。建议网页上快速过内容即可，视频不用细看。\n","date":"5 March 2020","permalink":"/posts/20200305-pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","section":"Posts","summary":"","title":"PyTorch深度学习（1）"},{"content":"","date":"4 March 2019","permalink":"/tags/git/","section":"Tags","summary":"","title":"git"},{"content":"以下操作基于macOS，Windows仅供参考。\ngit初始化文件夹 # 进入目录\ngit init 新建.gitignore # 然后在其中加入需要忽略的文件或文件夹.gitignore 例如public\\\ngit删除.DS_Store文件 # 从该仓库中删除已存在的DS_Store文件1 find . -name .DS_Store -print0 | xargs -0 git rm -f --ignore-unmatch 新建.gitignore_global文件并将.DS_Store以及*/.DS_Store加入其中 vi .gitignore_global # 写入.DS_Store，*/.DS_Store git config --global core.excludesfile ~/.gitignore_global 推到仓库 git add .gitignore git commit -m \u0026#39;.DS_Store banished!\u0026#39; 检查仓库中是否还有 git status git删除远程分支文件 # 当我们需要删除暂存区或分支上的文件, 同时工作区也不需要这个文件了, 可以使用\ngit rm file_path 当我们需要删除暂存区或分支上的文件, 但本地又需要使用, 只是不希望这个文件被版本控制, 可以使用\ngit rm –cached file_path 所以我们经常使用以下命令来删除git中的文件\ngit rm -r --cached filename git commit -m \u0026#39;delete some file\u0026#39; git push origin master git冲突处理 # git远程分支修改，本地也修改了准备提交出现冲突\n先拉在推 git pull --rebase #检查合并是否冲突 git push -u origin master 强制按本地更新 git push -f git子模块（submodule） # 对于公共资源或者常用的代码，你可能会把最新版本逐个复制到N个项目中，如果使用了submodule模块，那么只需要在各个项目中\ngit submodule update 进入子模块目录正常操作即可\ngit多账户切换 # 删除keychain access种存储账户\nGit删除.DS_Store\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"4 March 2019","permalink":"/posts/20190304-git/","section":"Posts","summary":"","title":"Git常用命令"},{"content":"","date":"4 March 2019","permalink":"/categories/syntax/","section":"Categories","summary":"","title":"Syntax"}]