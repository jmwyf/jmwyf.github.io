<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><meta http-equiv=x-ua-compatible content="ie=edge"><title>医学人工智能周刊6｜模态无关的学习方法在医学影像以及生理信号中的评测 &#183; 从百草园到三味书屋</title><meta name=title content="医学人工智能周刊6｜模态无关的学习方法在医学影像以及生理信号中的评测 &#183; 从百草园到三味书屋"><meta name=description content><link rel=canonical href=https://youngforever.tech/ai4h/20230711-ai4h@6/><link type=text/css rel=stylesheet href=/css/main.bundle.min.1b07026ad335b957c216b421a55e1733ac5df10d02ae749f769a48518d7c9394.css integrity="sha256-GwcCatM1uVfCFrQhpV4XM6xd8Q0CrnSfdppIUY18k5Q="><script type=text/javascript src=/js/appearance.min.022d0ebc3b46a335eb1c7ef79b7f2de143d7cd5156d433638592ef1ce5f8554e.js integrity="sha256-Ai0OvDtGozXrHH73m38t4UPXzVFW1DNjhZLvHOX4VU4="></script>
<script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.75206d23ef83f4908b2bdd2317bf6ddff399e9173a16fff5451c40b8e857cfa8.js integrity="sha256-dSBtI++D9JCLK90jF79t3/OZ6Rc6Fv/1RRxAuOhXz6g=" data-copy=Copy data-copied=Copied></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="医学人工智能周刊6｜模态无关的学习方法在医学影像以及生理信号中的评测"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:url" content="https://youngforever.tech/ai4h/20230711-ai4h@6/"><meta property="article:section" content="AI4H"><meta property="article:published_time" content="2023-07-10T20:00:00+00:00"><meta property="article:modified_time" content="2023-07-10T20:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="医学人工智能周刊6｜模态无关的学习方法在医学影像以及生理信号中的评测"><meta name=twitter:description content><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"医学人工智能周刊","name":"医学人工智能周刊6｜模态无关的学习方法在医学影像以及生理信号中的评测","headline":"医学人工智能周刊6｜模态无关的学习方法在医学影像以及生理信号中的评测","inLanguage":"en","url":"https:\/\/youngforever.tech\/ai4h\/20230711-ai4h@6\/","author":{"@type":"Person","name":"Yong"},"copyrightYear":"2023","dateCreated":"2023-07-10T20:00:00\u002b00:00","datePublished":"2023-07-10T20:00:00\u002b00:00","dateModified":"2023-07-10T20:00:00\u002b00:00","keywords":["artificial intelligence","healthcare"],"mainEntityOfPage":"true","wordCount":"2577"}]</script><meta name=author content="Yong"><link href=https://github.com/yongfanbeta rel=me><link href=https://www.zhihu.com/people/Havefan rel=me><link href=mailto:yongfan2020@outlook.com rel=me><script async src="https://www.googletagmanager.com/gtag/js?id=G-F4R8ND3VQP"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-F4R8ND3VQP",{anonymize_ip:!1})}</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold pe-2 text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral print:hidden sm:py-10"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>从百草园到三味书屋</a></div><ul class="flex list-none flex-col ltr:text-right rtl:text-left sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">自画像</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/ai4h/ title=医学人工智能周刊><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">知无涯</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">朝花夕拾</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/people/ title=Friends><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">远方朋</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/tags/ title=Tags><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">标签</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="relative inline-block align-text-bottom px-1 icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">医学人工智能周刊6｜模态无关的学习方法在医学影像以及生理信号中的评测</h1><div class="mt-8 mb-12 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2023-07-10 20:00:00 +0000 UTC">10 July 2023</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">6 mins</span></div><div class="my-1 text-xs leading-relaxed text-neutral-500 dark:text-neutral-400"><a href=/categories/ai4h/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">AI4H</a>
<a href=/tags/artificial-intelligence/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">artificial intelligence</a>
<a href=/tags/healthcare/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">healthcare</a></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="-ms-5 block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="-ms-5 border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#摘要>摘要</a></li><li><a href=#引言>引言</a></li><li><a href=#相关工作>相关工作</a></li><li><a href=#模态和数据集>模态和数据集</a><ul><li><a href=#12-lead-ecgs>12-lead ECGs</a></li><li><a href=#eeg>EEG</a></li><li><a href=#chest-x-rays>Chest X-Rays</a></li><li><a href=#mammograms>Mammograms</a></li><li><a href=#dermoscopic>Dermoscopic</a></li><li><a href=#fundus-images>Fundus Images</a></li><li><a href=#low-dose-computer-tomography-scansldct>Low Dose Computer Tomography Scans（LDCT）</a></li></ul></li><li><a href=#实验>实验</a><ul><li><a href=#网络架构>网络架构</a></li><li><a href=#预训练>预训练</a></li><li><a href=#迁移学习以及分布外评价>迁移学习以及分布外评价</a></li></ul></li><li><a href=#结果>结果</a></li><li><a href=#思考>思考</a></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-prose grow"><h2 id=摘要 class="relative group">摘要 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%91%98%e8%a6%81 aria-label=Anchor>#</a></span></h2><p>目的：建立基准测试BenchMD，用于测试模型无关的方法包括<strong>架构</strong>和<strong>训练技术</strong>（例如自监督学习、预训练）在临床相关的医疗任务上的表现。简言之，就是测试最新一些通用人工智能方法在医疗任务上的表现。</p><p>BenchMD包括19个公开数据集，7种医疗数据模态，1维传感器数据、2维图片、3维扫描数据。</p><p>结果表明，没有一种与模态无关的技术在所有模态上都能实现强大的性能，基准模型有充足的改进空间。</p><h2 id=引言 class="relative group">引言 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%bc%95%e8%a8%80 aria-label=Anchor>#</a></span></h2><p>背景：Transformers模型和自监督学习（SSL）对标签数据需求小、能灵活运用到多种模态的数据。</p><p>问题：衡量这些进展在领域内的效果需要制定具有广度和深度的评测，以捕捉应用和模式的多样性，并通过让专家参与评测过程来确保外部有效性。</p><p>当前医疗AI领域应用时针对具体问题，通过试验选择不同的架构以及自监督学习方法，期望发展一种灵活、与模态无关无需定制化就能应用到各类问题的方法。</p><p>解决：BenchMD针对每种模态构建标准化、临床有效的评估方法，并通过专家验证；同时探索了基准<strong>数据标签不足</strong>情况和<strong>数据偏移</strong>情况下的表现；</p><p>同时，为了让BenchMD更加容易使用：</p><ul><li>易用性：新架构和任务即插即用</li><li>易复现：全部使用公开数据集</li></ul><p>结果显示医疗AI领域通用、泛化性强方法仍需继续研究。</p><h2 id=相关工作 class="relative group">相关工作 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c aria-label=Anchor>#</a></span></h2><ul><li>模态无关的技术：SSL<ul><li>掩码建模</li><li>对比学习</li></ul></li><li>模态无关的医学人工智能<ul><li>在医学图像上自监督MAE比ImageNet上预训练要好</li><li>医学影像有无监督预训练加上监督学习表现好</li></ul></li><li>现有多种模态的基准测试<ul><li><a href=https://github.com/alextamkin/dabs target=_blank rel="noreferrer noopener">GitHub - alextamkin/dabs: A Domain-Agnostic Benchmark for Self-Supervised Learning</a></li><li><a href=https://github.com/p-lambda/wilds target=_blank rel="noreferrer noopener">GitHub - p-lambda/wilds: A machine learning benchmark of in-the-wild distribution shifts, with data loaders, evaluators, and default models.</a></li></ul></li></ul><h2 id=模态和数据集 class="relative group">模态和数据集 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%a8%a1%e6%80%81%e5%92%8c%e6%95%b0%e6%8d%ae%e9%9b%86 aria-label=Anchor>#</a></span></h2><p>整理了一系列高影响模态数据以及精心挑选的数据源和目标数据集，用于评估分布外 （OOD） 性能。</p><h3 id=12-lead-ecgs class="relative group">12-lead ECGs <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#12-lead-ecgs aria-label=Anchor>#</a></span></h3><p>利用5秒采样频率为500Hz的12导心电数据进行7分类：正常、传导障碍、心肌肥厚、心肌梗死、缺血性ST-T改变、心房颤动/心房扑动及其他。</p><p>数据包括：</p><ul><li>PTB-XL (18k) 1989-1996<ul><li><a href=https://physionet.org/content/ptb-xl/1.0.3/ target=_blank rel="noreferrer noopener">PTB-XL, a large publicly available electrocardiography dataset v1.0.3</a></li></ul></li><li>Chapman-Shaoxing (10k) 2020<ul><li><a href=https://figshare.com/collections/ChapmanECG/4560497/2 target=_blank rel="noreferrer noopener">A 12-lead electrocardiogram database for arrhythmia research covering more than 10,000 patients</a></li></ul></li><li>Georgia 12-Lead ECG Challenge (10k) 2020<ul><li><a href=https://physionet.org/content/challenge-2020/1.0.2/#files target=_blank rel="noreferrer noopener">Classification of 12-lead ECGs: The PhysioNet/Computing in Cardiology Challenge 2020 v1.0.2</a></li></ul></li><li>China Physiological Signal Challenge (6.8k) 2018</li></ul><h3 id=eeg class="relative group">EEG <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#eeg aria-label=Anchor>#</a></span></h3><p>30秒单导EEG睡眠分期任务。使用AASM睡眠分期标准：觉醒、快速眼动、非快速眼动I期、非快速眼动2期、非快速眼动3期。</p><p>数据包括：</p><ul><li>SHHS (5.8k) 1995-1998<ul><li>includes 5,804 adults aged 40 and older</li><li><a href=https://biolincc.nhlbi.nih.gov/studies/shhs/ target=_blank rel="noreferrer noopener">BioLINCC: Sleep Heart Health Study (SHHS)</a></li></ul></li><li>ISRUC-Sleep（0.1k）2009-2013<ul><li>collected from subjects in hospital whose ages range from 20 years old to 85 years old, with an average age of 51</li><li><a href=https://sleeptight.isr.uc.pt/ target=_blank rel="noreferrer noopener">ISRUC-SLEEP Dataset | A comprehensive public dataset for sleep researchers</a></li></ul></li></ul><h3 id=chest-x-rays class="relative group">Chest X-Rays <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#chest-x-rays aria-label=Anchor>#</a></span></h3><p>使用2D灰度胸片进行单标签分类任务，包括肺不张、心脏扩大、实变、水肿和胸腔积液。</p><p>数据包括：</p><ul><li>MIMIC-CXR (227k) 2011-2016<ul><li>a large publicly available dataset of chest radiographs in DICOM format with free-text radiology reports.</li><li><a href=https://physionet.org/content/mimic-cxr/2.0.0/ target=_blank rel="noreferrer noopener">MIMIC-CXR Database v2.0.0</a></li></ul></li><li>CheXpert (65k) 2002-2017<ul><li>a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients.</li><li>[CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison](<a href=https://stanfordmlgroup.github.io/competitions/chexpert/ target=_blank rel="noreferrer noopener">https://stanfordmlgroup.github.io/competitions/chexpert/</a></li></ul></li><li>VinDr-CXR (18k) 2018-2020<ul><li>The published dataset consists of 18,000 postero-anterior (PA) view CXR scans that come with both the localization of critical findings and the classification of common thoracic diseases.</li><li><a href=https://vindr.ai/datasets/cxr target=_blank rel="noreferrer noopener">VinDr-CXR: An open dataset and benchmarks for disease classification and abnormality localization on chest radiographs | VinDr</a></li></ul></li></ul><h3 id=mammograms class="relative group">Mammograms <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mammograms aria-label=Anchor>#</a></span></h3><p>乳腺X光检查包括患者左右乳房不同视图的2D灰度图像从1-5类BI-RADS分级。</p><p>数据包括：</p><ul><li>VinDr-Mammo (5k) 2018-2020<ul><li>a large-scale benchmark dataset of full-field digital mammography, called VinDr-Mammo</li><li><a href=https://vindr.ai/datasets/mammo target=_blank rel="noreferrer noopener">VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography | VinDr</a></li></ul></li><li>CBIS-DDSM (2.6k)1988-1999<ul><li>The DDSM is a database of 2,620 scanned film mammography studies. It contains normal, benign, and malignant cases with verified pathology information.</li><li><a href="https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=22516629" target=_blank rel="noreferrer noopener">Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) - The Cancer Imaging Archive (TCIA) Public Access - Cancer Imaging Archive Wiki</a></li></ul></li></ul><h3 id=dermoscopic class="relative group">Dermoscopic <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dermoscopic aria-label=Anchor>#</a></span></h3><p>基于2D RGB皮肤图像进行单标签分类，共5类：AKIEC“（包括光化性角化病、上皮内癌和鳞状细胞癌，因为所有这些都是鳞状细胞癌的连续体）、”BCC“（基底细胞癌）、”MEL“（黑色素瘤）、”NEV“（痣）和”其他疾病“（皮肤纤维瘤等）。</p><p>数据包括：</p><ul><li>BCN20000（19k）2010-2016<ul><li>paper: <a href=https://arxiv.org/pdf/1908.02288.pdf target=_blank rel="noreferrer noopener">BCN20000: DERMOSCOPIC LESIONS IN THE WILD</a></li><li>data: <a href=https://challenge.isic-archive.com/data/#2019 target=_blank rel="noreferrer noopener">ISIC Challenge</a></li></ul></li><li>HAM10000（10k）<ul><li>paper: <a href=https://www.nature.com/articles/sdata2018161 target=_blank rel="noreferrer noopener">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions | Scientific Data</a></li><li>data: <a href=https://isic-archive.com/ target=_blank rel="noreferrer noopener">ISIC | International Skin Imaging Collaboration</a></li></ul></li><li>PAD-UFES-20（1.37k）2018-2019<ul><li>a nonprofit program that provides free skin lesion treatment, in particular, to low-income people who cannot afford private treatment.</li><li>paper: <a href=https://arxiv.org/abs/2007.00478 target=_blank rel="noreferrer noopener">[2007.00478] PAD-UFES-20: a skin lesion dataset composed of patient data and clinical images collected from smartphones</a></li><li>data: <a href=https://data.mendeley.com/datasets/zr7vgbcyr2/1 target=_blank rel="noreferrer noopener">PAD-UFES-20: a skin lesion dataset composed of patient data and clinical images collected from smartphones - Mendeley Data</a></li></ul></li></ul><h3 id=fundus-images class="relative group">Fundus Images <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fundus-images aria-label=Anchor>#</a></span></h3><p>基于2D RGB眼底图像预测糖尿病视网膜病变严重程度，基于ICDR分级共5类</p><p>数据包括：</p><ul><li>Messidor-2（0.5k）2004-2010<ul><li>a collection of Diabetic Retinopathy (DR) examinations, each consisting of two macula-centered eye fundus images (one per eye)</li><li><a href=https://www.adcis.net/en/third-party/messidor2/ target=_blank rel="noreferrer noopener">Messidor-2 - ADCIS</a></li></ul></li><li>APTOS 2019（3.6k）2019<ul><li>3662 samples collected from many participants of rural India</li><li><a href=https://www.kaggle.com/competitions/aptos2019-blindness-detection/data target=_blank rel="noreferrer noopener">APTOS 2019 Blindness Detection | Kaggle</a></li></ul></li><li>Jinchi Medical University dataset（2.7k）2011-2015<ul><li>paper：<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5480986/#:~:text=Here%2C%20we%20show%20an%20AI%20that%20grades%20diabetic,staging%20and%20can%20suggest%20treatments%20and%20predict%20prognoses." target=_blank rel="noreferrer noopener">Applying artificial intelligence to disease staging: Deep learning for improved staging of diabetic retinopathy - PMC</a></li><li>data：<a href=https://figshare.com/articles/figure/Davis_Grading_of_One_and_Concatenated_Figures/4879853/1 target=_blank rel="noreferrer noopener">Davis Grading of One and Concatenated Figures</a></li></ul></li></ul><h3 id=low-dose-computer-tomography-scansldct class="relative group">Low Dose Computer Tomography Scans（LDCT） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#low-dose-computer-tomography-scansldct aria-label=Anchor>#</a></span></h3><p>基于3D CT影像进行判断结节大小</p><p>数据包括：</p><ul><li>LIDC-IDRI（1.0k）2010<ul><li><a href="https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=1966254" target=_blank rel="noreferrer noopener">Data from The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans (LIDC-IDRI) - The Cancer Imaging Archive (TCIA) Public Access - Cancer Imaging Archive Wiki</a></li></ul></li><li>LNDb（294）2016-2018<ul><li><a href=https://zenodo.org/record/7153205 target=_blank rel="noreferrer noopener">LNDb Dataset | Zenodo</a></li></ul></li></ul><h2 id=实验 class="relative group">实验 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%ae%9e%e9%aa%8c aria-label=Anchor>#</a></span></h2><p>对5种技术进行评估：3种SSL算法、ImageNet预训练、从头训练，然后使用多种迁移学习方法测试在分布外（OOD）数据中性能。</p><h3 id=网络架构 class="relative group">网络架构 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e7%bd%91%e7%bb%9c%e6%9e%b6%e6%9e%84 aria-label=Anchor>#</a></span></h3><p>分别使用1D、2D和3D的embedding模块处理原始数据形成256维度的嵌入空间，不同输入维度的信息能混合。编码器使用的是标准的ViT架构。</p><h3 id=预训练 class="relative group">预训练 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e9%a2%84%e8%ae%ad%e7%bb%83 aria-label=Anchor>#</a></span></h3><p>三种自监督方法</p><ul><li>Contrastive embedding-Mixup(e-Mix)<ul><li>使用一定的比例系数对原始输入嵌入加权并相加，训练编码器为混合输入产生一个向量，该向量与原始输入经过混合因子加权相加尽量相近</li></ul></li><li>Shuffled embedding prediction(ShED)<ul><li>打乱一部分输入嵌入，使用带分类器的编码器来预测被扰动过的嵌入</li></ul></li><li>MAE<ul><li>对输入嵌入表达进行75%的掩码，训练模型重建输入对嵌入表达</li></ul></li></ul><h3 id=迁移学习以及分布外评价 class="relative group">迁移学习以及分布外评价 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0%e4%bb%a5%e5%8f%8a%e5%88%86%e5%b8%83%e5%a4%96%e8%af%84%e4%bb%b7 aria-label=Anchor>#</a></span></h3><p>固定模型骨架利用分布内数据训练一个线性分类器进行微调。然后在分布外数据集中进行zero-shot评估。微调数据集的选取，单标签任务选取，多标签任务选取。</p><p><figure><img class="mx-auto my-0 rounded-md" src=https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/benchmdpng alt loading=lazy></figure></p><h2 id=结果 class="relative group">结果 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e7%bb%93%e6%9e%9c aria-label=Anchor>#</a></span></h2><ul><li>各类自监督方法在各模态数据上表现不一致，需要探索在各模态数据中表现更加一致的SSL算法</li><li>SSL有时优于预训练方法，有时预训练也可以和SSL表现相当。<strong>未来需要探索在其他数据集例如imagenet中进行预训练，然后在医疗数据集中进行SSL</strong>，即预训练与自监督结合。</li><li>微调过程中标化数据量影响模型性能，越多越好，但也要防止过拟合的情况发生。</li><li>分布内与分布外数据模型性能比较表明需要探索提升模型可泛化性能的正则化技术</li></ul><h2 id=思考 class="relative group">思考 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%80%9d%e8%80%83 aria-label=Anchor>#</a></span></h2><p>自监督技术和预训练技术在NLP和CV领域应用广泛，如何应用到各类医疗数据中，是不是在所有种类的医疗数据中表现都比不使用要好，本文通过构建各类基准模型尝试回答该问题，相比于NLP和CV领域，医疗领域数据种类繁多导致当前没有一种统一的方法适用于所有的情形，需要进一步研究判断对各种类的数据适合使用的方法，以及预训练与自监督技术联合使用的方法。</p><blockquote><p>Wantlin, K. et al. BenchMD: A Benchmark for Modality-Agnostic Learning on Medical Images and Sensors. Preprint at <a href=https://doi.org/10.48550/arXiv.2304.08486 target=_blank rel="noreferrer noopener">https://doi.org/10.48550/arXiv.2304.08486</a> (2023).</p></blockquote></div></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/ai4h/20230624-ai4h@5/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">医学人工智能周刊5｜提高医疗领域AI算法研究透明性清单</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-06-23 20:00:00 +0000 UTC">23 June 2023</time></span></span></a></span>
<span></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><div id=cusdis_thread data-host=https://cusdis.com data-app-id=1cfc2662-243a-4573-b9e7-8b2b4a986fba data-page-id=eb62c0f935a46caa3f97cba624231be1 data-page-url=https://youngforever.tech/ai4h/20230711-ai4h@6/ data-page-title=医学人工智能周刊6｜模态无关的学习方法在医学影像以及生理信号中的评测></div><script src=https://cusdis.com/js/cusdis.es.js></script></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2023
Yong</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://git.io/hugo-congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex h-12 w-12 items-center justify-center dark:hidden" title="Switch to dark appearance"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="hidden h-12 w-12 items-center justify-center dark:flex" title="Switch to light appearance"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://youngforever.tech/><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative inline-block align-text-bottom px-1 icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>