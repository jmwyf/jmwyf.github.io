<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><meta http-equiv=x-ua-compatible content="ie=edge"><title>ClinicalBERT: 对医学文本建模用于再入院预测 &#183; 从百草园到三味书屋</title><meta name=title content="ClinicalBERT: 对医学文本建模用于再入院预测 &#183; 从百草园到三味书屋"><meta name=description content="ClinicalBERT Modeling Clinical Notes and Predicting Hospital Readmission论文解读"><link rel=canonical href=https://jmwyf.github.io/posts/20230302-clinicalbert/><link type=text/css rel=stylesheet href=/css/main.bundle.min.07a3bf6cbbdad29800c6e96484e7844e32bec72bb48a8b402459f22a0d5582851a267dc2e6725df43594a8b661eb5f2ee57048128d54a78d6284d062843a37c3.css integrity="sha512-B6O/bLva0pgAxulkhOeETjK+xyu0iotAJFnyKg1VgoUaJn3C5nJd9DWUqLZh618u5XBIEo1Up41ihNBihDo3ww=="><script type=text/javascript src=/js/appearance.min.c6198a5ecbc6c7b35a8568d29315f50cf5d775d4461c515a476359767f1a745c245aa83fa96b0c7d83da976cf77481e1fc29537a2391a53ffe69a991638802e6.js integrity="sha512-xhmKXsvGx7NahWjSkxX1DPXXddRGHFFaR2NZdn8adFwkWqg/qWsMfYPal2z3dIHh/ClTeiORpT/+aamRY4gC5g=="></script>
<script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.6d78c827ca7bcbf72056dbf698bf9aeb759a08966686187deb23f5949f73eca5f39b461284900cdfc08e2976d99eb80a8663648de778ba2a83e633ae16dbfc25.js integrity="sha512-bXjIJ8p7y/cgVtv2mL+a63WaCJZmhhh96yP1lJ9z7KXzm0YShJAM38COKXbZnrgKhmNkjed4uiqD5jOuFtv8JQ==" data-copy=Copy data-copied=Copied></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="ClinicalBERT: 对医学文本建模用于再入院预测"><meta property="og:description" content="ClinicalBERT Modeling Clinical Notes and Predicting Hospital Readmission论文解读"><meta property="og:type" content="article"><meta property="og:url" content="https://jmwyf.github.io/posts/20230302-clinicalbert/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-21T23:13:00+00:00"><meta property="article:modified_time" content="2023-02-21T23:13:00+00:00"><meta property="og:site_name" content="从百草园到三味书屋"><meta name=twitter:card content="summary"><meta name=twitter:title content="ClinicalBERT: 对医学文本建模用于再入院预测"><meta name=twitter:description content="ClinicalBERT Modeling Clinical Notes and Predicting Hospital Readmission论文解读"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"ClinicalBERT: 对医学文本建模用于再入院预测","headline":"ClinicalBERT: 对医学文本建模用于再入院预测","abstract":"ClinicalBERT Modeling Clinical Notes and Predicting Hospital Readmission论文解读","inLanguage":"en","url":"https:\/\/jmwyf.github.io\/posts\/20230302-clinicalbert\/","author":{"@type":"Person","name":"Yong"},"copyrightYear":"2023","dateCreated":"2023-02-21T23:13:00\u002b00:00","datePublished":"2023-02-21T23:13:00\u002b00:00","dateModified":"2023-02-21T23:13:00\u002b00:00","keywords":["nlp","bert","mimic-iii"],"mainEntityOfPage":"true","wordCount":"1706"}]</script><meta name=author content="Yong"><link href=mailto:yongfan2020@outlook.com rel=me><link type=text/css rel=stylesheet href=/lib/katex/katex.min.990c289bc36ce28a7e1f6f680e40ff2d73bf3ac5cfbc215f92066414056b86178fcd12d4f0d508dcd926e373615944a68f7a7909c6b61d3124310997411c0341.css integrity="sha512-mQwom8Ns4op+H29oDkD/LXO/OsXPvCFfkgZkFAVrhhePzRLU8NUI3Nkm43NhWUSmj3p5Cca2HTEkMQmXQRwDQQ=="><script defer src=/lib/katex/katex.min.b0748d2c40912522be045b3b13c0e215cea97fa9fdd1b2c8f391268b0d6386b79db9984add55e0abd978900b7d79320760072e4012872f4d502ace3fdd7825f2.js integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g=="></script>
<script defer src=/lib/katex/auto-render.min.8968ae052e67b7aafad1f0b3dba35dd19a9ed276e4d594c841b9772afee462c5fec8a314147ce3687dbe02733abe9d97b3e80d99a0405562634a6b8fc3be847e.js integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-F4R8ND3VQP"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-F4R8ND3VQP",{anonymize_ip:!1})}</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral print:hidden sm:py-10"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>从百草园到三味书屋</a></div><ul class="flex flex-col list-none ltr:text-right rtl:text-left sm:flex-row"><li class="mb-1 group sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">自画像</span></a></li><li class="mb-1 group sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">朝花夕拾</span></a></li><li class="mb-1 group sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a href=/tags/ title=Tags><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">签</span></a></li><li class="mb-1 group sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><button id=search-button-1 title="Search (/)">
<span class="transition-colors group-dark:hover:text-primary-400 group-hover:text-primary-600"><span class="relative inline-block align-text-bottom icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">ClinicalBERT: 对医学文本建模用于再入院预测</h1><div class="mt-8 mb-12 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2023-02-21 23:13:00 +0000 UTC">21 February 2023</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">4 mins</span></div><div class="my-1 text-xs leading-relaxed text-neutral-500 dark:text-neutral-400"><a href=/categories/deep-learning/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Deep Learning</a>
<a href=/tags/nlp/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">nlp</a>
<a href=/tags/bert/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">bert</a>
<a href=/tags/mimic-iii/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">mimic-iii</a></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#引言>引言</a><ul><li><a href=#背景>背景</a></li><li><a href=#该工作的重要性>该工作的重要性：</a></li></ul></li><li><a href=#方法>方法</a><ul><li><a href=#什么是bert>什么是BERT</a></li><li><a href=#临床文本嵌入>临床文本嵌入</a></li><li><a href=#自注意力机制>自注意力机制</a></li><li><a href=#预训练>预训练</a></li><li><a href=#微调>微调</a></li></ul></li><li><a href=#实验>实验</a><ul><li><a href=#数据>数据</a></li><li><a href=#实证研究i>实证研究I</a></li><li><a href=#实证研究ii>实证研究II</a></li></ul></li><li><a href=#讨论>讨论</a></li><li><a href=#代码>代码</a></li><li><a href=#思考>思考</a></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-prose grow"><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><p>使用临床文本预训练BERT然后在再入院任务中微调</p><h2 id=引言 class="relative group">引言 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%bc%95%e8%a8%80 aria-label=Anchor>#</a></span></h2><p>非结构化、高维稀疏信息例如临床文本难以在临床机器学习模型中使用。临床文本中包含什么样的临床价值？更加丰富、详细。然而重症监护室医生在有限时间内需要做出最优决策，读大量的临床文本，增加工作量。</p><p>再入院会降低患者生活质量、增加花费。这篇文章旨在发展一个出院决策模型，根据医护人员笔记动态的赋予患者30天再入院的风险。</p><h3 id=背景 class="relative group">背景 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e8%83%8c%e6%99%af aria-label=Anchor>#</a></span></h3><p>临床文本会有缩写、黑话、不标准的语法结构，从临床文本中学习有用的表征具有挑战。以往的方法无法捕捉获取临床意义的文本长程依赖，介绍BERT，以及用BERT已经开展的工作，已经有人把BERT用在临床文本了，本文在再入院任务上评估改进ClinicalBERT并且在更长的序列上进行预训练。</p><p>介绍前人在ICU再入院预测上的工作，缺点：大多数工作都只用了出院的信息，ClinicalBERT使用患者住院整个时间段信息。</p><h3 id=该工作的重要性 class="relative group">该工作的重要性： <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e8%af%a5%e5%b7%a5%e4%bd%9c%e7%9a%84%e9%87%8d%e8%a6%81%e6%80%a7 aria-label=Anchor>#</a></span></h3><ul><li>用出院信息来预测意味着减少了再入院风险的机会少了，都要出院了，此刻告诉有再入院的风险，难以采取措施</li><li>由于很多误报警，医疗模型需要高PPV<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，该模型最高的recall<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></li><li>模型中attention能用于可视化解释</li></ul><h2 id=方法 class="relative group">方法 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%96%b9%e6%b3%95 aria-label=Anchor>#</a></span></h2><h3 id=什么是bert class="relative group">什么是BERT <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e4%bb%80%e4%b9%88%e6%98%afbert aria-label=Anchor>#</a></span></h3><p>BERT是基于transformer编码器架构的深度神经网络，它用于学习文本的嵌入表达。</p><ul><li>自注意力机制</li><li>BERT模型通过2个无监督任务进行预训练：掩码模型和下一个句子预测。</li></ul><h3 id=临床文本嵌入 class="relative group">临床文本嵌入 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e4%b8%b4%e5%ba%8a%e6%96%87%e6%9c%ac%e5%b5%8c%e5%85%a5 aria-label=Anchor>#</a></span></h3><ul><li>先分词成token<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>，这里是子词粒度的tokenization<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></li><li>ClinicalBert的token包括子词、分段嵌入、位置嵌入<ul><li>分段嵌入是当多个序列输入时，表示当前的token属于哪一段</li><li>位置嵌入即在输入序列中token的位置</li></ul></li></ul><h3 id=自注意力机制 class="relative group">自注意力机制 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label=Anchor>#</a></span></h3><p>用于输入token之间的关系捕捉</p><h3 id=预训练 class="relative group">预训练 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e9%a2%84%e8%ae%ad%e7%bb%83 aria-label=Anchor>#</a></span></h3><p>BERT是在BooksCorpus和Wikipedia中预训练的，临床文本黑话缩写，与一般文本可能语法也不一样，需要在临床文本中进行预训练。损失函数是预测掩码单词任务和预测两个句子是否连续任务损失函数之和。</p><h3 id=微调 class="relative group">微调 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%be%ae%e8%b0%83 aria-label=Anchor>#</a></span></h3><p>在再入院任务中微调
$$P(readmit = 1 | h_{[cls]}) = \sigma(Wh_{[cls]})$$
式中W为参数，h为BERT模型输出。</p><h2 id=实验 class="relative group">实验 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%ae%9e%e9%aa%8c aria-label=Anchor>#</a></span></h2><h3 id=数据 class="relative group">数据 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%95%b0%e6%8d%ae aria-label=Anchor>#</a></span></h3><p>MIMIC-III中2083180份去隐私化后的文本，五折每一轮其中四折预训练，最后一折微调</p><h3 id=实证研究i class="relative group">实证研究I <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6i aria-label=Anchor>#</a></span></h3><ul><li>在临床语言建模中ClinicalBERT与BERT进行比较：预测掩码token以及2个句子是否连续任务中均优于BERT</li><li>定性分析：专家给出相似医学概念，ClinicalBERT学习嵌入表达后，进行降维可视化，发现相近</li><li>定量分析：采用相似度度量公式计算表征之前相似度，然后与专家打分的相似度进行关联分析计算pearson相关系数</li></ul><h3 id=实证研究ii class="relative group">实证研究II <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6ii aria-label=Anchor>#</a></span></h3><ul><li>再入院队列：34560患者，2963再入院，42358负样本，<em>这里为啥有这么多负样本？</em></li><li>调整后的再入院预测：
$$P(readmit = 1|h_{patient}) = \frac{P^n_{max}+P^n_{mean}n/c}{1+n/c}$$<ul><li>有些文本是比较重要，有些文本对再入院预测不重要，所以要包括最大的概率</li><li>噪声会降低性能，消除噪音的方法还是取大多数值的平均，如果序列越长，噪声出现的可能性越大，所以需要平均值的权重越大，引入了n/c作为比例因子</li><li>分母则是用于概率归一化到0,1区间</li></ul></li><li>评估指标<ul><li>AUROC</li><li>AUPRC</li><li>RP80：准确度为80%时候到召回率</li></ul></li><li>模型比较：Bag of words，BI-LSTM，BERT</li><li>用出院记录来进行再入院预测</li><li>用24-48小时数据预测，以及48-72小时数据预测</li><li>可解释性<ul><li>给出一句话的self-attention权重示意图</li></ul></li></ul><h2 id=讨论 class="relative group">讨论 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e8%ae%a8%e8%ae%ba aria-label=Anchor>#</a></span></h2><p>建议在私有数据集上重新训练后在下游任务中使用</p><h2 id=代码 class="relative group">代码 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e4%bb%a3%e7%a0%81 aria-label=Anchor>#</a></span></h2><p><a href=https://github.com/kexinhuang12345/clinicalBERT target=_blank rel="noreferrer noopener">GitHub - kexinhuang12345/clinicalBERT: ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission (CHIL 2020 Workshop)</a></p><h2 id=思考 class="relative group">思考 <span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%80%9d%e8%80%83 aria-label=Anchor>#</a></span></h2><p>自chatgpt后，大型语言模型受到广泛关注，医学语言模型的发展似乎有多种路径，一种是直接在通用文本上预训练，一种是在医学文本中预训练，或是通用模型在领域微调，个人感觉应该是第三种效果会较好。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Huang, K., Altosaar, J. & Ranganath, R. ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission. in <em>CHIL</em> (arXiv, 2020). doi:<a href=https://doi.org/10.48550/arXiv.1904.05342 target=_blank rel="noreferrer noopener">10.48550/arXiv.1904.05342</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>PPV: 阳性预测里面真正的阳性比例&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>recall: 正样本中实际预测为正，即真阳性率&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>token：将原始文本切分成子单元的过程就叫做Tokenization，子单元即token&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/20230222-embedding/><span class="mr-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-2 text-neutral-700 transition-transform group-hover:translate-x-[2px] group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Embedding是什么？</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-02-21 23:13:00 +0000 UTC">21 February 2023</time></span></span></a></span>
<span></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><div id=cusdis_thread data-host=https://cusdis.com data-app-id=1cfc2662-243a-4573-b9e7-8b2b4a986fba data-page-id=92db271fc7eeb4d55ed9e2e64de8ad0b data-page-url=https://jmwyf.github.io/posts/20230302-clinicalbert/ data-page-title="ClinicalBERT: 对医学文本建模用于再入院预测"></div><script async defer src=https://cusdis.com/js/cusdis.es.js></script></div></div></footer></article><div class="pointer-events-none absolute top-[100vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2023
Yong</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://git.io/hugo-congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="ltr:mr-14 rtl:ml-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex items-center justify-center w-12 h-12 dark:hidden" title="Switch to dark appearance"><span class="relative inline-block align-text-bottom icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden w-12 h-12 dark:flex" title="Switch to light appearance"><span class="relative inline-block align-text-bottom icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://jmwyf.github.io/><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative inline-block align-text-bottom icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative inline-block align-text-bottom icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>