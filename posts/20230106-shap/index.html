<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><meta http-equiv=x-ua-compatible content="ie=edge"><title>《A Unified Approach to interpreting Model Predictions》论文解读 &#183; 从百草园到三味书屋</title><meta name=title content="《A Unified Approach to interpreting Model Predictions》论文解读 &#183; 从百草园到三味书屋"><meta name=description content="SHAP值是一种复杂模型解释方法，它的计算原理是什么样的呢？"><link rel=canonical href=https://youngforever.tech/posts/20230106-shap/><link type=text/css rel=stylesheet href=/css/main.bundle.min.1b07026ad335b957c216b421a55e1733ac5df10d02ae749f769a48518d7c9394.css integrity="sha256-GwcCatM1uVfCFrQhpV4XM6xd8Q0CrnSfdppIUY18k5Q="><script type=text/javascript src=/js/appearance.min.022d0ebc3b46a335eb1c7ef79b7f2de143d7cd5156d433638592ef1ce5f8554e.js integrity="sha256-Ai0OvDtGozXrHH73m38t4UPXzVFW1DNjhZLvHOX4VU4="></script>
<script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.75206d23ef83f4908b2bdd2317bf6ddff399e9173a16fff5451c40b8e857cfa8.js integrity="sha256-dSBtI++D9JCLK90jF79t3/OZ6Rc6Fv/1RRxAuOhXz6g=" data-copy=Copy data-copied=Copied></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="《A Unified Approach to interpreting Model Predictions》论文解读"><meta property="og:description" content="SHAP值是一种复杂模型解释方法，它的计算原理是什么样的呢？"><meta property="og:type" content="article"><meta property="og:url" content="https://youngforever.tech/posts/20230106-shap/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-06T16:59:00+00:00"><meta property="article:modified_time" content="2023-01-06T16:59:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="《A Unified Approach to interpreting Model Predictions》论文解读"><meta name=twitter:description content="SHAP值是一种复杂模型解释方法，它的计算原理是什么样的呢？"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"《A Unified Approach to interpreting Model Predictions》论文解读","headline":"《A Unified Approach to interpreting Model Predictions》论文解读","description":"SHAP值是一种复杂模型解释方法，它的计算原理是什么样的呢？","inLanguage":"en","url":"https:\/\/youngforever.tech\/posts\/20230106-shap\/","author":{"@type":"Person","name":"Yong"},"copyrightYear":"2023","dateCreated":"2023-01-06T16:59:00\u002b00:00","datePublished":"2023-01-06T16:59:00\u002b00:00","dateModified":"2023-01-06T16:59:00\u002b00:00","keywords":["machine learning","SHAP"],"mainEntityOfPage":"true","wordCount":"2095"}]</script><meta name=author content="Yong"><link href=https://github.com/yongfanbeta rel=me><link href=https://www.zhihu.com/people/kingofnight rel=me><link href=mailto:yongfan2020@outlook.com rel=me><link type=text/css rel=stylesheet href=/lib/katex/katex.min.b7600b193c9447a8351c98870a649381edb21ac786a1f74ef3c43ecce590c7f3748a479c570d18bb239eb7dc590d5f3e571d18d27d5addfdd0c7757d09c9ec28.css integrity="sha512-t2ALGTyUR6g1HJiHCmSTge2yGseGofdO88Q+zOWQx/N0ikecVw0YuyOet9xZDV8+Vx0Y0n1a3f3Qx3V9CcnsKA=="><script defer src=/lib/katex/katex.min.10a5b962f294de1a72c8e70dea34270313bf2fc82db3e61d615e98ca6b65f2993d625605b6a2608a1391b91a015caf3f70e2256a9a5d53a32cf0a74aeb204280.js integrity="sha512-EKW5YvKU3hpyyOcN6jQnAxO/L8gts+YdYV6Yymtl8pk9YlYFtqJgihORuRoBXK8/cOIlappdU6Ms8KdK6yBCgA=="></script>
<script defer src=/lib/katex/auto-render.min.8968ae052e67b7aafad1f0b3dba35dd19a9ed276e4d594c841b9772afee462c5fec8a314147ce3687dbe02733abe9d97b3e80d99a0405562634a6b8fc3be847e.js integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-F4R8ND3VQP"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-F4R8ND3VQP",{anonymize_ip:!1})}</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold pe-2 text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral print:hidden sm:py-10"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>从百草园到三味书屋</a></div><ul class="flex list-none flex-col ltr:text-right rtl:text-left sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">自画像</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/ai4h/ title=医学人工智能周刊><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">知无涯</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">朝花夕拾</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/people/ title=Friends><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">远方朋</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/tags/ title=Tags><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">标签</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="relative inline-block align-text-bottom px-1 icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">《A Unified Approach to interpreting Model Predictions》论文解读</h1><div class="mt-8 mb-12 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2023-01-06 16:59:00 +0000 UTC">6 January 2023</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">5 mins</span></div><div class="my-1 text-xs leading-relaxed text-neutral-500 dark:text-neutral-400"><a href=/categories/paper/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Paper</a>
<a href=/tags/machine-learning/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">machine learning</a>
<a href=/tags/shap/ class="rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">SHAP</a></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="-ms-5 block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="-ms-5 border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#interpretation-model-properties>Interpretation model properties</a></li><li><a href=#additive-feature-attribution-methods>Additive Feature Attribution methods</a></li><li><a href=#classic-shapley-value-estimation>Classic Shapley Value Estimation</a></li><li><a href=#shap>SHAP</a></li><li><a href=#kernel-shap>Kernel SHAP</a><ul><li><a href=#lime>LIME</a></li><li><a href=#kernelshaplinear-lime--shapley-value>KernelSHAP（Linear LIME + Shapley value）</a></li></ul></li><li><a href=#deep-shap>Deep SHAP</a><ul><li><a href=#deepliftlearning-important-features>DeepLIFT（Learning Important FeaTures）</a></li><li><a href=#deepshapdeeplift--shapley-value>DeepSHAP(DeepLIFT + Shapley value)</a></li></ul></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-prose grow"><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><h2 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><ul><li>大数据让复杂模型的优势明显</li><li>提出一种新颖统一的方法用于模型解释<ul><li>用模型的方法来解释复杂模型（用魔法打败魔法）</li><li>提出SHAP值作为各种方法近似统一特征重要度度量</li><li>提出新的SHAP值估计方法</li></ul></li></ul><h2 id=interpretation-model-properties class="relative group">Interpretation model properties <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#interpretation-model-properties aria-label=Anchor>#</a></span></h2><p>描述解释模型需要有的三个性质，而现在解释方法的缺陷有哪些<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><ul><li>局部准确性：如果x‘是x的简化特征，对应解释模型g(x&rsquo;) = f(x)，即解释模型在给定的特征情况下能解释为什么模型预测值是这么多。</li><li>缺失性：当x&rsquo;=0的时候，贡献度$\phi$为0</li><li>一致性：模型改变导致特征变的更重要时，贡献度也应该变大</li></ul><h2 id=additive-feature-attribution-methods class="relative group">Additive Feature Attribution methods <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#additive-feature-attribution-methods aria-label=Anchor>#</a></span></h2><p>一大类方法中解释模型是一系列二元变量的线性函数
称为<strong>Additive Feature Attribution methods</strong>（AFA）相加特征归因方法
$$g(z&rsquo;) = \phi_0 + \sum_{i=1}^{M}\phi_i z&rsquo;_{i}$$
$z&rsquo; \in {0, 1}^M$</p><h2 id=classic-shapley-value-estimation class="relative group">Classic Shapley Value Estimation <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#classic-shapley-value-estimation aria-label=Anchor>#</a></span></h2><p>$$\phi_{i} = \sum_{S \subseteq F\backslash{i}}\frac{|S|!(|F|-|S|-1)!}{|F|!}[f_{S\cup{i}}(x_{S\cup{i}})-f_{S(x_S)}]$$
基于上面公式精确计算很麻烦，特征的排列有$2^F$种，计算量巨大</p><h2 id=shap class="relative group">SHAP <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#shap aria-label=Anchor>#</a></span></h2><p>SHAP分为模型无关和模型相关两类方法用来近似求解，模型无关的代表是kernelSHAP，而模型相关的代表则有DeepSHAP和TreeSHAP，一个是针对深度学习，一个是针对树模型。</p><h2 id=kernel-shap class="relative group">Kernel SHAP <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-shap aria-label=Anchor>#</a></span></h2><h3 id=lime class="relative group">LIME <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#lime aria-label=Anchor>#</a></span></h3><p>LIME<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>（Local interpretable model-agnostic explanations)（why should I trust you: Explaining the predictions of any classifier）通过生成的包含需要解释点周围的扰动数据和基于黑箱模型预测结果的数据集，训练一个可以解释的模型，比如逻辑回归、决策树，这个可解释模型需要在解释点周围达到较好的效果。
$$\xi = \mathop{\arg\min}\limits_{g\in G}L(f,g,\pi_x) + \Omega(g)$$</p><ul><li>f为需解释模型</li><li>g为可能的解释模型</li><li>$\pi_x$为定义实例周围多大范围</li></ul><p>算法过程：</p><ul><li>选择需要解释感兴趣的实例</li><li>对其进行扰动，并得到黑箱模型对应结果产生新数据集</li><li>根据与实例的接近程度，对新数据集进行赋予权重</li><li>基于新数据集和上述损失函数求解可解释模型</li><li>解释预测值<figure><img class="mx-auto my-0 rounded-md" src=https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/weightshap.png alt loading=lazy></figure>Figure 3: Toy example to present intuition for LIME</li></ul><h3 id=kernelshaplinear-lime--shapley-value class="relative group">KernelSHAP（Linear LIME + Shapley value） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernelshaplinear-lime--shapley-value aria-label=Anchor>#</a></span></h3><p>LIME（Local interpretable model-agnostic explanations）方法的拓展，通过修改LIME需要求解loss等式参数，其主要思路是利用核变换，让$\phi$符合shapley value<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>
算法过程<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>：</p><ul><li>根据$z_k^{&rsquo;} \in {0, 1}^M$选择k个样本</li><li>将$z_k^{&rsquo;} \in {0, 1}^M$转化为原始特征值并计算黑箱模型预测值</li><li>基于SHAP kernel计算$z_k^{&rsquo;}$ 样本权重，$z_k$里面1的个数不一样权重就不一样</li><li>拟合线性模型</li><li>从线性模型中返回Shapley values
SHAP核为(推导过程见2补充材料)：
$$\pi_{x&rsquo;}(z&rsquo;)= \frac{(M-1)}{(M choose |z&rsquo;|)|z&rsquo;|(M-|z&rsquo;|)}$$
$$L(f,g, \pi_{x&rsquo;})=\sum_{z&rsquo;\in Z}[f(h_x^{-1}(z&rsquo;))-g(z&rsquo;)]^2\pi_{x&rsquo;}(z&rsquo;)$$
x&rsquo;为简化输入，$x=h_x(x&rsquo;)$, $z&rsquo; \subseteq x&rsquo;$
其中第二步：The function h maps 1’s to the corresponding value from the instance x that we want to explain. For tabular data, it maps 0’s to the values of another instance that we sample from the data. This means that we equate “feature value is absent” with “feature value is replaced by random feature value from data”.</li></ul><h2 id=deep-shap class="relative group">Deep SHAP <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#deep-shap aria-label=Anchor>#</a></span></h2><h3 id=deepliftlearning-important-features class="relative group">DeepLIFT（Learning Important FeaTures） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#deepliftlearning-important-features aria-label=Anchor>#</a></span></h3><p>DeepLIFT<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>方法使用神经元的激活与其“参考”进行比较，其中参考是神经元在网络获得“参考输入”时具有的激活状态（参考输入根据具体任务的内容定义）。该方法赋予每个特征重要度分数之和等于预测值与基于参考输入的预测值之间的差异<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>。
能解决基于梯度方法的不足，例如参考的差异不是0的情况下梯度仍然可能是0。
$$\sum_{i=1}^n C_{\Delta x_i \Delta t} = \Delta t \tag1$$
$\Delta t = t - t^0$, 神经元输出与参考输出的差异，$\Delta x$输入相对参考输入的变化， C即特征的贡献</p><p>$$m_{\Delta x\Delta t} = \frac{C_{\Delta x\Delta t}}{\Delta x} \tag2$$
定义乘子multiplier，满足链式法则
算法步骤<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>：</p><ul><li>定义参考值<ul><li>choosing a good reference would rely on domain-specific knowledge, and in some cases it may be best to compute DeepLIFT scores against multiple different references</li><li>比如图像用全0或者模糊版本，基因数据用本底期望频率</li></ul></li><li>区分正负贡献（2019）</li><li>贡献度规则<ul><li>线性层直接是系数$w* \Delta x_i$</li><li>非线性变化是$m_{\Delta x\Delta y} = \frac{C_{\Delta x\Delta y}}{\Delta x} =\frac{\Delta y}{\Delta x}$</li></ul></li></ul><h3 id=deepshapdeeplift--shapley-value class="relative group">DeepSHAP(DeepLIFT + Shapley value) <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#deepshapdeeplift--shapley-value aria-label=Anchor>#</a></span></h3><p>尽管kernelSHAP是适用于所有模型的包括深度学习模型的一种可解释方法，但是有没有能利用神经网络特性的可解释方法从而提高计算效率。
DeepLIFT计算的分数近似于Shapley value?并不是这个思路</p><ul><li>the Shapely values measure the average marginal effect of including an input over all possible orderings in which inputs can be included. If we define “including” an input as setting it to its actual value instead of its reference value, DeepLIFT can be thought of as a fast approximation of the Shapely values<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup></li><li>Though a variety of methods exist for estimating SHAP values, we implemented a modified version of the DeepLIFT algorithm, which computes SHAP by estimating differences in model activations during backpropagation relative to a standard reference.<figure><img class="mx-auto my-0 rounded-md" src=https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/components.png alt loading=lazy></figure><figure><img class="mx-auto my-0 rounded-md" src=https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/shap.png alt loading=lazy></figure>figure from ref[3]</li><li>基于Shapley value的定义以及公式可以看出重要的一部分即边际效应，即模型包含该特征减去未包含该部分。在上述一个神经网络模块里面，特征顺序选择都不存在。在认为包含特征即相对于参考输入是真实输入的情况下，把包含特征后乘子直接链式法则做为SHAP值近似公式</li><li>在上述简单网络组件里面，输入到输出之间可以看作线性近似从而得到公式16</li><li>把用实际值代替参考值看作是包含某个特征，DeepLIFT方法与DeepSHAP似乎看不到区别?</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://blog.ml.cmu.edu/2020/08/31/6-interpretability/ target=_blank rel="noreferrer noopener">6 – Interpretability – Machine Learning Blog | ML@CMU | Carnegie Mellon University</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Ribeiro, M. T., Singh, S. & Guestrin, C. ‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier. <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> 1135–1144 (2016) doi:<a href=https://doi.org/10.1145/2939672.2939778 target=_blank rel="noreferrer noopener">10.1145/2939672.2939778</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html target=_blank rel="noreferrer noopener">A Unified Approach to Interpreting Model Predictions</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://christophm.github.io/interpretable-ml-book/shap.html target=_blank rel="noreferrer noopener">9.6 SHAP (SHapley Additive exPlanations) | Interpretable Machine Learning</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a href=https://libraries.io/pypi/deeplift target=_blank rel="noreferrer noopener">deeplift 0.6.13.0 on PyPI - Libraries.io</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p><a href=https://towardsdatascience.com/explainable-neural-networks-recent-advancements-part-3-6a838d15f2fb target=_blank rel="noreferrer noopener">Explainable Neural Networks: Recent Advancements, Part 3 | by G Roshan Lal | Towards Data Science</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Shrikumar, A., Greenside, P., Shcherbina, A. & Kundaje, A. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences. Preprint at <a href=https://doi.org/10.48550/arXiv.1605.01713 target=_blank rel="noreferrer noopener">https://doi.org/10.48550/arXiv.1605.01713</a> (2017).&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Shrikumar, A., Greenside, P. & Kundaje, A. Learning Important Features Through Propagating Activation Differences. Preprint at <a href=http://arxiv.org/abs/1704.02685 target=_blank rel="noreferrer noopener">http://arxiv.org/abs/1704.02685</a> (2019).&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/20200401-pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">PyTorch深度学习（2）</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2020-04-01 14:51:32 +0000 UTC">1 April 2020</time></span></span></a></span>
<span><a class="group flex text-right" href=/posts/20230113-introduction-deep-learning/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">MIT 6.S91 Introduction Deep Learning Notes</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-01-13 16:59:00 +0000 UTC">13 January 2023</time></span></span>
<span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><div id=cusdis_thread data-host=https://cusdis.com data-app-id=1cfc2662-243a-4573-b9e7-8b2b4a986fba data-page-id=069357d56acf419ea7d5151d50aa5c89 data-page-url=https://youngforever.tech/posts/20230106-shap/ data-page-title="《A Unified Approach to interpreting Model Predictions》论文解读"></div><script src=https://cusdis.com/js/cusdis.es.js></script></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2023
Yong</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://git.io/hugo-congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex h-12 w-12 items-center justify-center dark:hidden" title="Switch to dark appearance"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="hidden h-12 w-12 items-center justify-center dark:flex" title="Switch to light appearance"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://youngforever.tech/><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative inline-block align-text-bottom px-1 icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>