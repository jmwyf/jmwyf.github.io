<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><title>Skip-gram模型（2） &#183; 从百草园到三味书屋</title>
<meta name=title content="Skip-gram模型（2） &#183; 从百草园到三味书屋"><script type=text/javascript src=/js/appearance.min.022d0ebc3b46a335eb1c7ef79b7f2de143d7cd5156d433638592ef1ce5f8554e.js integrity="sha256-Ai0OvDtGozXrHH73m38t4UPXzVFW1DNjhZLvHOX4VU4="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.7a4eb2393c3473718b354035a4da83197bb0e83d35aef6fd3c306ca3e7d332e3.css integrity="sha256-ek6yOTw0c3GLNUA1pNqDGXuw6D01rvb9PDBso+fTMuM="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.ef84f1e4c888f2aa2d84745ad4e1d75d27a8f29d55f84702214c97ecd79fba5b.js integrity="sha256-74Tx5MiI8qothHRa1OHXXSeo8p1V+EcCIUyX7Nefuls=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      Skip-gram模型pytorch和tensorflow实践
    "><link rel=canonical href=https://youngforever.tech/posts/20230205-skip-gram-part2/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="Skip-gram模型（2）"><meta property="og:description" content="Skip-gram模型pytorch和tensorflow实践"><meta property="og:type" content="article"><meta property="og:url" content="https://youngforever.tech/posts/20230205-skip-gram-part2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-12T19:36:02+00:00"><meta property="article:modified_time" content="2023-07-12T19:36:02+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Skip-gram模型（2）"><meta name=twitter:description content="Skip-gram模型pytorch和tensorflow实践"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Skip-gram模型（2）","headline":"Skip-gram模型（2）","description":"Skip-gram模型pytorch和tensorflow实践","inLanguage":"en","url":"https:\/\/youngforever.tech\/posts\/20230205-skip-gram-part2\/","author":{"@type":"Person","name":"Yong"},"copyrightYear":"2023","dateCreated":"2023-07-12T19:36:02\u002b00:00","datePublished":"2023-07-12T19:36:02\u002b00:00","dateModified":"2023-07-12T19:36:02\u002b00:00","keywords":["nlp"],"mainEntityOfPage":"true","wordCount":"1547"}</script><meta name=author content="Yong"><link href=https://github.com/yongfanbeta rel=me><link href=https://www.zhihu.com/people/Havefan rel=me><link href=mailto:yongfan2020@outlook.com rel=me><link type=text/css rel=stylesheet href=/lib/katex/katex.min.7c7c1a59e6eec00ecd485e5083f69fe14783c7d5ea52962938682e6e8df25aef9bd88411e14790c1b6f8d938bb81502336d56823f23d4c4a187aef689b0702ea.css integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g=="><script defer src=/lib/katex/katex.min.2d037120c479ad7bfba3e6f597cf8dd4464c7e11bb88567d1e19db2644e9e338cdaf95afb2def902a51e143c5e4546bb979bc40f2522022a7ffebf8414b2c2c8.js integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA=="></script><script defer src=/lib/katex/auto-render.min.8968ae052e67b7aafad1f0b3dba35dd19a9ed276e4d594c841b9772afee462c5fec8a314147ce3687dbe02733abe9d97b3e80d99a0405562634a6b8fc3be847e.js integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-F4R8ND3VQP"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-F4R8ND3VQP",{anonymize_ip:!1})}</script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 sm:px-14 md:px-24 lg:px-32 dark:bg-neutral-800 dark:text-neutral"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 print:hidden sm:py-10 dark:text-neutral"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>从百草园到三味书屋</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">自画像</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/ai4h/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">知无涯</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">朝花夕拾</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/people/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">远方朋</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/tags/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">标签</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Skip-gram模型（2）</h1><div class="mb-12 mt-8 text-base text-neutral-500 print:hidden dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2023-07-12 19:36:02 +0000 UTC">12 July 2023</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">4 mins</span></div><div class="my-1 flex flex-wrap text-xs leading-relaxed text-neutral-500 dark:text-neutral-400"><a href=/categories/deep-learning/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Deep Learning</a>
<a href=/tags/nlp/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Nlp</a></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="-ms-5 block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 lg:hidden dark:bg-neutral-700 dark:text-neutral-100">Table of Contents</summary><div class="-ms-5 border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#回顾>回顾</a></li><li><a href=#计算>计算</a></li><li><a href=#skip-gram-pytorch实现>skip-gram PyTorch实现</a></li><li><a href=#skip-gram-tensorflow实现>skip-gram Tensorflow实现</a></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><blockquote><p><a href=https://youngforever.tech/posts/20230205-skip-gram-part1/ target=_blank rel=noreferrer>之前文章</a>介绍了skip-gram的原理，这篇文章给出模型的实现细节以及pytorch和tensorflow的实现。</p></blockquote><h2 id=回顾 class="relative group">回顾 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%9b%9e%e9%a1%be aria-label=Anchor>#</a></span></h2><p>假如用余弦相似度来计算两个词的one-hot编码得到0，即不能编码词之间的相似性，所以有了word2vec的方法，包括skip-gram和CBOW。</p><p>接前文，假如我们拥有10000个单词的词汇表，想嵌入300维的词向量，那么我们的<strong>输入-隐层权重矩阵</strong>和<strong>隐层-输出层的权重矩阵</strong>都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难。<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> 所以在具体实践上有一些计算技巧。</p><h2 id=计算 class="relative group">计算 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e8%ae%a1%e7%ae%97 aria-label=Anchor>#</a></span></h2><p>skip-gram模型基于单个输入来预测上下文，对于T个训练单词$w_1,&mldr;w_T$，即最大化以下概率$$\frac{1}{T}\sum_{t=1}^T\sum_{-c&lt;=j&lt;=c}logP(w_{t+j}|w_t)\tag1$$其中c为训练上下文的窗口大小，$p(w_{t+j}|w_t)$用softmax来计算$$p(w_o|w_i)=\frac{exp(v_{wo}^Tv_{wi})}{\sum_{w=1}^Wexp(v_{wo}^Tv_{wi})}$$其中$v_{wi}$为输入词向量的表征，在单词量W巨大的情况下（通常都有$10^5-10^7$），式1的计算开销巨大。</p><p>在skip-gram实际算法中使用多种策略来减少模型的资源使用（内存）以及提高词向量表征质量<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><ul><li>负采样<ul><li>从隐藏层到输出的Softmax层的计算量很大，因为要计算所有词的Softmax概率，再去找概率最大的值。例如当我们用训练样本 ( input word: &ldquo;fox&rdquo;，output word: &ldquo;quick&rdquo;) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。<strong>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重</strong>, 我们也会对我们的“positive” word进行权重更新<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>。在实践中，​通常使用的是unigram分布的平方根，​即词汇表中每个词的概率的0.75次方除以归一化常数来挑选负样本。</li></ul></li><li>高频词进行抽样<ul><li>原因：高频词相对于低频词来说提供的信息少；高频词随着样本增多本身表示也不会发生太大变化</li><li>使用概率P来丢掉一定的单词$$P(w)=1- \sqrt{\frac{t}{f(w_i)}}$$其中t为设定的阈值，$f(w_i)$为单词出现的频率，可以看到频率越高丢弃的概率越大，反之越小</li></ul></li><li>单词组合成词组作为单个词处理<ul><li>原因：组合词有特定的意思，不是简单把单个词的表示聚合起来</li><li>如何从文本中提取出词组研究不少，skip-gram文章选用了$$socre(w_i, w_j)=\frac{count(w_iw_j)-\delta}{count(w_i)*count(w_j)}$$其中$w_i, w_j$代表不同的单词，利用score得分与设定的阈值比较来确定是否为常见词组</li></ul></li></ul><h2 id=skip-gram-pytorch实现 class="relative group">skip-gram PyTorch实现 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#skip-gram-pytorch%e5%ae%9e%e7%8e%b0 aria-label=Anchor>#</a></span></h2><p>Word2vec skip-gram pytorch<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><p><a href=https://github.com/yongfanbeta/skip-gram/blob/main/skipgram-pytorch.ipynb target=_blank rel=noreferrer>skipgram-pytorch.ipynb</a></p><h2 id=skip-gram-tensorflow实现 class="relative group">skip-gram Tensorflow实现 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#skip-gram-tensorflow%e5%ae%9e%e7%8e%b0 aria-label=Anchor>#</a></span></h2><p>Word2vec skip-gram tensorflow<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></p><p><a href=https://github.com/yongfanbeta/skip-gram/blob/main/skipgram-tf.ipynb target=_blank rel=noreferrer>skipgram-tf.ipynb</a></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://zhuanlan.zhihu.com/p/27234078 target=_blank rel=noreferrer>理解 Word2Vec 之 Skip-Gram 模型 - 知乎</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://arxiv.org/pdf/1310.4546.pdf target=_blank rel=noreferrer>Distributed Representations of Words and Phrases and their Compositionality</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://www.jianshu.com/p/cf36d4c1ea39 target=_blank rel=noreferrer>关于skip-gram和负采样 - 简书</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=http://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html target=_blank rel=noreferrer>14.1. 词嵌入（word2vec） — 动手学深度学习 2.0.0 documentation</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a href=https://www.tensorflow.org/tutorials/text/word2vec#negative_sampling_for_one_skip-gram target=_blank rel=noreferrer>word2vec  |  TensorFlow Core</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/20240410-how-to-read-a-paper/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">如何阅读论文</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2023-04-09 19:59:00 +0000 UTC">9 April 2023</time>
</span></span></a></span><span></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><div id=cusdis_thread data-host=https://cusdis.com data-app-id=1cfc2662-243a-4573-b9e7-8b2b4a986fba data-page-id=47de757e067ca26c65c6aee5ffdb9a28 data-page-url=https://youngforever.tech/posts/20230205-skip-gram-part2/ data-page-title=Skip-gram模型（2）></div><script src=https://cusdis.com/js/cusdis.es.js></script></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
Yong</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"><div class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex h-12 w-12 items-center justify-center dark:hidden" title="Switch to dark appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="hidden h-12 w-12 items-center justify-center dark:flex" title="Switch to light appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm sm:p-6 md:p-[10vh] lg:p-[12vh] dark:bg-neutral-900/50" data-url=https://youngforever.tech/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>