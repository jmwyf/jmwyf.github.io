<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><title>重读XGBoost &#183; 从百草园到三味书屋</title>
<meta name=title content="重读XGBoost &#183; 从百草园到三味书屋"><script type=text/javascript src=/js/appearance.min.022d0ebc3b46a335eb1c7ef79b7f2de143d7cd5156d433638592ef1ce5f8554e.js integrity="sha256-Ai0OvDtGozXrHH73m38t4UPXzVFW1DNjhZLvHOX4VU4="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.7a4eb2393c3473718b354035a4da83197bb0e83d35aef6fd3c306ca3e7d332e3.css integrity="sha256-ek6yOTw0c3GLNUA1pNqDGXuw6D01rvb9PDBso+fTMuM="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.f29ffdffd9ab4cc95250c3c7196b2d5dae8ee6ef0a4139451073f90183ae7e31.js integrity="sha256-8p/9/9mrTMlSUMPHGWstXa6O5u8KQTlFEHP5AYOufjE=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      XGBoost: A Scalable Tree Boosting System
    "><link rel=canonical href=https://youngforever.tech/posts/20200323-%E9%87%8D%E8%AF%BBxgboost/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="重读XGBoost"><meta property="og:description" content="XGBoost: A Scalable Tree Boosting System"><meta property="og:type" content="article"><meta property="og:url" content="https://youngforever.tech/posts/20200323-%E9%87%8D%E8%AF%BBxgboost/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-03-23T16:18:13+00:00"><meta property="article:modified_time" content="2020-03-23T16:18:13+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="重读XGBoost"><meta name=twitter:description content="XGBoost: A Scalable Tree Boosting System"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"重读XGBoost","headline":"重读XGBoost","description":"XGBoost: A Scalable Tree Boosting System","inLanguage":"en","url":"https:\/\/youngforever.tech\/posts\/20200323-%E9%87%8D%E8%AF%BBxgboost\/","author":{"@type":"Person","name":"Yong"},"copyrightYear":"2020","dateCreated":"2020-03-23T16:18:13\u002b00:00","datePublished":"2020-03-23T16:18:13\u002b00:00","dateModified":"2020-03-23T16:18:13\u002b00:00","keywords":["machine learning","xgboost"],"mainEntityOfPage":"true","wordCount":"3199"}</script><meta name=author content="Yong"><link href=https://github.com/yongfanbeta rel=me><link href=https://www.zhihu.com/people/Havefan rel=me><link href=mailto:yongfan2020@outlook.com rel=me><link type=text/css rel=stylesheet href=/lib/katex/katex.min.7c7c1a59e6eec00ecd485e5083f69fe14783c7d5ea52962938682e6e8df25aef9bd88411e14790c1b6f8d938bb81502336d56823f23d4c4a187aef689b0702ea.css integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g=="><script defer src=/lib/katex/katex.min.2d037120c479ad7bfba3e6f597cf8dd4464c7e11bb88567d1e19db2644e9e338cdaf95afb2def902a51e143c5e4546bb979bc40f2522022a7ffebf8414b2c2c8.js integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA=="></script><script defer src=/lib/katex/auto-render.min.8968ae052e67b7aafad1f0b3dba35dd19a9ed276e4d594c841b9772afee462c5fec8a314147ce3687dbe02733abe9d97b3e80d99a0405562634a6b8fc3be847e.js integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-F4R8ND3VQP"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-F4R8ND3VQP",{anonymize_ip:!1})}</script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 sm:px-14 md:px-24 lg:px-32 dark:bg-neutral-800 dark:text-neutral"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 print:hidden sm:py-10 dark:text-neutral"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>从百草园到三味书屋</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">自画像</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/ai4h/ title=医学人工智能周刊><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">知无涯</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">朝花夕拾</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/people/ title=Friends><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">远方朋</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/tags/ title=Tags><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">标签</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">重读XGBoost</h1><div class="mb-12 mt-8 text-base text-neutral-500 print:hidden dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime="2020-03-23 16:18:13 +0000 UTC">23 March 2020</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">7 mins</span></div><div class="my-1 flex flex-wrap text-xs leading-relaxed text-neutral-500 dark:text-neutral-400"><a href=/categories/paper/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Paper</a>
<a href=/tags/machine-learning/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">machine learning</a>
<a href=/tags/xgboost/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">xgboost</a></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="-ms-5 block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 lg:hidden dark:bg-neutral-700 dark:text-neutral-100">Table of Contents</summary><div class="-ms-5 border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#1-引言>1. 引言</a></li><li><a href=#2-提升树tree-boosting简介>2. 提升树（Tree Boosting）简介</a><ul><li><a href=#21-正则化目标函数>2.1 正则化目标函数</a></li><li><a href=#22-梯度提升树gradient-tree-boosting>2.2 梯度提升树（Gradient Tree Boosting）</a></li><li><a href=#23-缩减和列抽样shrinkage-and-column-subsampling>2.3 缩减和列抽样（Shrinkage and Column Subsampling）</a></li></ul></li><li><a href=#3-分裂点寻找算法>3. 分裂点寻找算法</a><ul><li><a href=#31-精确贪婪算法basic-exact-greedy-algorithm>3.1 精确贪婪算法（Basic Exact Greedy Algorithm）</a></li><li><a href=#32-近似算法approximate-algorithm>3.2 近似算法（Approximate Algorithm）</a></li><li><a href=#33-加权分位数图weighted-quantile-sketch>3.3 加权分位数图（Weighted Quantile Sketch）</a></li><li><a href=#34-稀疏值感知分裂sparsity-aware-split-finding>3.4 稀疏值感知分裂（Sparsity-aware split finding）</a></li></ul></li><li><a href=#4-系统设计>4. 系统设计</a><ul><li><a href=#41-分块并行column-block-for-parallel-learning>4.1 分块并行（Column Block for Parallel Learning）</a></li><li><a href=#42-缓存优化>4.2 缓存优化</a></li><li><a href=#43-外存计算>4.3 外存计算</a></li></ul></li><li><a href=#5-端到端评估>5. 端到端评估</a></li><li><a href=#ref>ref</a></li><li><a href=#补充>补充</a></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><p>在使用xgboost方法调参时，对其中个别参数不是特别理解。故重新读了一遍原论文。</p><h2 id=1-引言 class="relative group">1. 引言 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#1-%e5%bc%95%e8%a8%80 aria-label=Anchor>#</a></span></h2><p>阐述机器学习和数据驱动的方法应用时两个重要的因素：</p><ul><li>能捕捉数据间复杂依赖关系的模型</li><li>可扩展的学习系统，可以从大量数据中学习</li></ul><p>在目前常用的方法中，梯度提升树（gradient tree boosting）在许多场景中效果都不错，作者列举了一些。提出xgboost方法在比赛以及各类问题中的应用。</p><p>叙述XGBoost的优点：运行更快、拓展性更好。创新点包括：</p><ul><li>高度可拓展的端到端提升树（tree boosting）系统</li><li>用于高效计算的加权分位数图（weighted quantile sketch）</li><li>新颖的稀疏感知算法（sparsity-aware algorithm），用于并行树学习</li><li>有效的缓存优化以及块（cache-aware block）结构用于外存（out-of-core）树学习
关于以上几点在正文中详解。</li></ul><p>论文结构：</p><ol><li>提升树（tree boosting）简介以及目标函数正则化</li><li>分裂点寻找的方法</li><li>系统设计，包括为每个优化提供量化支持的结果</li><li>相关工作</li><li>详细的端到端评估</li><li>总结</li></ol><h2 id=2-提升树tree-boosting简介 class="relative group">2. 提升树（Tree Boosting）简介 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#2-%e6%8f%90%e5%8d%87%e6%a0%91tree-boosting%e7%ae%80%e4%bb%8b aria-label=Anchor>#</a></span></h2><p>首先需要了解CART（Classification And Regression Tree）算法，对于cart分类树和回归树分别采用了：<code>Gini系数</code>、<code>和方差</code>度量方式来划分节点[1]。例如回归树，对于划分特征A, 划分点s使两边数据集D1和D2,求出使
D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小。
$$\underline{min}_{\text{A,s}}[\underline{min}_{\text{c1}}\sum_{x_i\in{D1(A,s)}}(y_i - c1)^2+\underline{min}_{\text{c2}}\sum_{x_i\in{D2(A,s)}}(y_i - c2)^2]$$
其中，c1为D1的样本输出均值，c2为D2的样本输出均值, 回归树采用叶子节点的均值或者中位数来预测输出结果，$y_i$即样本的label，此时的输出值即下文中用到的$w_{q(x)}$。</p><h3 id=21-正则化目标函数 class="relative group">2.1 正则化目标函数 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#21-%e6%ad%a3%e5%88%99%e5%8c%96%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0 aria-label=Anchor>#</a></span></h3><p>对于这种类型的集成树模型，用K棵树的结果来预测最后的结果（式1），那么问题来了我们怎么来求这些树的参数，每棵树都可以看做一个函数$f_i$包含树的结构以及最后叶节点权重，集成模型不像传统优化问题一样通过简单用梯度下降可以对所有的树进行学习求解，所以，在这里用到了加法策略，即固定已经学习到的，每次加一棵树来进行学习（式3）。
$$\hat{y_i} = \phi(x_i) = \sum_{k = 1}^{K} f_k(x_i)\tag1$$
其中$f(x) = w_{q(x)}$，每个$f_k$对应一个独立的树结构q以及其叶节点权重w，为了学习模型中的参数，最小化下面正则化的目标函数。
$$L(\phi) = \sum_i l(\hat{y_i}, y_i) + \sum_k \Omega(f_k)\tag2$$</p><p>$$\Omega(f_k) = \gamma T + \frac{1}{2}\lambda||w||^2$$
T是树的叶节点数</p><h3 id=22-梯度提升树gradient-tree-boosting class="relative group">2.2 梯度提升树（Gradient Tree Boosting） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#22-%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e6%a0%91gradient-tree-boosting aria-label=Anchor>#</a></span></h3><p>第t次预测值等于t-1加上第t棵树的结果
$$\hat{y_i} = \hat{y_i}^{t-1} + f_t(x_i)\tag3$$
此时目标函数(式2)可以写成
$$L^{(t)} = \sum_{i=1} ^n l(y_i, \hat{y_i}^{(t-1)}+f_t(x_i)) + \Omega(f_t)\tag4$$
该式子的二阶近似可以表达为(式5），可以参考补充中
<a href=#taylor>二阶泰勒展开的一般形式</a>
$$L^{(t)} \approx \sum_{i=1} ^n [l(y_i, \hat{y_i}^{(t-1)}) + g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)\tag5$$
其中 $g_i=\partial_{\hat{y}^{(t-1)}}{l(y_i, \hat{y_i}^{(t-1)})}， h_i=\partial_{\hat{y}^{(t-1)}}^2{l(y_i, \hat{y_i}^{(t-1)})}$</p><p>式5中去掉常数项，即label与第t-1次结果的损失函数，可以得到：</p><p>$$\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)\tag6$$</p><p>式6即对新树的优化目标函数，进一步合并可以写为：</p><p>$$obj^{t} \approx \sum_{i=1}^{n}[g_i w_{q(x_i)} + \frac{1}{2}h_i w^2_{q(x_i)}] + \gamma T+ \frac{1}{2}\lambda \sum _{j=1}^T w_j^2$$</p><p>定义$I_j = {i|q(x_i)=j}$即叶节点j上的实例。</p><p>$$obj^{t} \approx \sum_{j=1}^T[(\sum_{i\in{I_j}}g_i)w_j+\frac{1}{2}(\sum_{i\in{I_j}}h_i+\lambda)w_j^2]+\gamma T\tag7$$</p><p>上式7对$w_j$求导即可求出w最优值</p><p>$$w_j^* = -\frac{\sum_{i\in{I_j}}g_i}{\sum_{i\in{I_j}}h_i+\lambda} $$</p><p>令$G_j = \sum_{i\in{I_j}}g_i，H_j=\sum_{i\in{I_j}}h_i$</p><p>此时，对应的最小值为:
$$obj^* = -\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T$$
$\color{red}\frac{G_j^2}{H_j+\lambda}$越大，loss越小，所以对叶节点进行分裂，分裂后增益定义为
$$Gain=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gamma\tag8$$</p><h3 id=23-缩减和列抽样shrinkage-and-column-subsampling class="relative group">2.3 缩减和列抽样（Shrinkage and Column Subsampling） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#23-%e7%bc%a9%e5%87%8f%e5%92%8c%e5%88%97%e6%8a%bd%e6%a0%b7shrinkage-and-column-subsampling aria-label=Anchor>#</a></span></h3><p>除了在目标函数中引入正则项，在防止过拟合方面xgboost还运用了两项技术，给每一步tree boosting得到的结果一个权重$\eta$，来降低每一步的影响从而给后面树的形成留下空间，比喻成优化问题中的学习率缩减；同时还用到随机森林中的列抽样，即随机特征筛选。</p><h2 id=3-分裂点寻找算法 class="relative group">3. 分裂点寻找算法 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#3-%e5%88%86%e8%a3%82%e7%82%b9%e5%af%bb%e6%89%be%e7%ae%97%e6%b3%95 aria-label=Anchor>#</a></span></h2><h3 id=31-精确贪婪算法basic-exact-greedy-algorithm class="relative group">3.1 精确贪婪算法（Basic Exact Greedy Algorithm） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#31-%e7%b2%be%e7%a1%ae%e8%b4%aa%e5%a9%aa%e7%ae%97%e6%b3%95basic-exact-greedy-algorithm aria-label=Anchor>#</a></span></h3><p>即按照2.2中式8来寻找分裂点
python<code>scikit-learn</code>，R<code>gbm</code>，单机的xgboost都支持。</p><div align=center><img src=/images/xgboost/algorithm1.jpg width=50% heigth=50%></div><h3 id=32-近似算法approximate-algorithm class="relative group">3.2 近似算法（Approximate Algorithm） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#32-%e8%bf%91%e4%bc%bc%e7%ae%97%e6%b3%95approximate-algorithm aria-label=Anchor>#</a></span></h3><p>精确贪婪算法由于列举了所有可能的分裂点，在数据量很大不能全部写入内存时会导致不是那么高效。所以提出近似算法。对于每个特征，只考察分位点，减少计算复杂度。
近似算法存在两个变种：</p><ul><li>global: 学习每棵树前，提出候选分裂点</li><li>local: 每次分裂前，重新提出候选分裂点</li></ul><div align=center><img src=/images/xgboost/algorithm2.jpg width=50% heigth=50%></div><h3 id=33-加权分位数图weighted-quantile-sketch class="relative group">3.3 加权分位数图（Weighted Quantile Sketch） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#33-%e5%8a%a0%e6%9d%83%e5%88%86%e4%bd%8d%e6%95%b0%e5%9b%beweighted-quantile-sketch aria-label=Anchor>#</a></span></h3><p>近似算法中最重要一点即提出候选分裂点，xgboost不是简单的按照样本个体进行分位，而是以损失函数二阶导数值作为权重进行分位数分裂。如何寻找二阶导数分位点，首先是利用权重计算排序函数，然后相邻相减值作为判断依据。问题是为什么会想到利用损失函数二阶导数值作为权重来划分。
文中给出式6可以变形为
$$\sum_{i=1}^n\frac{1}{2}h_i(f_t(x_i)-g_i/h_i)^2 + \Omega(f_t) + constant\tag9$$
指出该式恰好是权重平方差损失函数，权重$h_i$以及label $g_i/h_i$
自己从式6变不到式9，觉得中间符号是+还差不多。
看有人理解说变成式10才对。是否作者真的是这样想的，不得而知。欢迎指正。
$$\sum_{i=1}^n\frac{1}{2}h_i(f_t(x_i)-(-g_i/h_i))^2 + \Omega(f_t) + constant\tag{10}$$
<a href=https://datascience.stackexchange.com/questions/10997/need-help-understanding-xgboosts-approximate-split-points-proposal target=_blank rel=noreferrer>stackexchange上关于理解xgboost近似分裂点</a></p><h3 id=34-稀疏值感知分裂sparsity-aware-split-finding class="relative group">3.4 稀疏值感知分裂（Sparsity-aware split finding） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#34-%e7%a8%80%e7%96%8f%e5%80%bc%e6%84%9f%e7%9f%a5%e5%88%86%e8%a3%82sparsity-aware-split-finding aria-label=Anchor>#</a></span></h3><p>造成稀疏值的原因：1）缺失值 2）统计过程中频繁的0值输入 3）one-hot编码以及其他特征工程
所以让算法注意数据中稀疏规律很重要，遍历所有特征，在划分子节点时，统一将该特征的缺失值划分到右支或者左支，计算最大的gain。</p><div align=center><img src=/images/xgboost/Sparsity.jpg width=50% heigth=50%></div><p>$\color{red}这里也有个疑问就是为什么排序第一次是升序，第二次是降序$</p><h2 id=4-系统设计 class="relative group">4. 系统设计 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#4-%e7%b3%bb%e7%bb%9f%e8%ae%be%e8%ae%a1 aria-label=Anchor>#</a></span></h2><h3 id=41-分块并行column-block-for-parallel-learning class="relative group">4.1 分块并行（Column Block for Parallel Learning） <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#41-%e5%88%86%e5%9d%97%e5%b9%b6%e8%a1%8ccolumn-block-for-parallel-learning aria-label=Anchor>#</a></span></h3><p>基于树学习过程中最耗时的是将数据排序，为了减少排序的时间成本，提出基于内存的block结构。</p><ul><li>在Exact greedy算法中，将整个数据集存放在一个Block中</li><li>在近似算法中，使用多个Block，每个Block对应原来数据的子集。不同的Block可以在不同的机器上并行计算</li></ul><h3 id=42-缓存优化 class="relative group">4.2 缓存优化 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#42-%e7%bc%93%e5%ad%98%e4%bc%98%e5%8c%96 aria-label=Anchor>#</a></span></h3><p>这里指利用CPU缓存对算法进行优化。</p><p>4.1中column block按特征大小顺序存储，相应的样本的梯度信息是分散的，造成内存的不连续访问，降低CPU cache命中率。
优化方法：</p><ul><li>对于精确贪婪算法，预取数据到buffer中（非连续->连续），再统计梯度信息。</li><li>对于近似算法，调节block的大小，设置过大则容易导致命中率低，过小则容易导致并行化效率不高。</li></ul><h3 id=43-外存计算 class="relative group">4.3 外存计算 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#43-%e5%a4%96%e5%ad%98%e8%ae%a1%e7%ae%97 aria-label=Anchor>#</a></span></h3><p>除了处理器以及内存，利用磁盘空间来处理不能进入内存的数据也十分重要，数据划分为多个Block并存放在磁盘上。计算的时候，使用独立的线程预先将Block放入主内存，因此可以在计算的同时读取磁盘。在减少计算资源开销以及提高磁盘输入输出方面主要用到以下技术：</p><ul><li>Block压缩，按列压缩，加载到主内存时由独立线程动态解压缩。具体压缩技术参看原文。</li><li>Block Sharding，将数据划分到不同硬盘上，提高磁盘吞吐率。</li></ul><h2 id=5-端到端评估 class="relative group">5. 端到端评估 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#5-%e7%ab%af%e5%88%b0%e7%ab%af%e8%af%84%e4%bc%b0 aria-label=Anchor>#</a></span></h2><p>利用4个数据集对xgboost评估：</p><ul><li>分类问题</li><li>排序问题</li><li>外存计算实验</li><li>分布计算实验</li></ul><p>这几个方面进行评估，详细结果见论文。</p><h2 id=ref class="relative group">ref <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ref aria-label=Anchor>#</a></span></h2><ol><li><a href=https://blog.csdn.net/hy592070616/article/details/81628956 target=_blank rel=noreferrer>CART分类树与回归树</a></li><li><a href=https://www.cnblogs.com/xym4869/p/11282586.html target=_blank rel=noreferrer>Markdown数学公式</a></li><li><a href=https://docs.mathjax.org/en/latest/web/configuration.html target=_blank rel=noreferrer>Mathjax应用在网页</a></li><li><a href=https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf target=_blank rel=noreferrer>XGBoost.ppt</a></li><li><a href=https://xgboost.readthedocs.io/en/latest/tutorials/model.html target=_blank rel=noreferrer>readthedocs xgboost tutorials</a>推荐</li><li><a href=http://wepon.me/files/gbdt.pdf target=_blank rel=noreferrer>gbdt.ppt</a></li><li><a href=https://arxiv.org/abs/1603.02754 target=_blank rel=noreferrer>xgboost原文</a></li></ol><h2 id=补充 class="relative group">补充 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e8%a1%a5%e5%85%85 aria-label=Anchor>#</a></span></h2><ol><li>文中很多术语翻译可能有不恰当的地方，欢迎指出。</li><li>二阶泰勒展开的一般形式：
$$f(x^t) = f(x^{t-1}+\Delta x)\approx{f(x^{t-1})+ f^{\prime}(x^{t-1})\Delta{x}+f^{\prime\prime}(x^{t-1})\frac{\Delta x^2}{2}}$$</li><li>式4中加入loss function是mean squared error(MSE)，可以求出相应的gi， hi作为一个特例来验证该做法。</li><li>基于树的算法理解时带着这几个问题去理解每一步是用来做什么的：选择哪个特征进行分裂？在特征什么点位进行分裂？分裂后叶节点取什么值？<blockquote><p>分别对应：遍历每个特征，加权分位数图，$w_j$</p></blockquote></li><li>对于系统设计中应用到的技术理解不是十分深刻，对应一个算法如何从计算机硬件的方方面面考虑去优化对非专业领域研究者还是比较难</li></ol></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/20200307-lstm%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">LSTM应用场景以及pytorch实例</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2020-03-09 15:07:13 +0000 UTC">9 March 2020</time>
</span></span></a></span><span><a class="group flex text-right" href=/posts/20200401-pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">PyTorch深度学习（2）</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2020-04-01 14:51:32 +0000 UTC">1 April 2020</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><div id=cusdis_thread data-host=https://cusdis.com data-app-id=1cfc2662-243a-4573-b9e7-8b2b4a986fba data-page-id=3a50965232ca714e5e99c94828a3f610 data-page-url=https://youngforever.tech/posts/20200323-%E9%87%8D%E8%AF%BBxgboost/ data-page-title=重读XGBoost></div><script src=https://cusdis.com/js/cusdis.es.js></script></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
Yong</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"><div class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex h-12 w-12 items-center justify-center dark:hidden" title="Switch to dark appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="hidden h-12 w-12 items-center justify-center dark:flex" title="Switch to light appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm sm:p-6 md:p-[10vh] lg:p-[12vh] dark:bg-neutral-900/50" data-url=https://youngforever.tech/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>