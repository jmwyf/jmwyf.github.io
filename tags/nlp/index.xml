<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>nlp on 从百草园到三味书屋</title><link>https://jmwyf.github.io/tags/nlp/</link><description>Recent content in nlp on 从百草园到三味书屋</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 04 Feb 2023 10:57:02 +0000</lastBuildDate><atom:link href="https://jmwyf.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Skip-gram模型（1）</title><link>https://jmwyf.github.io/p/skip-gram%E6%A8%A1%E5%9E%8B1/</link><pubDate>Sat, 04 Feb 2023 10:57:02 +0000</pubDate><guid>https://jmwyf.github.io/p/skip-gram%E6%A8%A1%E5%9E%8B1/</guid><description>&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Skip-gram&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>属于Word2Vec的一种，给定input，预测上下文，而CBOW是通过上下文来预测input。&lt;/p>
&lt;p>Word2Vec模型分为&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>：&lt;/p>
&lt;ul>
&lt;li>建立模型，使用模型获取嵌入词向量。这类方法与自编吗模型有点像，建模不是最终目的；&lt;/li>
&lt;li>通过模型获取嵌入词向量&lt;/li>
&lt;/ul>
&lt;h2 id="模型细节">模型细节&lt;/h2>
&lt;p>整体框架图&lt;/p>
&lt;h3 id="输入层">输入层&lt;/h3>
&lt;p>词不能直接输入神经网络模型，比如训练样本中有10000个不同的词，将某个词“我”或者“我们”进行one-hot编码，形成10000维的向量，其中“我”的地方为1，其他均为0。对于Skip-gram输入单个词向量，输出就是这个词附近的词组成的向量。&lt;/p>
&lt;p>模型输入输出均为10000维度向量$\{0,1\}^{10000}$&lt;/p>
&lt;h3 id="隐含层">隐含层&lt;/h3>
&lt;p>假如想用300个特征来表征词，那么隐含层单个神经元权重为[10000, 1]，300个神经元为[10000, 300]
&lt;img src="https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/sghidden.png"
loading="lazy"
>&lt;/p>
&lt;p>上图可以理解为输入经过隐含层作用刚好得到单词的表征。&lt;/p>
&lt;h3 id="输出层">输出层&lt;/h3>
&lt;p>模型输出为10000维度向量，要达到这样的输出层为10000个神经元，并且这些神经元输出和为1，使用softmax函数来达到这种效果
$$\begin{align}
[1, 10000]&lt;em>[10000, 300] = [1, 300]\\
[1,300]&lt;/em>[300, 10000] = [1, 10000]
\end{align}
$$&lt;/p>
&lt;h2 id="补充">补充&lt;/h2>
&lt;p>skip-gram名字由来&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>：&lt;/p>
&lt;p>首先n-gram是一系列连续的词（tokens），而skip-gram，或者skip-n-gram，skip的是token之间的gap，jumps over the是一个3-gram，那么(jumps, the)刚好skip了一个gram (over)。&lt;/p>
&lt;p>什么是Softmax？&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/p>
&lt;p>Softmax从字面上来说，可以分成soft和max两个部分。max故名思议就是最大值的意思。Softmax的核心在于soft，而soft有软的含义，与之相对的是hard硬。很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax。hardmax最大的特点就是只选出其中一个最大的值，即非黑即白。Softmax的含义就在于不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。经过使用指数形式的Softmax函数能够将差距大的数值距离拉的更大。&lt;/p>
&lt;p>CBOW是什么？&lt;/p>
&lt;p>Continuous Bag of Word Model连续词带模型，通过上下文来预测中间的词&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank" rel="noopener"
>Word2Vec Tutorial - The Skip-Gram Model · Chris McCormick&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/27234078?from=singlemessage" target="_blank" rel="noopener"
>理解 Word2Vec 之 Skip-Gram 模型 - 知乎&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://www.zhihu.com/question/302594410?utm_id=0" target="_blank" rel="noopener"
>skip-gramm模型skip了什么？为什么叫skip-gramm模型？ - 知乎&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/105722023" target="_blank" rel="noopener"
>一文详解Softmax函数 - 知乎&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>