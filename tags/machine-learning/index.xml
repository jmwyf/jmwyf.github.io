<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning on 从百草园到三味书屋</title><link>https://jmwyf.github.io/tags/machine-learning/</link><description>Recent content in machine learning on 从百草园到三味书屋</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 06 Jan 2023 16:59:00 +0000</lastBuildDate><atom:link href="https://jmwyf.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>《A Unified Approach to interpreting Model Predictions》论文解读</title><link>https://jmwyf.github.io/p/a-unified-approach-to-interpreting-model-predictions%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</link><pubDate>Fri, 06 Jan 2023 16:59:00 +0000</pubDate><guid>https://jmwyf.github.io/p/a-unified-approach-to-interpreting-model-predictions%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</guid><description>&lt;h1 id="a-unified-approach-to-interpreting-model-predictions论文解读">《A Unified Approach to Interpreting Model Predictions》论文解读&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;ul>
&lt;li>大数据让复杂模型的优势明显&lt;/li>
&lt;li>提出一种新颖统一的方法用于模型解释
&lt;ul>
&lt;li>用模型的方法来解释复杂模型（用魔法打败魔法）&lt;/li>
&lt;li>提出SHAP值作为各种方法近似统一特征重要度度量&lt;/li>
&lt;li>提出新的SHAP值估计方法&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="interpretation-model-properties">Interpretation model properties&lt;/h2>
&lt;p>描述解释模型需要有的三个性质，而现在解释方法的缺陷&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;ul>
&lt;li>局部准确性：如果x‘是x的简化特征，对应解释模型g(x&amp;rsquo;) = f(x)，即解释模型在给定的特征情况下能解释为什么模型预测值是这么多。&lt;/li>
&lt;li>缺失性：当x&amp;rsquo;=0的时候，贡献度$\phi$为0&lt;/li>
&lt;li>一致性：模型改变导致特征变的更重要时，贡献度也应该变大&lt;/li>
&lt;/ul>
&lt;h2 id="additive-feature-attribution-methods">Additive Feature Attribution methods&lt;/h2>
&lt;p>一大类方法中解释模型是一系列二元变量的线性函数
称为&lt;strong>Additive Feature Attribution methods&lt;/strong>（AFA）相加特征归因方法
$$g(z&amp;rsquo;) = \phi_0 + \sum_{i=1}^{M}\phi_i z&amp;rsquo;_{i}$$
$z&amp;rsquo; \in {0, 1}^M$&lt;/p>
&lt;h2 id="classic-shapley-value-estimation">Classic Shapley Value Estimation&lt;/h2>
&lt;p>$$\phi_{i} = \sum_{S \subseteq F\backslash{i}}\frac{|S|!(|F|-|S|-1)!}{|F|!}[f_{S\cup{i}}(x_{S\cup{i}})-f_{S(x_S)}]$$
基于上面公式精确计算很麻烦，特征的排列有$2^F$种，计算量巨大&lt;/p>
&lt;h2 id="shap">SHAP&lt;/h2>
&lt;p>SHAP分为模型无关和模型相关两类方法，模型无关的代表是kernelSHAP，而模型相关的代表则有DeepSHAP和TreeSHAP，一个是针对深度学习，一个是针对树模型。&lt;/p>
&lt;h2 id="kernel-shap">Kernel SHAP&lt;/h2>
&lt;h3 id="lime">LIME&lt;/h3>
&lt;p>LIME&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>（Local interpretable model-agnostic explanations)（why should I trust you: Explaining the predictions of any classifier）通过生成的包含需要解释点周围的扰动数据和基于黑箱模型预测结果的数据集，训练一个可以解释的模型，比如逻辑回归、决策树，这个可解释模型需要在解释点周围达到较好的效果。
$$\xi = \mathop{\arg\min}\limits_{g\in G}L(f,g,\pi_x) + \Omega(g)$$&lt;/p>
&lt;ul>
&lt;li>f为需解释模型&lt;/li>
&lt;li>g为可能的解释模型&lt;/li>
&lt;li>$\pi_x$为定义实例周围多大范围&lt;/li>
&lt;/ul>
&lt;p>算法过程：&lt;/p>
&lt;ul>
&lt;li>选择需要解释感兴趣的实例&lt;/li>
&lt;li>对其进行扰动，并得到黑箱模型对应结果产生新数据集&lt;/li>
&lt;li>根据与实例的接近程度，对新数据集进行赋予权重&lt;/li>
&lt;li>基于新数据集和上述损失函数求解可解释模型&lt;/li>
&lt;li>解释预测值
&lt;img src="https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/weightshap.png"
loading="lazy"
>
Figure 3: Toy example to present intuition for LIME&lt;/li>
&lt;/ul>
&lt;h3 id="kernelshaplinear-lime--shapley-value">KernelSHAP（Linear LIME + Shapley value）&lt;/h3>
&lt;p>LIME（Local interpretable model-agnostic explanations）方法的拓展，通过修改LIME需要求解loss等式参数，其主要思路是利用核变换，让$\phi$符合shapley value&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>
算法过程&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>：&lt;/p>
&lt;ul>
&lt;li>根据$z_k^{&amp;rsquo;} \in {0, 1}^M$选择k个样本&lt;/li>
&lt;li>将$z_k^{&amp;rsquo;} \in {0, 1}^M$转化为原始特征值并计算黑箱模型预测值&lt;/li>
&lt;li>基于SHAP kernel计算$z_k^{&amp;rsquo;}$ 样本权重，$z_k$里面1的个数不一样权重就不一样&lt;/li>
&lt;li>拟合线性模型&lt;/li>
&lt;li>从线性模型中返回Shapley values
SHAP核为(推导过程见2补充材料)：
$$\pi_{x&amp;rsquo;}(z&amp;rsquo;)= \frac{(M-1)}{(M choose |z&amp;rsquo;|)|z&amp;rsquo;|(M-|z&amp;rsquo;|)}$$
$$L(f,g, \pi_{x&amp;rsquo;})=\sum_{z&amp;rsquo;\in Z}[f(h_x^{-1}(z&amp;rsquo;))-g(z&amp;rsquo;)]^2\pi_{x&amp;rsquo;}(z&amp;rsquo;)$$
x&amp;rsquo;为简化输入，$x=h_x(x&amp;rsquo;)$, $z&amp;rsquo; \subseteq x&amp;rsquo;$
其中第二步：The function h maps 1’s to the corresponding value from the instance x that we want to explain. For tabular data, it maps 0’s to the values of another instance that we sample from the data. This means that we equate “feature value is absent” with “feature value is replaced by random feature value from data”.&lt;/li>
&lt;/ul>
&lt;h2 id="deep-shap">Deep SHAP&lt;/h2>
&lt;h3 id="deepliftlearning-important-features">DeepLIFT（Learning Important FeaTures）&lt;/h3>
&lt;p>DeepLIFT&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>方法使用神经元的激活与其“参考”进行比较，其中参考是神经元在网络获得“参考输入”时具有的激活状态（参考输入根据具体任务的内容定义）。该方法赋予每个特征重要度分数之和等于预测值与基于参考输入的预测值之间的差异&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>。
能解决基于梯度方法的不足，例如参考的差异不是0的情况下梯度仍然可能是0。
$$\sum_{i=1}^n C_{\Delta x_i \Delta t} = \Delta t \tag1$$
$\Delta t = t - t^0$, 神经元输出与参考输出的差异，$\Delta x$输入相对参考输入的变化， C即特征的贡献&lt;/p>
&lt;p>$$m_{\Delta x\Delta t} = \frac{C_{\Delta x\Delta t}}{\Delta x} \tag2$$
定义乘子multiplier，满足链式法则
算法步骤&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>：&lt;/p>
&lt;ul>
&lt;li>定义参考值
&lt;ul>
&lt;li>choosing a good reference would rely on domain-specific knowledge, and in some cases it may be best to compute DeepLIFT scores against multiple different references&lt;/li>
&lt;li>比如图像用全0或者模糊版本，基因数据用本底期望频率&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>区分正负贡献（2019）&lt;/li>
&lt;li>贡献度规则
&lt;ul>
&lt;li>线性层直接是系数$w* \Delta x_i$&lt;/li>
&lt;li>非线性变化是$m_{\Delta x\Delta y} = \frac{C_{\Delta x\Delta y}}{\Delta x} =\frac{\Delta y}{\Delta x}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="deepshapdeeplift--shapley-value">DeepSHAP(DeepLIFT + Shapley value)&lt;/h3>
&lt;p>尽管kernelSHAP是适用于所有模型的包括深度学习模型的一种可解释方法，但是有没有能利用神经网络特性的可解释方法从而提高计算效率。
DeepLIFT计算的分数近似于Shapley value?并不是这个思路&lt;/p>
&lt;ul>
&lt;li>the Shapely values measure the average marginal effect of including an input over all possible orderings in which inputs can be included. If we define “including” an input as setting it to its actual value instead of its reference value, DeepLIFT can be thought of as a fast approximation of the Shapely values&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;/li>
&lt;li>Though a variety of methods exist for estimating SHAP values, we implemented a modified version of the DeepLIFT algorithm, which computes SHAP by estimating differences in model activations during backpropagation relative to a standard reference.
&lt;img src="https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/components.png"
loading="lazy"
>
&lt;img src="https://cdn.jsdelivr.net/gh/jmwyf/pichosting@master/shap.png"
loading="lazy"
>
figure from ref[3]&lt;/li>
&lt;li>基于Shapley value的定义以及公式可以看出重要的一部分即边际效应，即模型包含该特征减去未包含该部分。在上述一个神经网络模块里面，特征顺序选择都不存在。在认为包含特征即相对于参考输入是真实输入的情况下，把包含特征后乘子直接链式法则做为SHAP值近似公式&lt;/li>
&lt;li>在上述简单网络组件里面，输入到输出之间可以看作线性近似从而得到公式16&lt;/li>
&lt;li>把用实际值代替参考值看作是包含某个特征，DeepLIFT方法与DeepSHAP似乎看不到区别?&lt;/li>
&lt;/ul>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://blog.ml.cmu.edu/2020/08/31/6-interpretability/" target="_blank" rel="noopener"
>6 – Interpretability – Machine Learning Blog | ML@CMU | Carnegie Mellon University&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Ribeiro, M. T., Singh, S. &amp;amp; Guestrin, C. ‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier. &lt;em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining&lt;/em> 1135–1144 (2016) doi:&lt;a class="link" href="https://doi.org/10.1145/2939672.2939778" target="_blank" rel="noopener"
>10.1145/2939672.2939778&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html" target="_blank" rel="noopener"
>A Unified Approach to Interpreting Model Predictions&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>&lt;a class="link" href="https://christophm.github.io/interpretable-ml-book/shap.html" target="_blank" rel="noopener"
>9.6 SHAP (SHapley Additive exPlanations) | Interpretable Machine Learning&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>&lt;a class="link" href="https://libraries.io/pypi/deeplift" target="_blank" rel="noopener"
>deeplift 0.6.13.0 on PyPI - Libraries.io&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>&lt;a class="link" href="https://towardsdatascience.com/explainable-neural-networks-recent-advancements-part-3-6a838d15f2fb" target="_blank" rel="noopener"
>Explainable Neural Networks: Recent Advancements, Part 3 | by G Roshan Lal | Towards Data Science&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>Shrikumar, A., Greenside, P., Shcherbina, A. &amp;amp; Kundaje, A. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences. Preprint at &lt;a class="link" href="https://doi.org/10.48550/arXiv.1605.01713" target="_blank" rel="noopener"
>https://doi.org/10.48550/arXiv.1605.01713&lt;/a> (2017).&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>Shrikumar, A., Greenside, P. &amp;amp; Kundaje, A. Learning Important Features Through Propagating Activation Differences. Preprint at &lt;a class="link" href="http://arxiv.org/abs/1704.02685" target="_blank" rel="noopener"
>http://arxiv.org/abs/1704.02685&lt;/a> (2019).&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>重读XGBoost</title><link>https://jmwyf.github.io/p/%E9%87%8D%E8%AF%BBxgboost/</link><pubDate>Mon, 23 Mar 2020 16:18:13 +0000</pubDate><guid>https://jmwyf.github.io/p/%E9%87%8D%E8%AF%BBxgboost/</guid><description>&lt;img src="https://jmwyf.github.io/images/xgboost/algorithm1.jpg" alt="Featured image of post 重读XGBoost" />&lt;h1 id="重读xgboost">重读XGBoost&lt;/h1>
&lt;p>在使用xgboost方法调参时，对其中个别参数不是特别理解。故重新读了一遍原论文。&lt;/p>
&lt;h2 id="1-引言">1. 引言&lt;/h2>
&lt;p>阐述机器学习和数据驱动的方法应用时两个重要的因素：&lt;/p>
&lt;ul>
&lt;li>能捕捉数据间复杂依赖关系的模型&lt;/li>
&lt;li>可扩展的学习系统，可以从大量数据中学习&lt;/li>
&lt;/ul>
&lt;p>在目前常用的方法中，梯度提升树（gradient tree boosting）在许多场景中效果都不错，作者列举了一些。提出xgboost方法在比赛以及各类问题中的应用。&lt;/p>
&lt;p>叙述XGBoost的优点：运行更快、拓展性更好。创新点包括：&lt;/p>
&lt;ul>
&lt;li>高度可拓展的端到端提升树（tree boosting）系统&lt;/li>
&lt;li>用于高效计算的加权分位数图（weighted quantile sketch）&lt;/li>
&lt;li>新颖的稀疏感知算法（sparsity-aware algorithm），用于并行树学习&lt;/li>
&lt;li>有效的缓存优化以及块（cache-aware block）结构用于外存（out-of-core）树学习
关于以上几点在正文中详解。&lt;/li>
&lt;/ul>
&lt;p>论文结构：&lt;/p>
&lt;ol>
&lt;li>提升树（tree boosting）简介以及目标函数正则化&lt;/li>
&lt;li>分裂点寻找的方法&lt;/li>
&lt;li>系统设计，包括为每个优化提供量化支持的结果&lt;/li>
&lt;li>相关工作&lt;/li>
&lt;li>详细的端到端评估&lt;/li>
&lt;li>总结&lt;/li>
&lt;/ol>
&lt;h2 id="2-提升树tree-boosting简介">2. 提升树（Tree Boosting）简介&lt;/h2>
&lt;p>首先需要了解CART（Classification And Regression Tree）算法，对于cart分类树和回归树分别采用了：&lt;code>Gini系数&lt;/code>、&lt;code>和方差&lt;/code>度量方式来划分节点[1]。例如回归树，对于划分特征A, 划分点s使两边数据集D1和D2,求出使
D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小。
$$\underline{min}_{\text{A,s}}[\underline{min}_{\text{c1}}\sum_{x_i\in{D1(A,s)}}(y_i - c1)^2+\underline{min}_{\text{c2}}\sum_{x_i\in{D2(A,s)}}(y_i - c2)^2]$$
其中，c1为D1的样本输出均值，c2为D2的样本输出均值, 回归树采用叶子节点的均值或者中位数来预测输出结果，$y_i$即样本的label，此时的输出值即下文中用到的$w_{q(x)}$。&lt;/p>
&lt;h3 id="21-正则化目标函数">2.1 正则化目标函数&lt;/h3>
&lt;p>对于这种类型的集成树模型，用K棵树的结果来预测最后的结果（式1），那么问题来了我们怎么来求这些树的参数，每棵树都可以看做一个函数$f_i$包含树的结构以及最后叶节点权重，集成模型不像传统优化问题一样通过简单用梯度下降可以对所有的树进行学习求解，所以，在这里用到了加法策略，即固定已经学习到的，每次加一棵树来进行学习（式3）。
$$\hat{y_i} = \phi(x_i) = \sum_{k = 1}^{K} f_k(x_i)\tag1$$
其中$f(x) = w_{q(x)}$，每个$f_k$对应一个独立的树结构q以及其叶节点权重w，为了学习模型中的参数，最小化下面正则化的目标函数。
$$L(\phi) = \sum_i l(\hat{y_i}, y_i) + \sum_k \Omega(f_k)\tag2$$&lt;/p>
&lt;p>$$\Omega(f_k) = \gamma T + \frac{1}{2}\lambda||w||^2$$
T是树的叶节点数&lt;/p>
&lt;h3 id="22-梯度提升树gradient-tree-boosting">2.2 梯度提升树（Gradient Tree Boosting）&lt;/h3>
&lt;p>第t次预测值等于t-1加上第t棵树的结果
$$\hat{y_i} = \hat{y_i}^{t-1} + f_t(x_i)\tag3$$
此时目标函数(式2)可以写成
$$L^{(t)} = \sum_{i=1} ^n l(y_i, \hat{y_i}^{(t-1)}+f_t(x_i)) + \Omega(f_t)\tag4$$
该式子的二阶近似可以表达为(式5），可以参考补充中&lt;a class="link" href="#taylor" >二阶泰勒展开的一般形式&lt;/a>
$$L^{(t)} \approx \sum_{i=1} ^n [l(y_i, \hat{y_i}^{(t-1)}) + g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)\tag5$$
其中 $g_i=\partial_{\hat{y}^{(t-1)}}{l(y_i, \hat{y_i}^{(t-1)})}， h_i=\partial_{\hat{y}^{(t-1)}}^2{l(y_i, \hat{y_i}^{(t-1)})}$&lt;/p>
&lt;p>式5中去掉常数项，即label与第t-1次结果的损失函数，可以得到：&lt;/p>
&lt;p>$$\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)\tag6$$&lt;/p>
&lt;p>式6即对新树的优化目标函数，进一步合并可以写为：&lt;/p>
&lt;p>$$obj^{t} \approx \sum_{i=1}^{n}[g_i w_{q(x_i)} + \frac{1}{2}h_i w^2_{q(x_i)}] + \gamma T+ \frac{1}{2}\lambda \sum _{j=1}^T w_j^2$$&lt;/p>
&lt;p>定义$I_j = {i|q(x_i)=j}$即叶节点j上的实例。&lt;/p>
&lt;p>$$obj^{t} \approx \sum_{j=1}^T[(\sum_{i\in{I_j}}g_i)w_j+\frac{1}{2}(\sum_{i\in{I_j}}h_i+\lambda)w_j^2]+\gamma T\tag7$$&lt;/p>
&lt;p>上式7对$w_j$求导即可求出w最优值&lt;/p>
&lt;p>$$w_j^* = -\frac{\sum_{i\in{I_j}}g_i}{\sum_{i\in{I_j}}h_i+\lambda} $$&lt;/p>
&lt;p>令$G_j = \sum_{i\in{I_j}}g_i，H_j=\sum_{i\in{I_j}}h_i$&lt;/p>
&lt;p>此时，对应的最小值为:
$$obj^* = -\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T$$
$\color{red}\frac{G_j^2}{H_j+\lambda}$越大，loss越小，所以对叶节点进行分裂，分裂后增益定义为
$$Gain=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gamma\tag8$$&lt;/p>
&lt;h3 id="23-缩减和列抽样shrinkage-and-column-subsampling">2.3 缩减和列抽样（Shrinkage and Column Subsampling）&lt;/h3>
&lt;p>除了在目标函数中引入正则项，在防止过拟合方面xgboost还运用了两项技术，给每一步tree boosting得到的结果一个权重$\eta$，来降低每一步的影响从而给后面树的形成留下空间，比喻成优化问题中的学习率缩减；同时还用到随机森林中的列抽样，即随机特征筛选。&lt;/p>
&lt;h2 id="3-分裂点寻找算法">3. 分裂点寻找算法&lt;/h2>
&lt;h3 id="31-精确贪婪算法basic-exact-greedy-algorithm">3.1 精确贪婪算法（Basic Exact Greedy Algorithm）&lt;/h3>
&lt;p>即按照2.2中式8来寻找分裂点
python&lt;code>scikit-learn&lt;/code>，R&lt;code>gbm&lt;/code>，单机的xgboost都支持。&lt;/p>
&lt;div align=center>
&lt;img src="https://jmwyf.github.io/images/xgboost/algorithm1.jpg" width=50% heigth=50% />
&lt;/div>
&lt;!-- ![Algorithm1](/images/xgboost/algorithm1.jpg) -->
&lt;h3 id="32-近似算法approximate-algorithm">3.2 近似算法（Approximate Algorithm）&lt;/h3>
&lt;p>精确贪婪算法由于列举了所有可能的分裂点，在数据量很大不能全部写入内存时会导致不是那么高效。所以提出近似算法。对于每个特征，只考察分位点，减少计算复杂度。
近似算法存在两个变种：&lt;/p>
&lt;ul>
&lt;li>global: 学习每棵树前，提出候选分裂点&lt;/li>
&lt;li>local: 每次分裂前，重新提出候选分裂点&lt;/li>
&lt;/ul>
&lt;div align=center>
&lt;img src="https://jmwyf.github.io/images/xgboost/algorithm2.jpg" width=50% heigth=50% />
&lt;/div>
&lt;!-- ![Algorithm2](/images/xgboost/algorithm2.jpg) -->
&lt;h3 id="33-加权分位数图weighted-quantile-sketch">3.3 加权分位数图（Weighted Quantile Sketch）&lt;/h3>
&lt;p>近似算法中最重要一点即提出候选分裂点，xgboost不是简单的按照样本个体进行分位，而是以损失函数二阶导数值作为权重进行分位数分裂。如何寻找二阶导数分位点，首先是利用权重计算排序函数，然后相邻相减值作为判断依据。问题是为什么会想到利用损失函数二阶导数值作为权重来划分。
文中给出式6可以变形为
$$\sum_{i=1}^n\frac{1}{2}h_i(f_t(x_i)-g_i/h_i)^2 + \Omega(f_t) + constant\tag9$$
指出该式恰好是权重平方差损失函数，权重$h_i$以及label $g_i/h_i$
自己从式6变不到式9，觉得中间符号是+还差不多。
看有人理解说变成式10才对。是否作者真的是这样想的，不得而知。欢迎指正。
$$\sum_{i=1}^n\frac{1}{2}h_i(f_t(x_i)-(-g_i/h_i))^2 + \Omega(f_t) + constant\tag{10}$$
&lt;a class="link" href="https://datascience.stackexchange.com/questions/10997/need-help-understanding-xgboosts-approximate-split-points-proposal" target="_blank" rel="noopener"
>stackexchange上关于理解xgboost近似分裂点&lt;/a>&lt;/p>
&lt;h3 id="34-稀疏值感知分裂sparsity-aware-split-finding">3.4 稀疏值感知分裂（Sparsity-aware split finding）&lt;/h3>
&lt;p>造成稀疏值的原因：1）缺失值 2）统计过程中频繁的0值输入 3）one-hot编码以及其他特征工程
所以让算法注意数据中稀疏规律很重要，遍历所有特征，在划分子节点时，统一将该特征的缺失值划分到右支或者左支，计算最大的gain。&lt;/p>
&lt;div align=center>
&lt;img src="https://jmwyf.github.io/images/xgboost/Sparsity.jpg" width=50% heigth=50% />
&lt;/div>
&lt;!-- ![Sparsity](/images/xgboost/Sparsity.jpg) -->
&lt;p>$\color{red}这里也有个疑问就是为什么排序第一次是升序，第二次是降序$&lt;/p>
&lt;h2 id="4-系统设计">4. 系统设计&lt;/h2>
&lt;h3 id="41-分块并行column-block-for-parallel-learning">4.1 分块并行（Column Block for Parallel Learning）&lt;/h3>
&lt;p>基于树学习过程中最耗时的是将数据排序，为了减少排序的时间成本，提出基于内存的block结构。&lt;/p>
&lt;ul>
&lt;li>在Exact greedy算法中，将整个数据集存放在一个Block中&lt;/li>
&lt;li>在近似算法中，使用多个Block，每个Block对应原来数据的子集。不同的Block可以在不同的机器上并行计算&lt;/li>
&lt;/ul>
&lt;h3 id="42-缓存优化">4.2 缓存优化&lt;/h3>
&lt;p>这里指利用CPU缓存对算法进行优化。&lt;/p>
&lt;p>4.1中column block按特征大小顺序存储，相应的样本的梯度信息是分散的，造成内存的不连续访问，降低CPU cache命中率。
优化方法：&lt;/p>
&lt;ul>
&lt;li>对于精确贪婪算法，预取数据到buffer中（非连续-&amp;gt;连续），再统计梯度信息。&lt;/li>
&lt;li>对于近似算法，调节block的大小，设置过大则容易导致命中率低，过小则容易导致并行化效率不高。&lt;/li>
&lt;/ul>
&lt;h3 id="43-外存计算">4.3 外存计算&lt;/h3>
&lt;p>除了处理器以及内存，利用磁盘空间来处理不能进入内存的数据也十分重要，数据划分为多个Block并存放在磁盘上。计算的时候，使用独立的线程预先将Block放入主内存，因此可以在计算的同时读取磁盘。在减少计算资源开销以及提高磁盘输入输出方面主要用到以下技术：&lt;/p>
&lt;ul>
&lt;li>Block压缩，按列压缩，加载到主内存时由独立线程动态解压缩。具体压缩技术参看原文。&lt;/li>
&lt;li>Block Sharding，将数据划分到不同硬盘上，提高磁盘吞吐率。&lt;/li>
&lt;/ul>
&lt;h2 id="5-端到端评估">5. 端到端评估&lt;/h2>
&lt;p>利用4个数据集对xgboost评估：&lt;/p>
&lt;ul>
&lt;li>分类问题&lt;/li>
&lt;li>排序问题&lt;/li>
&lt;li>外存计算实验&lt;/li>
&lt;li>分布计算实验&lt;/li>
&lt;/ul>
&lt;p>这几个方面进行评估，详细结果见论文。&lt;/p>
&lt;h2 id="ref">ref&lt;/h2>
&lt;ol>
&lt;li>&lt;a class="link" href="https://blog.csdn.net/hy592070616/article/details/81628956" target="_blank" rel="noopener"
>CART分类树与回归树&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/xym4869/p/11282586.html" target="_blank" rel="noopener"
>Markdown数学公式&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.mathjax.org/en/latest/web/configuration.html" target="_blank" rel="noopener"
>Mathjax应用在网页&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="noopener"
>XGBoost.ppt&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" target="_blank" rel="noopener"
>readthedocs xgboost tutorials&lt;/a>推荐&lt;/li>
&lt;li>&lt;a class="link" href="http://wepon.me/files/gbdt.pdf" target="_blank" rel="noopener"
>gbdt.ppt&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/1603.02754" target="_blank" rel="noopener"
>xgboost原文&lt;/a>&lt;/li>
&lt;/ol>
&lt;h2 id="补充">补充&lt;/h2>
&lt;ol>
&lt;li>文中很多术语翻译可能有不恰当的地方，欢迎指出。&lt;/li>
&lt;li>二阶泰勒展开的一般形式：
$$f(x^t) = f(x^{t-1}+\Delta x)\approx{f(x^{t-1})+ f^{\prime}(x^{t-1})\Delta{x}+f^{\prime\prime}(x^{t-1})\frac{\Delta x^2}{2}}$$&lt;/li>
&lt;li>式4中加入loss function是mean squared error(MSE)，可以求出相应的gi， hi作为一个特例来验证该做法。&lt;/li>
&lt;li>基于树的算法理解时带着这几个问题去理解每一步是用来做什么的：选择哪个特征进行分裂？在特征什么点位进行分裂？分裂后叶节点取什么值？
&lt;blockquote>
&lt;p>分别对应：遍历每个特征，加权分位数图，$w_j$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>对于系统设计中应用到的技术理解不是十分深刻，对应一个算法如何从计算机硬件的方方面面考虑去优化对非专业领域研究者还是比较难&lt;/li>
&lt;/ol></description></item></channel></rss>